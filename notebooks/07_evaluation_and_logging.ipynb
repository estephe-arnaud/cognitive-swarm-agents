{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb885dcb",
   "metadata": {},
   "source": [
    "# Notebook 07: Évaluation du Système et Logging avec Weights & Biases\n",
    "\n",
    "Ce notebook montre comment utiliser nos modules d'évaluation (`RagEvaluator`, `SynthesisEvaluator`) pour mesurer la performance de \"Cognitive Swarm\" et comment intégrer ces résultats avec Weights & Biases (W&B) en utilisant `WandBMetricsLogger`.\n",
    "\n",
    "**Prérequis :**\n",
    "* **Environnement de Base :** Avoir exécuté le notebook `00_setup_environment.ipynb` pour configurer l'environnement Conda, les dépendances Python (notamment `wandb`, `pandas`, et les bibliothèques LangChain/LlamaIndex nécessaires), et s'assurer que le fichier `.env` à la racine du projet est correctement rempli.\n",
    "* **Configuration des Fournisseurs (dans `.env`) :** Les évaluateurs de ce notebook dépendent des configurations suivantes :\n",
    "    * **Pour `RagEvaluator` (via `RetrievalEngine`) :** La configuration de `DEFAULT_EMBEDDING_PROVIDER` et les variables associées (clés API, URLs, noms de modèles) sont cruciales pour l'embedding des requêtes.\n",
    "        * Si `DEFAULT_EMBEDDING_PROVIDER` est `\"openai\"` : `OPENAI_API_KEY`, `OPENAI_EMBEDDING_MODEL_NAME`.\n",
    "        * Si `DEFAULT_EMBEDDING_PROVIDER` est `\"huggingface\"` (local Sentence Transformers) : `HUGGINGFACE_EMBEDDING_MODEL_NAME`.\n",
    "        * Si `DEFAULT_EMBEDDING_PROVIDER` est `\"ollama\"` : `OLLAMA_BASE_URL`, `OLLAMA_EMBEDDING_MODEL_NAME` et le modèle Ollama correspondant doit être disponible localement.\n",
    "    * **Pour `SynthesisEvaluator` (LLM Juge via `get_llm`) :** La configuration de `DEFAULT_LLM_MODEL_PROVIDER` et les variables associées (clés API, URLs, noms de modèles) sont utilisées.\n",
    "        * Si `DEFAULT_LLM_MODEL_PROVIDER` est `\"openai\"` : `OPENAI_API_KEY`, `DEFAULT_OPENAI_GENERATIVE_MODEL`.\n",
    "        * Si `DEFAULT_LLM_MODEL_PROVIDER` est `\"huggingface_api\"` : `HUGGINGFACE_API_KEY`, `HUGGINGFACE_REPO_ID`.\n",
    "        * Si `DEFAULT_LLM_MODEL_PROVIDER` est `\"ollama\"` : `OLLAMA_BASE_URL`, `OLLAMA_GENERATIVE_MODEL_NAME` et le modèle Ollama correspondant doit être disponible localement.\n",
    "* **Base de Données MongoDB :** `MONGODB_URI` doit être configuré dans `.env` et l'instance accessible. Pour que l'évaluation RAG soit significative, la base de données doit être peuplée (via `01_data_ingestion_and_embedding.ipynb` ou `scripts/run_ingestion.py`) avec des embeddings correspondant au `DEFAULT_EMBEDDING_PROVIDER` actif.\n",
    "* **Jeux de Données d'Évaluation :**\n",
    "    * Pour `RagEvaluator` : Un fichier JSON (par exemple, `data/evaluation/rag_eval_dataset.json`, chemin configurable via `settings.EVALUATION_DATASET_PATH`) contenant des requêtes et les `chunk_id` pertinents attendus. Un jeu de données de démo interne est utilisé si le fichier n'est pas trouvé ou spécifié.\n",
    "    * Pour `SynthesisEvaluator` : Des triplets `(requête, contexte, synthèse_à_évaluer)` sont nécessaires. Ce notebook utilise des exemples directement dans le code, mais pour une évaluation à plus grande échelle, un fichier de données serait utilisé (comme dans `scripts/run_evaluation.py`).\n",
    "* **Weights & Biases :** Pour logger les métriques sur W&B, `WANDB_API_KEY` doit être configurée dans `.env` (ou vous devez être connecté via `wandb login`). Le logging W&B peut être désactivé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67db443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import pandas as pd # Pour créer des tables pour W&B\n",
    "from typing import Optional, List, Dict, Any \n",
    "\n",
    "# --- Gestion asyncio pour Jupyter ---\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "# --- Fin Gestion asyncio ---\n",
    "\n",
    "# --- Configuration du PYTHONPATH et chargement de .env ---\n",
    "# NOTE IMPORTANTE SUR LE CWD (Current Working Directory) :\n",
    "# La ligne suivante `project_root = Path().resolve().parent` suppose que le CWD du notebook\n",
    "# est le dossier `/notebooks/`. Si vous avez configuré VS Code pour que le CWD\n",
    "# soit la racine du projet (`cognitive-swarm-agents/`), alors `Path().resolve()`\n",
    "# donnerait déjà la racine du projet, et vous devriez utiliser :\n",
    "# project_root = Path().resolve()\n",
    "# Vérifiez votre CWD avec `import os; print(os.getcwd())` ou `from pathlib import Path; print(Path().resolve())` pour confirmer.\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "    print(f\"Ajout de {project_root} au PYTHONPATH\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# De même, si CWD est la racine du projet, dotenv_path serait `Path().resolve() / \".env\"`\n",
    "dotenv_path = project_root / \".env\"\n",
    "if dotenv_path.exists():\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    print(f\"Variables d'environnement chargées depuis : {dotenv_path}\")\n",
    "else:\n",
    "    print(f\"ATTENTION: Fichier .env non trouvé à {dotenv_path}. Assurez-vous qu'il est à la racine du projet.\")\n",
    "\n",
    "from config.settings import settings\n",
    "from config.logging_config import setup_logging\n",
    "\n",
    "from src.rag.retrieval_engine import RetrievalEngine \n",
    "from src.evaluation.rag_evaluator import RagEvaluator, RagEvaluationMetrics\n",
    "from src.evaluation.synthesis_evaluator import SynthesisEvaluator, SynthesisEvaluationResult, EvaluationAspectScore\n",
    "from src.evaluation.metrics_logger import WandBMetricsLogger\n",
    "from src.vector_store.mongodb_manager import MongoDBManager \n",
    "\n",
    "setup_logging(level=\"INFO\")\n",
    "logger = logging.getLogger(\"nb_07_evaluation_logging\")\n",
    "\n",
    "# --- Vérifications critiques pour ce notebook ---\n",
    "logger.info(f\"--- Configuration Active pour l'Évaluation (depuis settings.py et .env) ---\")\n",
    "config_valid = True # Indicateur global de validité de la configuration\n",
    "\n",
    "# 1. Fournisseur d'embedding (pour RagEvaluator via RetrievalEngine)\n",
    "active_embedding_provider = settings.DEFAULT_EMBEDDING_PROVIDER.lower()\n",
    "logger.info(f\"Fournisseur d'embedding (pour RAG) : '{active_embedding_provider}'\")\n",
    "if active_embedding_provider == \"openai\":\n",
    "    if not (settings.OPENAI_API_KEY and settings.OPENAI_EMBEDDING_MODEL_NAME):\n",
    "        logger.error(\"ERREUR : Embedding Provider est 'openai', mais OPENAI_API_KEY et/ou OPENAI_EMBEDDING_MODEL_NAME manquants.\")\n",
    "        config_valid = False\n",
    "elif active_embedding_provider == \"huggingface\":\n",
    "    if not settings.HUGGINGFACE_EMBEDDING_MODEL_NAME:\n",
    "        logger.error(\"ERREUR : Embedding Provider est 'huggingface', mais HUGGINGFACE_EMBEDDING_MODEL_NAME manquant.\")\n",
    "        config_valid = False\n",
    "elif active_embedding_provider == \"ollama\":\n",
    "    if not (settings.OLLAMA_BASE_URL and settings.OLLAMA_EMBEDDING_MODEL_NAME):\n",
    "        logger.error(\"ERREUR : Embedding Provider est 'ollama', mais OLLAMA_BASE_URL et/ou OLLAMA_EMBEDDING_MODEL_NAME manquants.\")\n",
    "        config_valid = False\n",
    "else:\n",
    "    logger.error(f\"ERREUR : Fournisseur d'embedding inconnu : '{active_embedding_provider}'\")\n",
    "    config_valid = False\n",
    "\n",
    "# 2. Fournisseur LLM Juge (pour SynthesisEvaluator via get_llm)\n",
    "llm_judge_provider = settings.DEFAULT_LLM_MODEL_PROVIDER.lower()\n",
    "logger.info(f\"Fournisseur LLM Juge (pour Évaluation de Synthèse) : '{llm_judge_provider}'\")\n",
    "if llm_judge_provider == \"openai\":\n",
    "    if not (settings.OPENAI_API_KEY and settings.DEFAULT_OPENAI_GENERATIVE_MODEL):\n",
    "        logger.error(\"ERREUR : LLM Juge Provider est 'openai', mais OPENAI_API_KEY et/ou DEFAULT_OPENAI_GENERATIVE_MODEL manquants.\")\n",
    "        config_valid = False\n",
    "elif llm_judge_provider == \"huggingface_api\":\n",
    "    if not (settings.HUGGINGFACE_API_KEY and settings.HUGGINGFACE_REPO_ID):\n",
    "        logger.error(\"ERREUR : LLM Juge Provider est 'huggingface_api', mais HUGGINGFACE_API_KEY et/ou HUGGINGFACE_REPO_ID manquants.\")\n",
    "        config_valid = False\n",
    "elif llm_judge_provider == \"ollama\":\n",
    "    if not (settings.OLLAMA_BASE_URL and settings.OLLAMA_GENERATIVE_MODEL_NAME):\n",
    "        logger.error(\"ERREUR : LLM Juge Provider est 'ollama', mais OLLAMA_BASE_URL et/ou OLLAMA_GENERATIVE_MODEL_NAME manquants.\")\n",
    "        config_valid = False\n",
    "else:\n",
    "    logger.error(f\"ERREUR : Fournisseur LLM Juge inconnu : '{llm_judge_provider}'\")\n",
    "    config_valid = False\n",
    "\n",
    "# 3. Weights & Biases\n",
    "if not settings.WANDB_API_KEY and not os.environ.get(\"WANDB_API_KEY\"): \n",
    "    logger.warning(\"WANDB_API_KEY non trouvé. Le logging sur W&B pourrait échouer ou demander une authentification interactive.\")\n",
    "else:\n",
    "    logger.info(\"Clé WANDB_API_KEY trouvée ou W&B est potentiellement configuré globalement.\")\n",
    "\n",
    "# 4. MongoDB\n",
    "if not settings.MONGODB_URI or (\"<user>\" in settings.MONGODB_URI and \"<password>\" in settings.MONGODB_URI) or \"<cluster_url>\" in settings.MONGODB_URI:\n",
    "    logger.error(\"ERREUR CRITIQUE : MONGODB_URI non trouvé ou semble non configuré. RetrievalEngine ne pourra pas fonctionner pour l'évaluation RAG.\")\n",
    "    config_valid = False\n",
    "else:\n",
    "    logger.info(\"MongoDB URI configuré.\")\n",
    "\n",
    "if not config_valid:\n",
    "    logger.critical(\"Des configurations essentielles sont manquantes ou incorrectes. Veuillez vérifier votre fichier .env et settings.py. Les évaluations risquent d'échouer.\")\n",
    "\n",
    "# Nom de la collection pour les tests RAG\n",
    "USE_NOTEBOOK_TEST_COLLECTION_FOR_RAG_EVAL = True \n",
    "if USE_NOTEBOOK_TEST_COLLECTION_FOR_RAG_EVAL:\n",
    "    COLLECTION_NAME_EVAL = \"arxiv_chunks_notebook_test\" \n",
    "    logger.info(f\"Évaluation RAG ciblera la collection de test du notebook: '{COLLECTION_NAME_EVAL}'\")\n",
    "else:\n",
    "    COLLECTION_NAME_EVAL = MongoDBManager.DEFAULT_CHUNK_COLLECTION_NAME\n",
    "    logger.info(f\"Évaluation RAG ciblera la collection principale par défaut: '{COLLECTION_NAME_EVAL}'\")\n",
    "\n",
    "logger.info(\"--- Fin de la Vérification de Configuration Active ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73430684",
   "metadata": {},
   "source": [
    "### 1. Connexion à Weights & Biases\n",
    "\n",
    "Avant de commencer les évaluations, nous allons nous assurer que nous pouvons nous connecter à W&B. Le `WandBMetricsLogger` s'en chargera. Si vous n'êtes pas déjà connecté via le CLI (`wandb login`), la présence de `WANDB_API_KEY` dans votre `.env` est fortement recommandée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64021862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell from 07_evaluation_and_logging.ipynb (typically the one after W&B setup)\n",
    "# Make sure to replace the entire content of this cell in your notebook.\n",
    "\n",
    "# La variable COLLECTION_NAME_EVAL est définie dans la cellule précédente (ID b67db443)\n",
    "\n",
    "def run_rag_evaluation_notebook_demo(top_k_eval: int = 3) -> Optional[RagEvaluationMetrics]: \n",
    "    logger.info(f\"--- Début de l'Évaluation RAG (k={top_k_eval}) ---\")\n",
    "    \n",
    "    can_proceed = True\n",
    "    if not settings.MONGODB_URI or (\"<user>\" in settings.MONGODB_URI): \n",
    "        logger.error(\"RAG EVAL ERROR: MONGODB_URI non configuré correctement pour RetrievalEngine.\")\n",
    "        can_proceed = False\n",
    "\n",
    "    active_embedding_provider_check = settings.DEFAULT_EMBEDDING_PROVIDER.lower()\n",
    "    if active_embedding_provider_check == \"openai\":\n",
    "        if not (settings.OPENAI_API_KEY and settings.OPENAI_EMBEDDING_MODEL_NAME):\n",
    "            logger.error(\"RAG EVAL ERROR: Configuration OpenAI embedding incomplète (OPENAI_API_KEY ou OPENAI_EMBEDDING_MODEL_NAME).\")\n",
    "            can_proceed = False\n",
    "    elif active_embedding_provider_check == \"huggingface\": \n",
    "        if not settings.HUGGINGFACE_EMBEDDING_MODEL_NAME:\n",
    "            logger.error(\"RAG EVAL ERROR: HUGGINGFACE_EMBEDDING_MODEL_NAME non configuré pour le provider 'huggingface'.\")\n",
    "            can_proceed = False\n",
    "    elif active_embedding_provider_check == \"ollama\":\n",
    "        if not (settings.OLLAMA_BASE_URL and settings.OLLAMA_EMBEDDING_MODEL_NAME):\n",
    "            logger.error(\"RAG EVAL ERROR: Configuration Ollama embedding incomplète (OLLAMA_BASE_URL ou OLLAMA_EMBEDDING_MODEL_NAME).\")\n",
    "            can_proceed = False\n",
    "    else: \n",
    "        logger.error(f\"RAG EVAL ERROR: Fournisseur d'embedding '{active_embedding_provider_check}' non supporté ou mal configuré.\")\n",
    "        can_proceed = False\n",
    "\n",
    "    if not can_proceed:\n",
    "        print(\"ERREUR: Prérequis pour l'évaluation RAG non remplis. Vérifiez les logs et votre configuration .env.\")\n",
    "        return None\n",
    "\n",
    "    config_for_wandb_rag = {\n",
    "        \"evaluation_type\": \"RAG_Notebook_Demo\",\n",
    "        \"retrieval_top_k_eval\": top_k_eval,\n",
    "        \"rag_dataset_path\": str(settings.EVALUATION_DATASET_PATH) if settings.EVALUATION_DATASET_PATH else \"Config: Internal Default or Not Specified\",\n",
    "        \"mongo_collection\": COLLECTION_NAME_EVAL, \n",
    "        \"active_embedding_provider\": active_embedding_provider_check\n",
    "    }\n",
    "    if active_embedding_provider_check == \"openai\":\n",
    "        config_for_wandb_rag[\"active_embedding_model\"] = settings.OPENAI_EMBEDDING_MODEL_NAME\n",
    "        config_for_wandb_rag[\"active_embedding_dimension\"] = settings.OPENAI_EMBEDDING_DIMENSION\n",
    "    elif active_embedding_provider_check == \"huggingface\":\n",
    "        config_for_wandb_rag[\"active_embedding_model\"] = settings.HUGGINGFACE_EMBEDDING_MODEL_NAME\n",
    "        config_for_wandb_rag[\"active_embedding_dimension\"] = settings.HUGGINGFACE_EMBEDDING_MODEL_DIMENSION\n",
    "    elif active_embedding_provider_check == \"ollama\":\n",
    "        config_for_wandb_rag[\"active_embedding_model\"] = settings.OLLAMA_EMBEDDING_MODEL_NAME\n",
    "        config_for_wandb_rag[\"active_embedding_dimension\"] = settings.OLLAMA_EMBEDDING_MODEL_DIMENSION\n",
    "\n",
    "    wb_logger = WandBMetricsLogger(\n",
    "        project_name=\"CognitiveSwarm-Evaluations-Notebook\", \n",
    "        run_name=f\"RAG_Eval_Demo_k{top_k_eval}_{active_embedding_provider_check.replace('/', '_')}\",\n",
    "        config_to_log=config_for_wandb_rag,\n",
    "        tags=[\"rag\", \"notebook_demo\", f\"k{top_k_eval}\", active_embedding_provider_check]\n",
    "    )\n",
    "\n",
    "    rag_metrics_result: Optional[RagEvaluationMetrics] = None\n",
    "    run_started_by_this_func = False \n",
    "\n",
    "    try:\n",
    "        if not wb_logger.is_disabled:\n",
    "            if wb_logger.start_run(): \n",
    "                 run_started_by_this_func = True\n",
    "            else: \n",
    "                logger.warning(\"W&B run n'a pas pu démarrer via wb_logger.start_run(). Le logging W&B sera désactivé.\")\n",
    "        \n",
    "        retrieval_engine_instance_rag = RetrievalEngine(collection_name=COLLECTION_NAME_EVAL)\n",
    "        \n",
    "        rag_eval_dataset_path_input = Path(settings.EVALUATION_DATASET_PATH) if settings.EVALUATION_DATASET_PATH and Path(settings.EVALUATION_DATASET_PATH).exists() else None\n",
    "        evaluator_rag = RagEvaluator(\n",
    "            retrieval_engine=retrieval_engine_instance_rag,\n",
    "            eval_dataset_path=rag_eval_dataset_path_input\n",
    "        )\n",
    "\n",
    "        if run_started_by_this_func and wb_logger.wandb_run:\n",
    "            # MODIFIED LINE: Using the new attribute name 'dataset_source_path'\n",
    "            actual_dataset_path_logged = str(evaluator_rag.dataset_source_path) if evaluator_rag.dataset_source_path else \"Internal Default Demo\"\n",
    "            if config_for_wandb_rag.get(\"rag_dataset_path\") != actual_dataset_path_logged:\n",
    "                 wb_logger.log_configuration({\"rag_dataset_path_used\": actual_dataset_path_logged})\n",
    "\n",
    "        if not evaluator_rag.eval_dataset:\n",
    "            logger.error(\"Jeu de données d'évaluation RAG vide ou non chargé. Arrêt de l'évaluation RAG.\")\n",
    "        else:\n",
    "            # MODIFIED LINE: Using the new attribute name 'dataset_source_path' for logging\n",
    "            logger.info(f\"Évaluation RAG avec {len(evaluator_rag.eval_dataset)} requêtes (Dataset: {str(evaluator_rag.dataset_source_path) if evaluator_rag.dataset_source_path else 'Internal Default Demo'}).\")\n",
    "            rag_metrics_result = evaluator_rag.evaluate(k=top_k_eval)\n",
    "\n",
    "            if rag_metrics_result:\n",
    "                evaluator_rag.print_results(rag_metrics_result)\n",
    "                if run_started_by_this_func and wb_logger.wandb_run:\n",
    "                    wb_logger.log_rag_evaluation_results(rag_metrics_result, eval_name=f\"RAG_Demo_Eval_k{top_k_eval}\")\n",
    "                    \n",
    "                    eval_details_list = []\n",
    "                    for item in evaluator_rag.eval_dataset: \n",
    "                        eval_details_list.append({\n",
    "                            \"query_id\": item[\"query_id\"],\n",
    "                            \"query_text\": item[\"query_text\"],\n",
    "                            \"expected_relevant_chunk_ids\": \", \".join(item[\"expected_relevant_chunk_ids\"])\n",
    "                        })\n",
    "                    if eval_details_list:\n",
    "                        try:\n",
    "                            details_df = pd.DataFrame(eval_details_list)\n",
    "                            wb_logger.log_dataframe_as_table(details_df, \"RAG_Evaluation_Queries_Used_Demo\")\n",
    "                        except Exception as e_df:\n",
    "                            logger.error(f\"Erreur lors du logging de la table de détails RAG : {e_df}\")\n",
    "            else:\n",
    "                logger.warning(\"L'évaluation RAG n'a pas produit de métriques.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Une exception s'est produite pendant l'évaluation RAG : {e}\", exc_info=True)\n",
    "        if run_started_by_this_func and wb_logger.wandb_run: \n",
    "            try:\n",
    "                wb_logger.log_summary_metrics({\"rag_eval_error\": 1, \"error_message\": str(e)[:250]})\n",
    "            except Exception as e_log:\n",
    "                logger.error(f\"Erreur lors du logging de l'erreur RAG sur W&B : {e_log}\")\n",
    "    finally:\n",
    "        if run_started_by_this_func and wb_logger.wandb_run: \n",
    "            wb_logger.end_run()\n",
    "            logger.info(\"Run W&B pour l'évaluation RAG terminé.\")\n",
    "            \n",
    "    return rag_metrics_result\n",
    "\n",
    "# --- Exécution de la démo d'évaluation RAG ---\n",
    "print(\"Lancement de la démonstration d'évaluation RAG...\") # Already in user's code\n",
    "rag_evaluation_results = run_rag_evaluation_notebook_demo(top_k_eval=3) # Already in user's code\n",
    "\n",
    "if rag_evaluation_results:\n",
    "    print(\"\\n✅ Évaluation RAG terminée avec succès et métriques produites.\") # Already in user's code\n",
    "else:\n",
    "    print(\"\\n⚠️ L'évaluation RAG a échoué, n'a pas pu démarrer en raison de prérequis manquants, ou n'a pas produit de métriques. Vérifiez les logs ci-dessus.\") # Already in user's code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc896710",
   "metadata": {},
   "source": [
    "### 2. Démonstration de l'Évaluation RAG\n",
    "\n",
    "Nous allons instancier `RetrievalEngine` et `RagEvaluator`, exécuter une évaluation sur un petit jeu de données (le jeu par défaut de `RagEvaluator` ou un fichier JSON si configuré), puis logger les résultats et la configuration sur W&B.\n",
    "\n",
    "**Action Requise :** Pour des résultats significatifs, assurez-vous d'avoir un fichier `rag_eval_dataset.json` (chemin configurable via `settings.EVALUATION_DATASET_PATH`) contenant des requêtes et les `chunk_id` pertinents attendus pour votre corpus. Le `RagEvaluator` utilisera son jeu de données de démo interne si ce fichier n'est pas trouvé ou non spécifié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e59a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La variable LOG_LEVEL_NOTEBOOK est définie dans la première cellule (ID b67db443)\n",
    "# nest_asyncio.apply() a aussi été appelé dans la première cellule.\n",
    "\n",
    "async def run_synthesis_evaluation_notebook_demo(\n",
    "    judge_model_name_override: Optional[str] = None\n",
    ") -> Optional[SynthesisEvaluationResult]: \n",
    "    \n",
    "    actual_judge_provider = settings.DEFAULT_LLM_MODEL_PROVIDER.lower()\n",
    "    actual_judge_model_name = judge_model_name_override \n",
    "    \n",
    "    if not actual_judge_model_name:\n",
    "        if actual_judge_provider == \"openai\":\n",
    "            actual_judge_model_name = settings.DEFAULT_OPENAI_GENERATIVE_MODEL\n",
    "        elif actual_judge_provider == \"huggingface_api\":\n",
    "            actual_judge_model_name = settings.HUGGINGFACE_REPO_ID\n",
    "        elif actual_judge_provider == \"ollama\":\n",
    "            actual_judge_model_name = settings.OLLAMA_GENERATIVE_MODEL_NAME\n",
    "        else:\n",
    "            actual_judge_model_name = \"Unknown_Default_Model\"\n",
    "            \n",
    "    logger.info(f\"--- Début de l'Évaluation de la Synthèse (Juge LLM Provider: {actual_judge_provider}, Juge LLM Model: {actual_judge_model_name}) ---\")\n",
    "\n",
    "    can_proceed = True\n",
    "    if actual_judge_provider == \"openai\":\n",
    "        if not (settings.OPENAI_API_KEY and settings.DEFAULT_OPENAI_GENERATIVE_MODEL):\n",
    "            logger.error(\"SYNTHESIS EVAL ERROR: Configuration OpenAI incomplète pour le LLM juge (OPENAI_API_KEY ou nom du modèle).\")\n",
    "            can_proceed = False\n",
    "    elif actual_judge_provider == \"huggingface_api\":\n",
    "        if not (settings.HUGGINGFACE_API_KEY and settings.HUGGINGFACE_REPO_ID): \n",
    "            logger.error(\"SYNTHESIS EVAL ERROR: Configuration HuggingFace API incomplète pour le LLM juge (HUGGINGFACE_API_KEY ou HUGGINGFACE_REPO_ID).\")\n",
    "            can_proceed = False\n",
    "    elif actual_judge_provider == \"ollama\":\n",
    "        if not (settings.OLLAMA_BASE_URL and settings.OLLAMA_GENERATIVE_MODEL_NAME): \n",
    "            logger.error(\"SYNTHESIS EVAL ERROR: Configuration Ollama incomplète pour le LLM juge (OLLAMA_BASE_URL ou OLLAMA_GENERATIVE_MODEL_NAME).\")\n",
    "            can_proceed = False\n",
    "    else:\n",
    "        logger.error(f\"SYNTHESIS EVAL ERROR: Fournisseur LLM Juge inconnu ou non supporté: '{actual_judge_provider}'\")\n",
    "        can_proceed = False\n",
    "        \n",
    "    if not can_proceed:\n",
    "        print(f\"ERREUR: Prérequis pour le LLM Juge (provider: {actual_judge_provider}) non remplis. Vérifiez les logs et votre configuration .env.\")\n",
    "        return None\n",
    "\n",
    "    sample_query_synth = \"What are the main challenges in robotic grasping using reinforcement learning?\"\n",
    "    sample_context_synth = \"\"\"\n",
    "Challenge 1: Sample Inefficiency. RL algorithms often require a vast amount of data (trials) to learn effective grasping policies. This is costly and time-consuming on physical robots.\n",
    "Challenge 2: Sim-to-Real Gap. Models trained in simulation may not transfer well to real robots due to differences in dynamics, sensing, and appearance.\n",
    "Challenge 3: Reward Design. Crafting appropriate reward functions that guide the agent towards successful and robust grasping without unintended behaviors is difficult.\n",
    "Challenge 4: High-Dimensional State/Action Spaces. Grasping involves continuous and high-dimensional inputs (e.g., camera images) and outputs (e.g., robot joint commands).\n",
    "(Source: Fictional summary based on general knowledge for demo purposes)\n",
    "\"\"\"\n",
    "    sample_synthesis_to_eval = \"\"\"\n",
    "Robotic grasping using reinforcement learning faces several key challenges. Firstly, sample inefficiency means many trials are needed.\n",
    "Secondly, bridging the sim-to-real gap is problematic due to mismatches between simulation and reality.\n",
    "Thirdly, designing effective reward functions is complex. Lastly, the high dimensionality of state and action spaces poses difficulties.\n",
    "These challenges are actively being researched.\n",
    "\"\"\"\n",
    "\n",
    "    config_for_wandb_synth = {\n",
    "        \"evaluation_type\": \"Synthesis_Notebook_Demo\",\n",
    "        \"original_query_snippet\": sample_query_synth[:100] + \"...\" if len(sample_query_synth) > 100 else sample_query_synth,\n",
    "        \"judge_llm_provider\": actual_judge_provider,\n",
    "        \"judge_llm_model_used\": actual_judge_model_name\n",
    "    }\n",
    "\n",
    "    wb_logger_synth = WandBMetricsLogger(\n",
    "        project_name=\"CognitiveSwarm-Evaluations-Notebook\",\n",
    "        run_name=f\"Synth_Eval_Demo_Judge_{actual_judge_provider}_{str(actual_judge_model_name).split('/')[-1].replace('.', '_')}\",\n",
    "        config_to_log=config_for_wandb_synth,\n",
    "        tags=[\"synthesis\", \"notebook_demo\", \"llm_as_judge\", actual_judge_provider]\n",
    "    )\n",
    "    \n",
    "    synthesis_metrics_result: Optional[SynthesisEvaluationResult] = None\n",
    "    run_started_by_this_func = False\n",
    "\n",
    "    try:\n",
    "        if not wb_logger_synth.is_disabled:\n",
    "            if wb_logger_synth.start_run():\n",
    "                run_started_by_this_func = True\n",
    "            else:\n",
    "                logger.warning(\"W&B run n'a pas pu démarrer pour l'évaluation de synthèse.\")\n",
    "        \n",
    "        evaluator_synth = SynthesisEvaluator(\n",
    "            judge_llm_provider=None, \n",
    "            judge_llm_model_name=judge_model_name_override \n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Évaluation de la synthèse pour la requête: '{sample_query_synth[:50]}...'\")\n",
    "        synthesis_metrics_result = await evaluator_synth.evaluate_synthesis(\n",
    "            query=sample_query_synth,\n",
    "            synthesis=sample_synthesis_to_eval,\n",
    "            context=sample_context_synth\n",
    "        )\n",
    "\n",
    "        if synthesis_metrics_result:\n",
    "            evaluator_synth.print_results(synthesis_metrics_result, query=sample_query_synth)\n",
    "            if run_started_by_this_func and wb_logger_synth.wandb_run:\n",
    "                wb_logger_synth.log_synthesis_evaluation_results(synthesis_metrics_result, eval_name=\"Synthesis_Demo_Eval\")\n",
    "                \n",
    "                synth_detail_data = [{\n",
    "                    \"query\": sample_query_synth,\n",
    "                    \"context_snippet\": sample_context_synth[:500] + \"...\" if len(sample_context_synth) > 500 else sample_context_synth,\n",
    "                    \"evaluated_synthesis\": sample_synthesis_to_eval,\n",
    "                    \"relevance_score\": synthesis_metrics_result.get(\"relevance\", {}).get(\"score\") if synthesis_metrics_result.get(\"relevance\") else None,\n",
    "                    \"relevance_reasoning\": synthesis_metrics_result.get(\"relevance\", {}).get(\"reasoning\") if synthesis_metrics_result.get(\"relevance\") else None,\n",
    "                    \"faithfulness_score\": synthesis_metrics_result.get(\"faithfulness\", {}).get(\"score\") if synthesis_metrics_result.get(\"faithfulness\") else None,\n",
    "                    \"faithfulness_reasoning\": synthesis_metrics_result.get(\"faithfulness\", {}).get(\"reasoning\") if synthesis_metrics_result.get(\"faithfulness\") else None,\n",
    "                }]\n",
    "                try:\n",
    "                    details_df_synth = pd.DataFrame(synth_detail_data)\n",
    "                    wb_logger_synth.log_dataframe_as_table(details_df_synth, \"Synthesis_Evaluation_Run_Detail_Demo\")\n",
    "                except Exception as e_df_s:\n",
    "                    logger.error(f\"Erreur lors du logging de la table de détails de synthèse: {e_df_s}\")\n",
    "        else:\n",
    "            logger.warning(\"L'évaluation de la synthèse n'a pas produit de métriques.\")\n",
    "\n",
    "    except ValueError as ve: \n",
    "        logger.error(f\"Erreur de configuration LLM pour SynthesisEvaluator: {ve}\", exc_info=True)\n",
    "        if run_started_by_this_func and wb_logger_synth.wandb_run:\n",
    "             wb_logger_synth.log_summary_metrics({\"synthesis_eval_error\": 1, \"error_message\": str(ve)[:250]})\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Une exception s'est produite pendant l'évaluation de la synthèse : {e}\", exc_info=True)\n",
    "        if run_started_by_this_func and wb_logger_synth.wandb_run:\n",
    "            wb_logger_synth.log_summary_metrics({\"synthesis_eval_error\": 1, \"error_message\": str(e)[:250]})\n",
    "    finally:\n",
    "        if run_started_by_this_func and wb_logger_synth.wandb_run:\n",
    "            wb_logger_synth.end_run()\n",
    "            logger.info(\"Run W&B pour l'évaluation de synthèse terminé.\")\n",
    "            \n",
    "    return synthesis_metrics_result\n",
    "\n",
    "# --- Exécution de la démo d'évaluation de synthèse ---\n",
    "print(\"Lancement de la démonstration d'évaluation de la synthèse...\")\n",
    "synthesis_evaluation_results = asyncio.run(run_synthesis_evaluation_notebook_demo(judge_model_name_override=None))\n",
    "\n",
    "if synthesis_evaluation_results:\n",
    "    print(\"\\n✅ Évaluation de la synthèse terminée avec succès et métriques produites.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ L'évaluation de la synthèse a échoué, n'a pas pu démarrer, ou n'a pas produit de métriques. Vérifiez les logs ci-dessus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aa849e",
   "metadata": {},
   "source": [
    "### 3. Démonstration de l'Évaluation de la Synthèse\n",
    "\n",
    "Nous allons maintenant évaluer la qualité d'une synthèse (pertinence, fidélité) en utilisant `SynthesisEvaluator` et un LLM comme juge.\n",
    "\n",
    "**Action Requise :** Pour cette section, vous devez fournir :\n",
    "1.  Une **requête utilisateur** (`sample_query_synth`).\n",
    "2.  Un **contexte** (`sample_context_synth`) qui aurait été fourni à l'agent de synthèse pour générer la réponse.\n",
    "3.  Une **synthèse à évaluer** (`sample_synthesis_to_eval`) qui est la sortie de votre `SynthesisAgent` pour cette requête et ce contexte.\n",
    "\n",
    "Vous pouvez obtenir ces éléments en exécutant le notebook `04_langgraph_workflow_design.ipynb` (ou `scripts/run_cognitive_swarm.py`) pour une requête donnée et en copiant/collant la requête, le contexte (qui peut être une combinaison des messages pertinents ou des chunks récupérés) et la sortie de synthèse finale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd18245b",
   "metadata": {},
   "source": [
    "### 4. Utilisation du Script `scripts/run_evaluation.py`\n",
    "\n",
    "Pour des évaluations plus complètes et automatisées, notamment sur des jeux de données plus larges, il est recommandé d'utiliser le script CLI `scripts/run_evaluation.py`.\n",
    "\n",
    "Ce script orchestre les `RagEvaluator` et `SynthesisEvaluator`, gère le chargement des datasets depuis des fichiers, et intègre le logging W&B de manière configurable.\n",
    "\n",
    "**Exemple de commande (à exécuter dans votre terminal, depuis la racine du projet) :**\n",
    "```bash\n",
    "python -m scripts.run_evaluation --eval_type all \\\n",
    "    --rag_dataset data/evaluation/rag_eval_dataset.json \\\n",
    "    --synthesis_dataset data/evaluation/synthesis_eval_dataset.json \\\n",
    "    --wandb_project \"CognitiveSwarm-MainEvals\" \\\n",
    "    --wandb_run_name \"Full_Eval_Run_$(date +%Y%m%d_%H%M)\" \\\n",
    "    --wandb_tags \"full_eval,scheduled\" \\\n",
    "    --log_level INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c787b49b",
   "metadata": {},
   "source": [
    "## Conclusion de l'Évaluation et du Logging\n",
    "\n",
    "Ce notebook a montré comment :\n",
    "- Utiliser `RagEvaluator` pour évaluer la performance de récupération.\n",
    "- Utiliser `SynthesisEvaluator` avec un LLM comme juge pour évaluer la qualité des synthèses.\n",
    "- Intégrer ces évaluations avec `WandBMetricsLogger` pour un suivi sur Weights & Biases.\n",
    "\n",
    "L'évaluation continue est une partie essentielle du développement de systèmes LLM robustes. Les métriques collectées peuvent guider les améliorations des prompts, des stratégies RAG, des modèles LLM choisis, etc.\n",
    "\n",
    "N'oubliez pas de consulter votre tableau de bord Weights & Biases pour visualiser les résultats loggés !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognitive-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
