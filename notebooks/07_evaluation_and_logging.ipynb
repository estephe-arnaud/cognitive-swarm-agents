{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb885dcb",
   "metadata": {},
   "source": [
    "# Notebook 07: Évaluation du Système et Logging avec Weights & Biases\n",
    "\n",
    "Ce notebook montre comment utiliser nos modules d'évaluation (`RagEvaluator`, `SynthesisEvaluator`) pour mesurer la performance de \"Cognitive Swarm\" et comment intégrer ces résultats avec Weights & Biases (W&B) en utilisant `WandBMetricsLogger`.\n",
    "\n",
    "**Prérequis :**\n",
    "* Environnement configuré (`00_setup_environment.ipynb`), y compris les clés API pour OpenAI et W&B dans `.env`.\n",
    "* MongoDB accessible et potentiellement peuplé avec des données via `01_data_ingestion_and_embedding.ipynb` (pour que l'évaluation RAG soit significative).\n",
    "* Les bibliothèques `wandb` et `pandas` doivent être installées (elles le sont si `environment.yml` a été utilisé).\n",
    "* Avoir créé des jeux de données d'évaluation (ou utiliser les exemples par défaut/simplifiés) :\n",
    "    * `rag_eval_dataset.json` pour `RagEvaluator`.\n",
    "    * Un jeu de données pour `SynthesisEvaluator` contenant des triplets `(query, context, synthesis_to_evaluate)`.\n",
    "\n",
    "**Étapes :**\n",
    "1. Configuration initiale (imports, logging, préparation W&B).\n",
    "2. Démonstration de l'évaluation RAG avec logging W&B.\n",
    "3. Démonstration de l'évaluation de la Synthèse avec logging W&B.\n",
    "4. Discussion sur l'utilisation de `scripts/run_evaluation.py` pour une orchestration complète."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67db443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import pandas as pd # Pour créer des tables pour W&B\n",
    "from typing import Optional, List, Dict, Any # Ajout de List, Dict, Any\n",
    "\n",
    "# Ajout de la racine du projet au PYTHONPATH\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "    print(f\"Ajout de {project_root} au PYTHONPATH\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "dotenv_path = project_root / \".env\"\n",
    "if dotenv_path.exists():\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    print(f\"Variables d'environnement chargées depuis : {dotenv_path}\")\n",
    "else:\n",
    "    print(f\"ATTENTION: Fichier .env non trouvé à {dotenv_path}.\")\n",
    "\n",
    "from config.settings import settings\n",
    "from config.logging_config import setup_logging\n",
    "\n",
    "# Importer les modules d'évaluation et le logger W&B\n",
    "from src.rag.retrieval_engine import RetrievalEngine # Nécessaire pour RagEvaluator\n",
    "from src.evaluation.rag_evaluator import RagEvaluator, RagEvaluationMetrics\n",
    "from src.evaluation.synthesis_evaluator import SynthesisEvaluator, SynthesisEvaluationResult, EvaluationAspectScore # Ajout de EvaluationAspectScore\n",
    "# L'import de SynthesisEvalItem n'est pas utilisé directement ici, mais est défini dans le script run_evaluation.py\n",
    "from src.evaluation.metrics_logger import WandBMetricsLogger\n",
    "from src.vector_store.mongodb_manager import MongoDBManager # Pour les noms de collection par défaut\n",
    "\n",
    "setup_logging(level=\"INFO\")\n",
    "logger = logging.getLogger(\"nb_07_evaluation_logging\")\n",
    "\n",
    "# --- Vérifications critiques pour ce notebook ---\n",
    "active_embedding_provider = settings.DEFAULT_EMBEDDING_PROVIDER.lower()\n",
    "if active_embedding_provider == \"openai\" and not settings.OPENAI_API_KEY:\n",
    "    logger.error(\"ERREUR : Le fournisseur d'embedding est 'openai', mais OPENAI_API_KEY n'est pas configurée. L'évaluation RAG (via RetrievalEngine) échouera.\")\n",
    "elif active_embedding_provider == \"ollama\" and not settings.OLLAMA_BASE_URL:\n",
    "    logger.error(\"ERREUR : Le fournisseur d'embedding est 'ollama', mais OLLAMA_BASE_URL n'est pas configurée. L'évaluation RAG échouera.\")\n",
    "\n",
    "active_llm_judge_provider = settings.DEFAULT_LLM_MODEL_PROVIDER.lower()\n",
    "if active_llm_judge_provider == \"openai\" and not settings.OPENAI_API_KEY:\n",
    "    logger.error(\"ERREUR : Le fournisseur de LLM (pour le juge de synthèse) est 'openai', mais OPENAI_API_KEY n'est pas configurée. L'évaluation de synthèse échouera.\")\n",
    "elif active_llm_judge_provider == \"huggingface_api\" and not settings.HUGGINGFACE_API_KEY:\n",
    "    logger.error(\"ERREUR : Le fournisseur de LLM (pour le juge de synthèse) est 'huggingface_api', mais HUGGINGFACE_API_KEY n'est pas configurée. L'évaluation de synthèse échouera.\")\n",
    "elif active_llm_judge_provider == \"ollama\" and not settings.OLLAMA_BASE_URL:\n",
    "    logger.error(\"ERREUR : Le fournisseur de LLM (pour le juge de synthèse) est 'ollama', mais OLLAMA_BASE_URL n'est pas configurée. L'évaluation de synthèse échouera.\")\n",
    "\n",
    "if not settings.WANDB_API_KEY and not os.environ.get(\"WANDB_API_KEY\"):\n",
    "    logger.warning(\"WANDB_API_KEY non trouvé. Le logging sur W&B pourrait échouer ou demander une authentification interactive.\")\n",
    "if not settings.MONGO_URI:\n",
    "    logger.error(\"MONGO_URI non configuré. RetrievalEngine ne pourra pas fonctionner pour l'évaluation RAG.\")\n",
    "\n",
    "# Nom de la collection pour les tests RAG (doit correspondre à ce qui a été ingéré)\n",
    "COLLECTION_NAME_EVAL = getattr(settings, 'COLLECTION_NAME_NOTEBOOK', MongoDBManager.DEFAULT_CHUNK_COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73430684",
   "metadata": {},
   "source": [
    "### 1. Connexion à Weights & Biases\n",
    "\n",
    "Avant de commencer les évaluations, nous allons nous assurer que nous pouvons nous connecter à W&B. Le `WandBMetricsLogger` s'en chargera. Si vous n'êtes pas déjà connecté via le CLI (`wandb login`), la présence de `WANDB_API_KEY` dans votre `.env` est fortement recommandée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64021862",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_rag_evaluation_notebook_demo(top_k_eval=3):\n",
    "    logger.info(f\"--- Début de l'Évaluation RAG (k={top_k_eval}) ---\")\n",
    "    \n",
    "    # Vérification des prérequis pour le fournisseur d'embedding actif\n",
    "    active_embedding_provider_check = settings.DEFAULT_EMBEDDING_PROVIDER.lower()\n",
    "    can_proceed = True\n",
    "    if active_embedding_provider_check == \"openai\" and not settings.OPENAI_API_KEY:\n",
    "        logger.error(\"RAG EVAL ERROR: OpenAI API Key manquant pour le fournisseur d'embedding OpenAI.\")\n",
    "        can_proceed = False\n",
    "    elif active_embedding_provider_check == \"ollama\" and not settings.OLLAMA_BASE_URL:\n",
    "        logger.error(\"RAG EVAL ERROR: OLLAMA_BASE_URL manquant pour le fournisseur d'embedding Ollama.\")\n",
    "        can_proceed = False\n",
    "    if not settings.MONGO_URI: # Nécessaire pour RetrievalEngine\n",
    "        logger.error(\"RAG EVAL ERROR: MONGO_URI manquant.\")\n",
    "        can_proceed = False\n",
    "    if not can_proceed:\n",
    "        return\n",
    "\n",
    "    # Configuration pour W&B\n",
    "    config_for_wandb_rag = {\n",
    "        \"evaluation_type\": \"RAG_Notebook_Demo\",\n",
    "        \"retrieval_top_k_eval\": top_k_eval,\n",
    "        \"rag_dataset_path\": str(settings.EVALUATION_DATASET_PATH) if settings.EVALUATION_DATASET_PATH else \"Internal Default\",\n",
    "        \"mongo_collection\": COLLECTION_NAME_EVAL, # Défini dans la cellule précédente\n",
    "        \"active_embedding_provider\": active_embedding_provider_check\n",
    "    }\n",
    "    if active_embedding_provider_check == \"openai\":\n",
    "        config_for_wandb_rag[\"active_embedding_model\"] = settings.OPENAI_EMBEDDING_MODEL_NAME\n",
    "        config_for_wandb_rag[\"active_embedding_dimension\"] = settings.OPENAI_EMBEDDING_DIMENSION\n",
    "    elif active_embedding_provider_check == \"huggingface\":\n",
    "        config_for_wandb_rag[\"active_embedding_model\"] = settings.HUGGINGFACE_EMBEDDING_MODEL_NAME\n",
    "        config_for_wandb_rag[\"active_embedding_dimension\"] = settings.HUGGINGFACE_EMBEDDING_MODEL_DIMENSION\n",
    "    elif active_embedding_provider_check == \"ollama\":\n",
    "        config_for_wandb_rag[\"active_embedding_model\"] = settings.OLLAMA_EMBEDDING_MODEL_NAME\n",
    "        config_for_wandb_rag[\"active_embedding_dimension\"] = settings.OLLAMA_EMBEDDING_MODEL_DIMENSION\n",
    "\n",
    "    wb_logger = WandBMetricsLogger(\n",
    "        project_name=\"CognitiveSwarm-Evaluations-Notebook\",\n",
    "        run_name=f\"RAG_Eval_Demo_k{top_k_eval}_{active_embedding_provider_check}\",\n",
    "        config_to_log=config_for_wandb_rag,\n",
    "        tags=[\"rag\", \"notebook_demo\", f\"k{top_k_eval}\", active_embedding_provider_check]\n",
    "    )\n",
    "\n",
    "    if wb_logger.is_disabled:\n",
    "        logger.warning(\"Logging W&B désactivé. Les métriques ne seront pas envoyées à W&B.\")\n",
    "    else:\n",
    "        wb_logger.start_run()\n",
    "\n",
    "    retrieval_engine_instance_rag = None\n",
    "    try:\n",
    "        retrieval_engine_instance_rag = RetrievalEngine(collection_name=COLLECTION_NAME_EVAL)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Échec d'initialisation du RetrievalEngine : {e}\", exc_info=True)\n",
    "        if wb_logger and not wb_logger.is_disabled and wb_logger.wandb_run: wb_logger.end_run(exit_code=1)\n",
    "        return\n",
    "\n",
    "    rag_eval_dataset_path = Path(settings.EVALUATION_DATASET_PATH) if settings.EVALUATION_DATASET_PATH else None\n",
    "    # RagEvaluator utilise son propre dataset par défaut si rag_eval_dataset_path est None ou si le fichier n'est pas trouvé\n",
    "    evaluator_rag = RagEvaluator(\n",
    "        retrieval_engine=retrieval_engine_instance_rag,\n",
    "        eval_dataset_path=rag_eval_dataset_path \n",
    "    )\n",
    "    \n",
    "    # Mettre à jour la config W&B avec le chemin réel utilisé par l'évaluateur si différent\n",
    "    if wb_logger and not wb_logger.is_disabled and wb_logger.wandb_run:\n",
    "        actual_dataset_path_logged = str(evaluator_rag.eval_dataset_path) if evaluator_rag.eval_dataset_path else \"Internal Default\"\n",
    "        if config_for_wandb_rag.get(\"rag_dataset_path\") != actual_dataset_path_logged:\n",
    "            wb_logger.log_configuration({\"rag_dataset_path_actual\": actual_dataset_path_logged})\n",
    "\n",
    "\n",
    "    if not evaluator_rag.eval_dataset:\n",
    "        logger.error(\"Jeu de données d'évaluation RAG vide. Arrêt.\")\n",
    "        if wb_logger and not wb_logger.is_disabled and wb_logger.wandb_run: wb_logger.end_run(exit_code=1)\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Évaluation RAG avec {len(evaluator_rag.eval_dataset)} requêtes (Dataset: {actual_dataset_path_logged}).\")\n",
    "    rag_metrics = evaluator_rag.evaluate(k=top_k_eval)\n",
    "\n",
    "    if rag_metrics:\n",
    "        evaluator_rag.print_results(rag_metrics)\n",
    "        if wb_logger and not wb_logger.is_disabled and wb_logger.wandb_run:\n",
    "            wb_logger.log_rag_evaluation_results(rag_metrics, eval_name=f\"RAG_Demo_Eval_k{top_k_eval}\")\n",
    "            \n",
    "            eval_details_list = []\n",
    "            for item in evaluator_rag.eval_dataset:\n",
    "                eval_details_list.append({\n",
    "                    \"query_id\": item[\"query_id\"],\n",
    "                    \"query_text\": item[\"query_text\"],\n",
    "                    \"expected_relevant_chunk_ids\": \", \".join(item[\"expected_relevant_chunk_ids\"])\n",
    "                })\n",
    "            if eval_details_list:\n",
    "                try:\n",
    "                    details_df = pd.DataFrame(eval_details_list)\n",
    "                    wb_logger.log_dataframe_as_table(details_df, \"RAG_Evaluation_Queries_Used\")\n",
    "                except ImportError: # pandas est importé au début, mais bonne pratique de garder\n",
    "                    logger.warning(\"Pandas n'est pas installé, impossible de logger la table de détails RAG.\")\n",
    "                except Exception as e_df:\n",
    "                    logger.error(f\"Erreur lors du logging de la table de détails RAG : {e_df}\")\n",
    "    else:\n",
    "        logger.warning(\"L'évaluation RAG n'a pas produit de métriques.\")\n",
    "\n",
    "    if wb_logger and not wb_logger.is_disabled and wb_logger.wandb_run:\n",
    "        wb_logger.end_run()\n",
    "\n",
    "# Exécuter la démo d'évaluation RAG\n",
    "asyncio.run(run_rag_evaluation_notebook_demo(top_k_eval=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc896710",
   "metadata": {},
   "source": [
    "### 2. Démonstration de l'Évaluation RAG\n",
    "\n",
    "Nous allons instancier `RetrievalEngine` et `RagEvaluator`, exécuter une évaluation sur un petit jeu de données (le jeu par défaut de `RagEvaluator` ou un fichier JSON si configuré), puis logger les résultats et la configuration sur W&B.\n",
    "\n",
    "**Action Requise :** Pour des résultats significatifs, assurez-vous d'avoir un fichier `rag_eval_dataset.json` (chemin configurable via `settings.EVALUATION_DATASET_PATH`) contenant des requêtes et les `chunk_id` pertinents attendus pour votre corpus. Le `RagEvaluator` utilisera son jeu de données de démo interne si ce fichier n'est pas trouvé ou non spécifié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e59a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_synthesis_evaluation_notebook_demo(judge_model_name_override: Optional[str] = None):\n",
    "    # Déterminer le provider et le nom du modèle LLM Juge qui sera utilisé\n",
    "    # SynthesisEvaluator utilise get_llm, qui prendra DEFAULT_LLM_MODEL_PROVIDER par défaut\n",
    "    actual_judge_provider = settings.DEFAULT_LLM_MODEL_PROVIDER.lower()\n",
    "    actual_judge_model_name = judge_model_name_override # L'override du nom du modèle pour le provider par défaut\n",
    "    \n",
    "    if not actual_judge_model_name: # Si pas d'override, prendre le modèle par défaut du provider\n",
    "        if actual_judge_provider == \"openai\":\n",
    "            actual_judge_model_name = settings.DEFAULT_OPENAI_GENERATIVE_MODEL\n",
    "        elif actual_judge_provider == \"huggingface_api\":\n",
    "            actual_judge_model_name = settings.HUGGINGFACE_REPO_ID\n",
    "        elif actual_judge_provider == \"ollama\":\n",
    "            actual_judge_model_name = settings.OLLAMA_GENERATIVE_MODEL_NAME\n",
    "        else:\n",
    "            actual_judge_model_name = \"Unknown_Default_Model\"\n",
    "            logger.warning(f\"Provider LLM Juge inconnu: {actual_judge_provider}. Le nom du modèle pourrait être incorrect.\")\n",
    "            \n",
    "    logger.info(f\"--- Début de l'Évaluation de la Synthèse (Juge Provider: {actual_judge_provider}, Juge Model: {actual_judge_model_name}) ---\")\n",
    "\n",
    "    # Vérification des prérequis pour le LLM Juge\n",
    "    can_proceed = True\n",
    "    if actual_judge_provider == \"openai\" and not settings.OPENAI_API_KEY:\n",
    "        logger.error(\"SYNTHESIS EVAL ERROR: OpenAI API Key manquant pour le LLM juge OpenAI.\")\n",
    "        can_proceed = False\n",
    "    elif actual_judge_provider == \"huggingface_api\" and not settings.HUGGINGFACE_API_KEY:\n",
    "        logger.error(\"SYNTHESIS EVAL ERROR: HuggingFace API Key manquant pour le LLM juge HuggingFace API.\")\n",
    "        can_proceed = False\n",
    "    elif actual_judge_provider == \"ollama\":\n",
    "        if not settings.OLLAMA_BASE_URL:\n",
    "            logger.error(\"SYNTHESIS EVAL ERROR: OLLAMA_BASE_URL manquant pour le LLM juge Ollama.\")\n",
    "            can_proceed = False\n",
    "        if not settings.OLLAMA_GENERATIVE_MODEL_NAME and not judge_model_name_override: # Si aucun nom de modèle n'est défini pour Ollama\n",
    "             logger.error(f\"SYNTHESIS EVAL ERROR: Nom du modèle Ollama pour le juge non défini (OLLAMA_GENERATIVE_MODEL_NAME ou override).\")\n",
    "             can_proceed = False\n",
    "    if not can_proceed:\n",
    "        return\n",
    "\n",
    "    # --- PRÉPARER VOS DONNÉES DE TEST ICI ---\n",
    "    sample_query_synth = \"What are the main challenges in robotic grasping using reinforcement learning?\"\n",
    "    sample_context_synth = \"\"\"\n",
    "Challenge 1: Sample Inefficiency. RL algorithms often require a vast amount of data (trials) to learn effective grasping policies. This is costly and time-consuming on physical robots.\n",
    "Challenge 2: Sim-to-Real Gap. Models trained in simulation may not transfer well to real robots due to differences in dynamics, sensing, and appearance.\n",
    "Challenge 3: Reward Design. Crafting appropriate reward functions that guide the agent towards successful and robust grasping without unintended behaviors is difficult.\n",
    "Challenge 4: High-Dimensional State/Action Spaces. Grasping involves continuous and high-dimensional inputs (e.g., camera images) and outputs (e.g., robot joint commands).\n",
    "(Source: Fictional summary based on general knowledge for demo purposes)\n",
    "\"\"\"\n",
    "    sample_synthesis_to_eval = \"\"\"\n",
    "Robotic grasping using reinforcement learning faces several key challenges. Firstly, sample inefficiency means many trials are needed.\n",
    "Secondly, bridging the sim-to-real gap is problematic due to mismatches between simulation and reality.\n",
    "Thirdly, designing effective reward functions is complex. Lastly, the high dimensionality of state and action spaces poses difficulties.\n",
    "These challenges are actively being researched.\n",
    "\"\"\"\n",
    "    # -----------------------------------------\n",
    "\n",
    "    config_for_wandb_synth = {\n",
    "        \"evaluation_type\": \"Synthesis_Notebook_Demo\",\n",
    "        \"original_query_snippet\": sample_query_synth[:100] + \"...\" if len(sample_query_synth) > 100 else sample_query_synth,\n",
    "        \"judge_llm_provider\": actual_judge_provider,\n",
    "        \"judge_llm_model_used\": actual_judge_model_name\n",
    "    }\n",
    "\n",
    "    wb_logger_synth = WandBMetricsLogger(\n",
    "        project_name=\"CognitiveSwarm-Evaluations-Notebook\",\n",
    "        run_name=f\"Synth_Eval_Demo_Judge_{actual_judge_provider}_{str(actual_judge_model_name).split('/')[-1].replace('.', '_')}\",\n",
    "        config_to_log=config_for_wandb_synth,\n",
    "        tags=[\"synthesis\", \"notebook_demo\", \"llm_as_judge\", actual_judge_provider]\n",
    "    )\n",
    "    if wb_logger_synth.is_disabled:\n",
    "        logger.warning(\"Logging W&B désactivé. Les métriques de synthèse ne seront pas envoyées à W&B.\")\n",
    "    else:\n",
    "        wb_logger_synth.start_run()\n",
    "    \n",
    "    evaluator_synth = None\n",
    "    try:\n",
    "        evaluator_synth = SynthesisEvaluator(\n",
    "            judge_llm_provider=None, \n",
    "            judge_llm_model_name=judge_model_name_override \n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Échec d'initialisation du SynthesisEvaluator: {e}\", exc_info=True)\n",
    "        if wb_logger_synth and not wb_logger_synth.is_disabled and wb_logger_synth.wandb_run: wb_logger_synth.end_run(exit_code=1)\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Évaluation de la synthèse pour la requête: '{sample_query_synth[:50]}...'\")\n",
    "    synthesis_metrics: Optional[SynthesisEvaluationResult] = await evaluator_synth.evaluate_synthesis(\n",
    "        query=sample_query_synth,\n",
    "        synthesis=sample_synthesis_to_eval,\n",
    "        context=sample_context_synth\n",
    "    )\n",
    "\n",
    "    if synthesis_metrics:\n",
    "        evaluator_synth.print_results(synthesis_metrics, query=sample_query_synth)\n",
    "        if wb_logger_synth and not wb_logger_synth.is_disabled and wb_logger_synth.wandb_run:\n",
    "            wb_logger_synth.log_synthesis_evaluation_results(synthesis_metrics, eval_name=\"Synthesis_Demo_Eval\")\n",
    "            \n",
    "            synth_detail_data = [{\n",
    "                \"query\": sample_query_synth,\n",
    "                \"context_snippet\": sample_context_synth[:500] + \"...\" if len(sample_context_synth) > 500 else sample_context_synth,\n",
    "                \"evaluated_synthesis\": sample_synthesis_to_eval,\n",
    "                \"relevance_score\": synthesis_metrics.get(\"relevance\", {}).get(\"score\") if synthesis_metrics.get(\"relevance\") else None,\n",
    "                \"relevance_reasoning\": synthesis_metrics.get(\"relevance\", {}).get(\"reasoning\") if synthesis_metrics.get(\"relevance\") else None,\n",
    "                \"faithfulness_score\": synthesis_metrics.get(\"faithfulness\", {}).get(\"score\") if synthesis_metrics.get(\"faithfulness\") else None,\n",
    "                \"faithfulness_reasoning\": synthesis_metrics.get(\"faithfulness\", {}).get(\"reasoning\") if synthesis_metrics.get(\"faithfulness\") else None,\n",
    "            }]\n",
    "            try:\n",
    "                details_df_synth = pd.DataFrame(synth_detail_data)\n",
    "                wb_logger_synth.log_dataframe_as_table(details_df_synth, \"Synthesis_Evaluation_Run_Detail\")\n",
    "            except ImportError: \n",
    "                logger.warning(\"Pandas n'est pas installé, impossible de logger la table de détails de synthèse.\")\n",
    "            except Exception as e_df_s:\n",
    "                logger.error(f\"Erreur lors du logging de la table de détails de synthèse: {e_df_s}\")\n",
    "\n",
    "    if wb_logger_synth and not wb_logger_synth.is_disabled and wb_logger_synth.wandb_run:\n",
    "        wb_logger_synth.end_run()\n",
    "\n",
    "# Exécuter la démo d'évaluation de synthèse\n",
    "# Vous pouvez passer un nom de modèle pour surcharger le juge : par exemple, judge_model_name_override=\"gpt-4o\"\n",
    "# Si None, il utilisera le modèle génératif par défaut du provider configuré dans settings.py\n",
    "asyncio.run(run_synthesis_evaluation_notebook_demo(judge_model_name_override=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aa849e",
   "metadata": {},
   "source": [
    "### 3. Démonstration de l'Évaluation de la Synthèse\n",
    "\n",
    "Nous allons maintenant évaluer la qualité d'une synthèse (pertinence, fidélité) en utilisant `SynthesisEvaluator` et un LLM comme juge.\n",
    "\n",
    "**Action Requise :** Pour cette section, vous devez fournir :\n",
    "1.  Une **requête utilisateur** (`sample_query_synth`).\n",
    "2.  Un **contexte** (`sample_context_synth`) qui aurait été fourni à l'agent de synthèse pour générer la réponse.\n",
    "3.  Une **synthèse à évaluer** (`sample_synthesis_to_eval`) qui est la sortie de votre `SynthesisAgent` pour cette requête et ce contexte.\n",
    "\n",
    "Vous pouvez obtenir ces éléments en exécutant le notebook `04_langgraph_workflow_design.ipynb` (ou `scripts/run_cognitive_swarm.py`) pour une requête donnée et en copiant/collant la requête, le contexte (qui peut être une combinaison des messages pertinents ou des chunks récupérés) et la sortie de synthèse finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeea25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_synthesis_evaluation_notebook_demo(judge_model_override: Optional[str] = None):\n",
    "    logger.info(f\"--- Début de l'Évaluation de la Synthèse (Juge: {judge_model_override or settings.DEFAULT_OPENAI_MODEL}) ---\")\n",
    "\n",
    "    if not settings.OPENAI_API_KEY: # Nécessaire pour le LLM Juge par défaut\n",
    "        logger.error(\"Clé API OpenAI non configurée. Impossible d'exécuter l'évaluation de la synthèse.\")\n",
    "        return\n",
    "\n",
    "    # --- PRÉPARER VOS DONNÉES DE TEST ICI ---\n",
    "    sample_query_synth = \"What are the main challenges in robotic grasping using reinforcement learning?\"\n",
    "    sample_context_synth = \"\"\"\n",
    "    Challenge 1: Sample Inefficiency. RL algorithms often require a vast amount of data (trials) to learn effective grasping policies. This is costly and time-consuming on physical robots.\n",
    "    Challenge 2: Sim-to-Real Gap. Models trained in simulation may not transfer well to real robots due to differences in dynamics, sensing, and appearance.\n",
    "    Challenge 3: Reward Design. Crafting appropriate reward functions that guide the agent towards successful and robust grasping without unintended behaviors is difficult.\n",
    "    Challenge 4: High-Dimensional State/Action Spaces. Grasping involves continuous and high-dimensional inputs (e.g., camera images) and outputs (e.g., robot joint commands).\n",
    "    (Source: Fictional summary based on general knowledge for demo purposes)\n",
    "    \"\"\"\n",
    "    # Cette synthèse est un exemple. Remplacez-la par une VRAIE sortie de votre SynthesisAgent.\n",
    "    sample_synthesis_to_eval = \"\"\"\n",
    "    Robotic grasping using reinforcement learning faces several key challenges. Firstly, sample inefficiency means many trials are needed.\n",
    "    Secondly, bridging the sim-to-real gap is problematic due to mismatches between simulation and reality.\n",
    "    Thirdly, designing effective reward functions is complex. Lastly, the high dimensionality of state and action spaces poses difficulties.\n",
    "    These challenges are actively being researched.\n",
    "    \"\"\"\n",
    "    # -----------------------------------------\n",
    "\n",
    "    # Initialiser le logger W&B\n",
    "    wb_logger_synth = WandBMetricsLogger(\n",
    "        project_name=\"CognitiveSwarm-Evaluations-Notebook\",\n",
    "        run_name=f\"Synthesis_Eval_Demo_Judge_{judge_model_override or settings.DEFAULT_OPENAI_MODEL.replace('.', '_')}\",\n",
    "        config_to_log={\n",
    "            \"evaluation_type\": \"Synthesis\",\n",
    "            \"original_query\": sample_query_synth,\n",
    "            \"judge_llm\": judge_model_override or settings.DEFAULT_OPENAI_MODEL,\n",
    "            # On pourrait aussi logger un hash ou un ID du contexte/synthèse pour la traçabilité\n",
    "        },\n",
    "        tags=[\"synthesis\", \"notebook_demo\", \"llm_as_judge\"]\n",
    "    )\n",
    "    if wb_logger_synth.is_disabled:\n",
    "        logger.warning(\"Logging W&B désactivé. Les métriques de synthèse ne seront pas envoyées à W&B.\")\n",
    "    else:\n",
    "        wb_logger_synth.start_run()\n",
    "    \n",
    "    evaluator_synth = None\n",
    "    try:\n",
    "        evaluator_synth = SynthesisEvaluator(judge_llm_model_name=judge_model_override)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Échec d'initialisation du SynthesisEvaluator: {e}\", exc_info=True)\n",
    "        if wb_logger_synth and not wb_logger_synth.is_disabled: wb_logger_synth.end_run(exit_code=1)\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Évaluation de la synthèse pour la requête: '{sample_query_synth[:50]}...'\")\n",
    "    synthesis_metrics = await evaluator_synth.evaluate_synthesis(\n",
    "        query=sample_query_synth,\n",
    "        synthesis=sample_synthesis_to_eval,\n",
    "        context=sample_context_synth\n",
    "    )\n",
    "\n",
    "    evaluator_synth.print_results(synthesis_metrics, query=sample_query_synth)\n",
    "\n",
    "    if wb_logger_synth and not wb_logger_synth.is_disabled and wb_logger_synth.wandb_run:\n",
    "        wb_logger_synth.log_synthesis_evaluation_results(synthesis_metrics, eval_name=\"Synthesis_Demo_Eval\")\n",
    "        \n",
    "        # Logger les détails (requête, contexte, synthèse, scores, raisons) dans une table W&B\n",
    "        synth_detail_data = [{\n",
    "            \"query\": sample_query_synth,\n",
    "            \"context_snippet\": sample_context_synth[:500] + \"...\", # Extrait du contexte\n",
    "            \"evaluated_synthesis\": sample_synthesis_to_eval,\n",
    "            \"relevance_score\": synthesis_metrics.get(\"relevance\", {}).get(\"score\") if synthesis_metrics.get(\"relevance\") else None,\n",
    "            \"relevance_reasoning\": synthesis_metrics.get(\"relevance\", {}).get(\"reasoning\") if synthesis_metrics.get(\"relevance\") else None,\n",
    "            \"faithfulness_score\": synthesis_metrics.get(\"faithfulness\", {}).get(\"score\") if synthesis_metrics.get(\"faithfulness\") else None,\n",
    "            \"faithfulness_reasoning\": synthesis_metrics.get(\"faithfulness\", {}).get(\"reasoning\") if synthesis_metrics.get(\"faithfulness\") else None,\n",
    "        }]\n",
    "        try:\n",
    "            details_df_synth = pd.DataFrame(synth_detail_data)\n",
    "            wb_logger_synth.log_dataframe_as_table(details_df_synth, \"Synthesis_Evaluation_Run_Detail\")\n",
    "        except ImportError:\n",
    "             logger.warning(\"Pandas not installé, impossible de logger la table de détails de synthèse.\")\n",
    "        except Exception as e_df_s:\n",
    "             logger.error(f\"Erreur lors du logging de la table de détails de synthèse: {e_df_s}\")\n",
    "\n",
    "\n",
    "    if wb_logger_synth and not wb_logger_synth.is_disabled and wb_logger_synth.wandb_run:\n",
    "        wb_logger_synth.end_run()\n",
    "\n",
    "# Exécuter la démo d'évaluation de synthèse\n",
    "asyncio.run(run_synthesis_evaluation_notebook_demo())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5efb32e",
   "metadata": {},
   "source": [
    "### 4. Utilisation du Script `scripts/run_evaluation.py`\n",
    "\n",
    "Pour des évaluations plus complètes et automatisées, notamment sur des jeux de données plus larges, il est recommandé d'utiliser le script CLI `scripts/run_evaluation.py`.\n",
    "\n",
    "Ce script orchestre les `RagEvaluator` et `SynthesisEvaluator`, gère le chargement des datasets depuis des fichiers, et intègre le logging W&B de manière configurable.\n",
    "\n",
    "**Exemple de commande (à exécuter dans votre terminal, depuis la racine du projet) :**\n",
    "```bash\n",
    "python -m scripts.run_evaluation --eval_type all \\\n",
    "    --rag_dataset data/evaluation/rag_eval_dataset.json \\\n",
    "    --synthesis_dataset data/evaluation/synthesis_eval_dataset.json \\\n",
    "    --wandb_project \"CognitiveSwarm-MainEvals\" \\\n",
    "    --wandb_run_name \"Full_Eval_Run_$(date +%Y%m%d_%H%M)\" \\\n",
    "    --wandb_tags \"full_eval,scheduled\" \\\n",
    "    --log_level INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b33941",
   "metadata": {},
   "source": [
    "## Conclusion de l'Évaluation et du Logging\n",
    "\n",
    "Ce notebook a montré comment :\n",
    "- Utiliser `RagEvaluator` pour évaluer la performance de récupération.\n",
    "- Utiliser `SynthesisEvaluator` avec un LLM comme juge pour évaluer la qualité des synthèses.\n",
    "- Intégrer ces évaluations avec `WandBMetricsLogger` pour un suivi sur Weights & Biases.\n",
    "\n",
    "L'évaluation continue est une partie essentielle du développement de systèmes LLM robustes. Les métriques collectées peuvent guider les améliorations des prompts, des stratégies RAG, des modèles LLM choisis, etc.\n",
    "\n",
    "N'oubliez pas de consulter votre tableau de bord Weights & Biases pour visualiser les résultats loggés !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognitive-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
