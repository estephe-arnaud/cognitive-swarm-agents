{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "666a8bbe",
   "metadata": {},
   "source": [
    "# Notebook 04: Conception et Exécution du Workflow LangGraph\n",
    "\n",
    "Ce notebook est dédié à l'exécution et à l'observation de notre workflow multi-agents \"Cognitive Swarm\", tel que défini dans `src/graph/main_workflow.py`. Nous allons soumettre une requête complexe et suivre le déroulement des opérations à travers les différents agents et outils.\n",
    "\n",
    "**Prérequis :**\n",
    "* **Environnement de Base :** Avoir exécuté le notebook `00_setup_environment.ipynb` pour configurer l'environnement Conda, les dépendances Python, et s'assurer que le fichier `.env` à la racine du projet est correctement rempli.\n",
    "* **Base de Données MongoDB :**\n",
    "    * `MONGO_URI` doit être correctement configuré dans `.env` et votre instance MongoDB doit être accessible. Ceci est utilisé par le checkpointer LangGraph (`MongoDBSaver`) et les outils RAG.\n",
    "    * La base de données doit être peuplée avec des documents et leurs embeddings (via `01_data_ingestion_and_embedding.ipynb` ou `scripts/run_ingestion.py`). Les embeddings stockés doivent correspondre au fournisseur configuré via `DEFAULT_EMBEDDING_PROVIDER` pour que les outils RAG fonctionnent de manière optimale.\n",
    "* **Configuration des Fournisseurs de Modèles (dans `.env`) :** Le workflow utilisera les fournisseurs configurés via les variables `DEFAULT_LLM_MODEL_PROVIDER` (pour les agents) et `DEFAULT_EMBEDDING_PROVIDER` (pour la RAG et l'embedding des requêtes).\n",
    "    * **Pour les LLMs des Agents (`DEFAULT_LLM_MODEL_PROVIDER`) :**\n",
    "        * Si réglé sur `\"openai\"` : `OPENAI_API_KEY` est requise.\n",
    "        * Si réglé sur `\"huggingface_api\"` : `HUGGINGFACE_API_KEY` et `HUGGINGFACE_REPO_ID` (pour le modèle génératif) sont requis.\n",
    "        * Si réglé sur `\"ollama\"` : Assurez-vous que `OLLAMA_BASE_URL` pointe vers votre instance Ollama en cours d'exécution, et que `OLLAMA_GENERATIVE_MODEL_NAME` est un modèle que vous avez téléchargé (`ollama pull ...`).\n",
    "    * **Pour les Embeddings (`DEFAULT_EMBEDDING_PROVIDER`, utilisé par `RetrievalEngine`) :**\n",
    "        * Si réglé sur `\"openai\"` : `OPENAI_API_KEY` est requise (pour `LlamaIndex OpenAIEmbedding`).\n",
    "        * Si réglé sur `\"huggingface\"` (local Sentence Transformers) : Aucune clé API spécifique n'est généralement requise pour cette étape.\n",
    "        * Si réglé sur `\"ollama\"` : Assurez-vous que `OLLAMA_BASE_URL` est correct et que `OLLAMA_EMBEDDING_MODEL_NAME` est un modèle d'embedding disponible sur votre instance Ollama.\n",
    "* **(Optionnel) Weights & Biases :** Si vous souhaitez utiliser le logging des évaluations avec W&B (via `scripts/run_evaluation.py`, non directement testé dans ce notebook mais bon à savoir), `WANDB_API_KEY` doit être configurée dans `.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58690d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import asyncio # Pour exécuter notre fonction de workflow asynchrone\n",
    "import uuid    # Pour générer des thread_id uniques\n",
    "from typing import Dict, Any, List # Ajout pour pretty_print_final_state\n",
    "\n",
    "# --- Configuration du PYTHONPATH et chargement de .env ---\n",
    "# NOTE IMPORTANTE SUR LE CWD (Current Working Directory) :\n",
    "# La ligne suivante `project_root = Path().resolve().parent` suppose que le CWD du notebook\n",
    "# est le dossier `/notebooks/`. Si vous avez configuré VS Code pour que le CWD\n",
    "# soit la racine du projet (`cognitive-swarm-agents/`), alors `Path().resolve()`\n",
    "# donnerait déjà la racine du projet, et vous devriez utiliser :\n",
    "# project_root = Path().resolve()\n",
    "# Vérifiez votre CWD avec `import os; print(os.getcwd())` ou `from pathlib import Path; print(Path().resolve())` pour confirmer.\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "    print(f\"Ajout de {project_root} au PYTHONPATH\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# De même, si CWD est la racine du projet, dotenv_path serait `Path().resolve() / \".env\"`\n",
    "dotenv_path = project_root / \".env\"\n",
    "if dotenv_path.exists():\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    print(f\"Variables d'environnement chargées depuis : {dotenv_path}\")\n",
    "else:\n",
    "    print(f\"ATTENTION: Fichier .env non trouvé à {dotenv_path}. Assurez-vous qu'il est à la racine du projet.\")\n",
    "\n",
    "from config.settings import settings\n",
    "from config.logging_config import setup_logging\n",
    "\n",
    "# Importer la fonction d'exécution du workflow principal\n",
    "# graph_app_v2_1 est le graphe compilé, utile si on veut l'inspecter ou l'utiliser directement.\n",
    "from src.graph.main_workflow import run_cognitive_swarm_v2_1, graph_app_v2_1 \n",
    "\n",
    "# Configurer le logging pour le notebook\n",
    "LOG_LEVEL_NOTEBOOK = \"INFO\" # Changer en \"DEBUG\" pour un maximum de détails du workflow LangGraph\n",
    "setup_logging(level=LOG_LEVEL_NOTEBOOK) \n",
    "logger = logging.getLogger(\"nb_04_workflow_execution\")\n",
    "\n",
    "# --- Vérification des prérequis pour le Workflow (LLMs, Embeddings, MongoDB) ---\n",
    "logger.info(f\"--- Configuration Active pour le Workflow (depuis settings.py et .env) ---\")\n",
    "\n",
    "# 1. Pour les LLMs génératifs (utilisés par les agents du workflow via llm_factory.py)\n",
    "generative_llm_provider = settings.DEFAULT_LLM_MODEL_PROVIDER.lower()\n",
    "logger.info(f\"Fournisseur LLM génératif principal pour les agents : '{generative_llm_provider}'\")\n",
    "if generative_llm_provider == \"openai\":\n",
    "    if not settings.OPENAI_API_KEY:\n",
    "        logger.error(\"ERREUR : LLM Provider est 'openai', mais OPENAI_API_KEY n'est pas configurée.\")\n",
    "    if not settings.DEFAULT_OPENAI_GENERATIVE_MODEL:\n",
    "         logger.warning(\"AVERTISSEMENT : DEFAULT_OPENAI_GENERATIVE_MODEL n'est pas explicitement défini (utilisera le défaut de ChatOpenAI).\")\n",
    "elif generative_llm_provider == \"huggingface_api\":\n",
    "    if not settings.HUGGINGFACE_API_KEY:\n",
    "        logger.error(\"ERREUR : LLM Provider est 'huggingface_api', mais HUGGINGFACE_API_KEY n'est pas configurée.\")\n",
    "    if not settings.HUGGINGFACE_REPO_ID:\n",
    "        logger.error(\"ERREUR : LLM Provider est 'huggingface_api', mais HUGGINGFACE_REPO_ID n'est pas configuré.\")\n",
    "elif generative_llm_provider == \"ollama\":\n",
    "    if not settings.OLLAMA_BASE_URL:\n",
    "        logger.error(\"ERREUR : LLM Provider est 'ollama', mais OLLAMA_BASE_URL n'est pas configurée.\")\n",
    "    if not settings.OLLAMA_GENERATIVE_MODEL_NAME:\n",
    "        logger.error(\"ERREUR : LLM Provider est 'ollama', mais OLLAMA_GENERATIVE_MODEL_NAME n'est pas configuré.\")\n",
    "else:\n",
    "    logger.error(f\"ERREUR : Fournisseur LLM génératif inconnu ou non supporté : '{generative_llm_provider}'\")\n",
    "\n",
    "# 2. Pour les modèles d'Embedding (utilisés par RetrievalEngine via les outils)\n",
    "embedding_provider = settings.DEFAULT_EMBEDDING_PROVIDER.lower()\n",
    "logger.info(f\"Fournisseur d'Embedding (pour RAG via RetrievalEngine) : '{embedding_provider}'\")\n",
    "if embedding_provider == \"openai\":\n",
    "    if not settings.OPENAI_API_KEY: # Nécessaire pour LlamaIndex OpenAIEmbedding\n",
    "        logger.error(\"ERREUR : Embedding Provider est 'openai', mais OPENAI_API_KEY n'est pas configurée.\")\n",
    "    if not settings.OPENAI_EMBEDDING_MODEL_NAME:\n",
    "        logger.warning(\"AVERTISSEMENT : OPENAI_EMBEDDING_MODEL_NAME n'est pas explicitement défini.\")\n",
    "elif embedding_provider == \"huggingface\":\n",
    "    # Les embeddings HuggingFace locaux (SentenceTransformers) ne nécessitent pas de clé API.\n",
    "    if not settings.HUGGINGFACE_EMBEDDING_MODEL_NAME:\n",
    "        logger.error(\"ERREUR : Embedding Provider est 'huggingface', mais HUGGINGFACE_EMBEDDING_MODEL_NAME n'est pas configuré.\")\n",
    "elif embedding_provider == \"ollama\":\n",
    "    if not settings.OLLAMA_BASE_URL: # Partagé avec le LLM génératif Ollama\n",
    "        logger.error(\"ERREUR : Embedding Provider est 'ollama', mais OLLAMA_BASE_URL n'est pas configurée.\")\n",
    "    if not settings.OLLAMA_EMBEDDING_MODEL_NAME:\n",
    "        logger.error(\"ERREUR : Embedding Provider est 'ollama', mais OLLAMA_EMBEDDING_MODEL_NAME n'est pas configuré.\")\n",
    "else:\n",
    "    logger.error(f\"ERREUR : Fournisseur d'embedding inconnu ou non supporté : '{embedding_provider}'\")\n",
    "\n",
    "# 3. Pour MongoDB (utilisé par le checkpointer LangGraph et RetrievalEngine)\n",
    "if not settings.MONGO_URI or \"<user>\" in settings.MONGO_URI: # Ajout d'un check pour les placeholders\n",
    "    logger.error(\"ERREUR : MONGO_URI non trouvé ou semble non configuré (contient des placeholders). Le checkpointer et le RetrievalEngine (RAG) échoueront.\")\n",
    "else:\n",
    "    logger.info(f\"MongoDB URI configuré (début): {settings.MONGO_URI[:30]}...\")\n",
    "    logger.info(f\"Base de données MongoDB: {settings.MONGO_DATABASE_NAME}\")\n",
    "    logger.info(f\"Collection des checkpoints LangGraph: {settings.LANGGRAPH_CHECKPOINTS_COLLECTION}\")\n",
    "\n",
    "logger.info(\"--- Fin de la Vérification de Configuration Active ---\")\n",
    "\n",
    "# --- Petite fonction pour afficher l'état final de manière plus structurée (inchangée) ---\n",
    "# (La définition de pretty_print_final_state que vous aviez était bonne, je la garde telle quelle)\n",
    "def pretty_print_final_state(final_state: Dict[str, Any]): # S'assurer que Dict et Any sont importés de typing\n",
    "    print(\"\\n--- État Final Détaillé du Graphe ---\")\n",
    "    if not final_state:\n",
    "        print(\"Aucun état final retourné.\")\n",
    "        return\n",
    "            \n",
    "    for key, value in final_state.items():\n",
    "        if key == \"messages\":\n",
    "            print(f\"\\n  {key.upper()}:\")\n",
    "            if isinstance(value, list):\n",
    "                for i, msg in enumerate(value[-5:]): # Afficher les 5 derniers messages pour concision\n",
    "                    msg_type = getattr(msg, 'type', 'UNKNOWN_MSG_TYPE').upper()\n",
    "                    msg_name = getattr(msg, 'name', None)\n",
    "                    msg_content_str = str(getattr(msg, 'content', 'N/A'))\n",
    "                    display_name = f\"{msg_type} ({msg_name})\" if msg_name else msg_type\n",
    "                    \n",
    "                    print(f\"    Message {len(value) - 5 + i +1 if len(value)>5 else i+1}: [{display_name}]\")\n",
    "                    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                        print(f\"      Contenu: {msg_content_str[:100]}... [Appels d'outils: {len(msg.tool_calls)}]\")\n",
    "                        for tc in msg.tool_calls:\n",
    "                            # La structure de tc peut varier, adaptez au besoin si ce n'est pas un dict direct\n",
    "                            tc_args = tc.get('args') if isinstance(tc, dict) else getattr(tc, 'args', {})\n",
    "                            tc_name = tc.get('name') if isinstance(tc, dict) else getattr(tc, 'name', 'unknown_tool')\n",
    "                            tc_id = tc.get('id') if isinstance(tc, dict) else getattr(tc, 'id', None)\n",
    "                            print(f\"        Tool Call ID: {tc_id}, Name: {tc_name}, Args: {tc_args}\")\n",
    "                    elif msg_type == \"TOOL\": # ToolMessage\n",
    "                        tool_call_id = getattr(msg, 'tool_call_id', 'N/A')\n",
    "                        parsed_tool_content = None\n",
    "                        if isinstance(msg_content_str, str):\n",
    "                            try:\n",
    "                                parsed_tool_content = json.loads(msg_content_str)\n",
    "                            except json.JSONDecodeError:\n",
    "                                pass \n",
    "                        \n",
    "                        if parsed_tool_content and isinstance(parsed_tool_content, list) and parsed_tool_content:\n",
    "                            print(f\"      Tool Call ID: {tool_call_id} - Résultat Outil (Liste de {len(parsed_tool_content)} éléments):\")\n",
    "                            for item_idx, item_data in enumerate(parsed_tool_content[:2]): \n",
    "                                if isinstance(item_data, dict):\n",
    "                                    print(f\"        Item {item_idx+1}: { {k: str(v)[:70] + '...' if isinstance(v,str) and len(str(v)) > 70 else v for k,v in item_data.items()} }\")\n",
    "                                else:\n",
    "                                    print(f\"        Item {item_idx+1}: {str(item_data)[:100]}...\")\n",
    "                            if len(parsed_tool_content) > 2:\n",
    "                                print(\"        ...\")\n",
    "                        else: # Si ce n'est pas une liste parsable ou si elle est vide\n",
    "                            print(f\"      Tool Call ID: {tool_call_id} - Contenu (Résultat Outil): {msg_content_str[:200]}...\")\n",
    "                    else:\n",
    "                        print(f\"      Contenu: {msg_content_str[:200]}...\")\n",
    "            else: # Si 'messages' n'est pas une liste\n",
    "                print(f\"    {str(value)[:500]}...\")\n",
    "        elif key in [\"research_plan\", \"synthesis_output\", \"document_analysis_summary\", \"user_query\", \"error_message\"]:\n",
    "            print(f\"\\n  {key.upper()}:\\n{str(value)[:1000]}{'...' if value and len(str(value)) > 1000 else ''}\\n\")\n",
    "        else: \n",
    "            print(f\"  {key.upper()}: {str(value)[:500]}{'...' if value and len(str(value)) > 500 else ''}\")\n",
    "    print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b98bc",
   "metadata": {},
   "source": [
    "### 1. Définition d'une Requête Utilisateur Complexe\n",
    "\n",
    "Nous allons utiliser une requête qui nécessite potentiellement plusieurs étapes de la part de nos agents (planification, recherche, analyse, synthèse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283861a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Analyze the latest advancements in reinforcement learning for robotic locomotion, focusing on how bipedal robots achieve stable gait. Include key challenges and future research directions based on recent ArXiv papers.\"\n",
    "# user_query = \"What are common methods for robot arm path planning based on recent ArXiv papers?\"\n",
    "\n",
    "logger.info(f\"Requête utilisateur pour ce test : '{user_query}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea283af",
   "metadata": {},
   "source": [
    "### 2. Exécution du Workflow \"Cognitive Swarm\" (Premier Passage)\n",
    "\n",
    "Nous exécutons ici la fonction `run_cognitive_swarm_v2_1` avec notre requête. Un nouvel ID de thread (`thread_id`) sera généré.\n",
    "La sortie de `astream_events` dans `run_cognitive_swarm_v2_1` affichera le flux en temps réel (chunks de LLM, appels d'outils).\n",
    "Le checkpointer MongoDB (activé par défaut dans `main_workflow.py`) sauvegardera l'état à chaque étape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad390ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer un ID de thread unique pour cette exécution\n",
    "test_thread_id = \"nb_workflow_run_\" + str(uuid.uuid4())\n",
    "\n",
    "# LOG_LEVEL_NOTEBOOK est défini dans la première cellule de ce notebook (ID 58690d91)\n",
    "print(f\"Lancement du workflow pour la requête avec thread_id: {test_thread_id}\")\n",
    "print(f\"Utilisation du LLM provider configuré: '{settings.DEFAULT_LLM_MODEL_PROVIDER}' et du provider d'embedding: '{settings.DEFAULT_EMBEDDING_PROVIDER}'.\")\n",
    "print(f\"Les logs du workflow (niveau '{LOG_LEVEL_NOTEBOOK}') et les sorties des agents/outils via astream_events apparaîtront ci-dessous.\")\n",
    "print(\"Le traitement peut prendre plusieurs minutes en fonction de la complexité de la requête et des modèles LLM utilisés.\")\n",
    "\n",
    "# MODIFICATION : Activer nest_asyncio pour permettre asyncio.run() dans Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "final_state_run1 = None\n",
    "\n",
    "if not settings.MONGO_URI or (\"<user>\" in settings.MONGO_URI and \"<password>\" in settings.MONGO_URI) or \"<cluster_url>\" in settings.MONGO_URI:\n",
    "    print(\"\\nERREUR CRITIQUE: MONGO_URI n'est pas configuré correctement dans le fichier .env (il manque ou contient des placeholders).\")\n",
    "    print(\"L'exécution du workflow est annulée car le checkpointer MongoDB est requis.\")\n",
    "    logger.error(\"MONGO_URI non configuré ou contient des placeholders. Workflow non exécuté.\")\n",
    "else:\n",
    "    try:\n",
    "        # asyncio.run() devrait maintenant fonctionner correctement grâce à nest_asyncio.apply()\n",
    "        final_state_run1 = asyncio.run(run_cognitive_swarm_v2_1(user_query, thread_id=test_thread_id))\n",
    "        \n",
    "    except ValueError as ve: \n",
    "        logger.error(f\"Erreur de configuration (ValueError) lors de l'exécution de run_cognitive_swarm_v2_1: {ve}\", exc_info=True)\n",
    "        print(f\"\\nERREUR DE CONFIGURATION PENDANT L'EXÉCUTION DU WORKFLOW : {ve}\")\n",
    "        print(\"Veuillez vérifier les configurations pour DEFAULT_LLM_MODEL_PROVIDER, DEFAULT_EMBEDDING_PROVIDER, \")\n",
    "        print(\"et leurs dépendances respectives (clés API, URLs de base, noms de modèles exacts) dans votre fichier .env et settings.py.\")\n",
    "        print(f\"Provider LLM actuel: {settings.DEFAULT_LLM_MODEL_PROVIDER}, Provider Embedding actuel: {settings.DEFAULT_EMBEDDING_PROVIDER}\")\n",
    "    except RuntimeError as re: # Attraper spécifiquement le RuntimeError si nest_asyncio n'a pas fonctionné\n",
    "        if \"cannot be called from a running event loop\" in str(re):\n",
    "            logger.error(f\"Erreur RuntimeError avec asyncio.run(): {re}. 'nest_asyncio.apply()' n'a peut-être pas fonctionné comme prévu.\", exc_info=True)\n",
    "            print(f\"\\nERREUR asyncio : {re}. Assurez-vous que 'nest_asyncio' est installé et que 'nest_asyncio.apply()' a été appelé.\")\n",
    "        else:\n",
    "            logger.error(f\"Erreur RuntimeError inattendue lors de l'exécution de asyncio.run(run_cognitive_swarm_v2_1): {re}\", exc_info=True)\n",
    "            print(f\"\\nERREUR RUNTIME INATTENDUE PENDANT L'EXÉCUTION DU WORKFLOW : {re}\")\n",
    "    except Exception as e: \n",
    "        logger.error(f\"Erreur inattendue lors de l'exécution de asyncio.run(run_cognitive_swarm_v2_1): {e}\", exc_info=True)\n",
    "        print(f\"\\nERREUR INATTENDUE PENDANT L'EXÉCUTION DU WORKFLOW : {e}\")\n",
    "\n",
    "if final_state_run1:\n",
    "    # pretty_print_final_state est défini dans la première cellule de code de ce notebook (ID 58690d91)\n",
    "    pretty_print_final_state(final_state_run1)\n",
    "else:\n",
    "    print(\"\\nL'exécution du workflow n'a pas retourné d'état final ou a échoué avant de pouvoir retourner un état.\")\n",
    "    if not settings.MONGO_URI or (\"<user>\" in settings.MONGO_URI and \"<password>\" in settings.MONGO_URI) or \"<cluster_url>\" in settings.MONGO_URI :\n",
    "        print(\"Rappel : MONGO_URI n'était pas (ou mal) configuré, ce qui a pu empêcher l'exécution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821f382",
   "metadata": {},
   "source": [
    "### 3. Analyse des Sorties et du Comportement\n",
    "\n",
    "Après l'exécution :\n",
    "* Examinez les logs produits dans la console du notebook (si le niveau de log est DEBUG pour `main_workflow` ou les agents, vous verrez beaucoup de détails).\n",
    "* Observez la sortie finale (`synthesis_output`) dans l'état final.\n",
    "* Si vous avez accès à MongoDB (par exemple, via MongoDB Compass ou un autre client), vous pouvez inspecter la collection des checkpoints (par défaut `langgraph_checkpoints` dans la base `cognitive_swarm_db`). Vous devriez y trouver des documents correspondant au `thread_id` utilisé. Chaque document représente un état sauvegardé du graphe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a61a3f",
   "metadata": {},
   "source": [
    "### 4. (Optionnel) Exécution d'une Requête de Suivi sur le Même Thread\n",
    "\n",
    "Si le checkpointer a fonctionné, l'historique des messages et l'état du thread précédent sont sauvegardés. Envoyer une nouvelle requête avec le *même `thread_id`* permettra au système de potentiellement utiliser cet historique.\n",
    "\n",
    "Notre workflow actuel est plutôt linéaire et ne gère pas explicitement les \"questions de suivi\" pour modifier un rapport existant. Une nouvelle invocation avec le même `thread_id` ajoutera à l'historique des messages et relancera le flux depuis le début, mais les agents verront l'historique complet.\n",
    "\n",
    "Pour une vraie \"reprise\" d'un graphe interrompu, LangGraph le gère automatiquement si vous relancez avec la même configuration (thread_id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test d'une Requête de Suivi (Optionnel) ---\n",
    "# Décommentez les lignes ci-dessous pour tester une requête de suivi sur le même thread.\n",
    "# Note: Cela relancera le flux avec l'historique des messages accumulé. Le workflow actuel\n",
    "# est plutôt linéaire et une nouvelle invocation relancera le processus depuis le début,\n",
    "# mais les agents verront l'historique complet. La persistance via checkpointer est démontrée.\n",
    "\n",
    "# # S'assurer que test_thread_id est défini par la cellule précédente (ID 8ad390ec)\n",
    "# if 'test_thread_id' in locals() and test_thread_id:\n",
    "# #     follow_up_query = \"Could you elaborate on the sim-to-real transfer challenges mentioned previously?\" # Exemple de requête de suivi\n",
    "# #     logger.info(f\"Requête de suivi pour le thread {test_thread_id}: '{follow_up_query}'\")\n",
    "# #     print(f\"\\nLancement d'une requête de suivi sur le même thread_id: {test_thread_id}\")\n",
    "# #     print(f\"Utilisation du LLM provider configuré: '{settings.DEFAULT_LLM_MODEL_PROVIDER}' et du provider d'embedding: '{settings.DEFAULT_EMBEDDING_PROVIDER}'.\")\n",
    "# #     print(\"Le traitement peut prendre plusieurs minutes...\")\n",
    "\n",
    "# #     final_state_run2 = None\n",
    "# #     # Vérification principale: MongoDB est essentiel.\n",
    "# #     if not settings.MONGO_URI or (\"<user>\" in settings.MONGO_URI and \"<password>\" in settings.MONGO_URI) or \"<cluster_url>\" in settings.MONGO_URI:\n",
    "# #         print(\"ERREUR CRITIQUE: MONGO_URI n'est pas configuré correctement. Requête de suivi annulée.\")\n",
    "# #         logger.error(\"MONGO_URI non configuré ou contient des placeholders. Requête de suivi non exécutée.\")\n",
    "# #     else:\n",
    "# #         try:\n",
    "# #             final_state_run2 = asyncio.run(run_cognitive_swarm_v2_1(follow_up_query, thread_id=test_thread_id))\n",
    "# #         except ValueError as ve: \n",
    "# #             logger.error(f\"Erreur de configuration (ValueError) lors de la requête de suivi: {ve}\", exc_info=True)\n",
    "# #             print(f\"\\nERREUR DE CONFIGURATION PENDANT LA REQUÊTE DE SUIVI : {ve}\")\n",
    "# #             print(\"Veuillez vérifier les configurations pour DEFAULT_LLM_MODEL_PROVIDER, DEFAULT_EMBEDDING_PROVIDER, et leurs dépendances.\")\n",
    "# #         except Exception as e: \n",
    "# #             logger.error(f\"Erreur inattendue lors de l'exécution de la requête de suivi : {e}\", exc_info=True)\n",
    "# #             print(f\"\\nERREUR INATTENDUE PENDANT LA REQUÊTE DE SUIVI : {e}\")\n",
    "\n",
    "# #     # pretty_print_final_state est défini dans la première cellule de code de ce notebook (ID 58690d91)\n",
    "# #     if final_state_run2:\n",
    "# #         pretty_print_final_state(final_state_run2)\n",
    "# #     else:\n",
    "# #         print(\"\\nL'exécution de la requête de suivi n'a pas retourné d'état final ou a échoué.\")\n",
    "# else:\n",
    "# #     # Ce message s'affichera si cette cellule est exécutée avant que test_thread_id ne soit défini.\n",
    "# #     print(\"\\nVariable 'test_thread_id' non trouvée ou non initialisée.\")\n",
    "# #     print(\"Veuillez exécuter la cellule précédente (exécution du workflow principal) pour définir un 'test_thread_id' avant de décommenter et d'exécuter cette cellule.\")\n",
    "# #     logger.warning(\"'test_thread_id' non défini. Saut de la cellule de requête de suivi optionnelle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a6790d",
   "metadata": {},
   "source": [
    "### 5. Inspection des Checkpoints dans MongoDB (Conceptuel)\n",
    "\n",
    "Si le `MongoDBSaver` est actif, vous pouvez vous connecter à votre instance MongoDB et examiner la collection `langgraph_checkpoints` (ou le nom que vous avez configuré dans `settings.py`). Vous y trouverez des documents JSON représentant les différents états sauvegardés pour chaque `thread_id`.\n",
    "\n",
    "Chaque document de checkpoint contient typiquement :\n",
    "* `thread_id`\n",
    "* `thread_ts` (un timestamp identifiant ce snapshot spécifique de l'état)\n",
    "* `checkpoint` (l'état sérialisé du graphe, incluant les valeurs des canaux comme `messages`)\n",
    "* `metadata` (métadonnées associées au checkpoint)\n",
    "* `parent_ts` (s'il y a un checkpoint parent)\n",
    "\n",
    "Cela démontre la persistance et la capacité de reprise du workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc9d2e",
   "metadata": {},
   "source": [
    "## Conclusion de la Démonstration du Workflow\n",
    "\n",
    "Ce notebook a permis de lancer le workflow LangGraph complet et d'observer son exécution.\n",
    "Les prochaines étapes pourraient inclure :\n",
    "* Des tests avec des requêtes plus variées.\n",
    "* L'analyse détaillée des checkpoints dans MongoDB.\n",
    "* L'utilisation du script `scripts/run_evaluation.py` pour évaluer quantitativement les résultats.\n",
    "* L'amélioration itérative de la logique de routage et des prompts des agents dans `src/graph/main_workflow.py` et `src/agents/agent_architectures.py`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognitive-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
