{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688b4077",
   "metadata": {},
   "source": [
    "# Notebook 03: Développement et Test des Agents Individuels et de leurs Outils\n",
    "\n",
    "Ce notebook se concentre sur le test et la démonstration de chaque agent que nous avons défini dans `src/agents/agent_architectures.py`. Nous allons instancier chaque agent, lui soumettre des tâches spécifiques, et observer comment il utilise ses outils (définis dans `src/agents/tool_definitions.py`) et comment il génère ses réponses.\n",
    "\n",
    "**Prérequis :**\n",
    "* Avoir exécuté `00_setup_environment.ipynb` (environnement configuré, clés API dans `.env`).\n",
    "* Pour tester `DocumentAnalysisAgent` efficacement, il est préférable d'avoir une base de données MongoDB peuplée via `01_data_ingestion_and_embedding.ipynb` (ou `scripts/run_ingestion.py`) et que le `RetrievalEngine` (utilisé par `knowledge_base_retrieval_tool`) soit fonctionnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63c5c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables d'environnement chargées depuis : /home/facetoface/cognitive-swarm-agents/.env\n",
      "\u001b[34m2025-06-02 22:53:03 - nb_03_agent_testing - INFO - Les agents et certaines outils (comme CrewAI) utiliseront le fournisseur LLM génératif configuré : 'ollama'.\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:03 - nb_03_agent_testing - INFO - Le 'knowledge_base_retrieval_tool' (via RetrievalEngine) utilisera le fournisseur d'embedding : 'ollama'\u001b[0m\n",
      "\u001b[33m2025-06-02 22:53:03 - nb_03_agent_testing - WARNING - Clé API TAVILY (TAVILY_API_KEY) non configurée (non critique pour les tests de base de ce notebook).\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25398/3028106793.py:93: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  if \"TAVILY_API_KEY\" in settings.model_fields and not settings.TAVILY_API_KEY:\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json # Pour un affichage lisible des sorties d'outils\n",
    "from typing import Optional, List # Ajout pour robustesse\n",
    "\n",
    "# --- Configuration du PYTHONPATH et chargement de .env ---\n",
    "# NOTE IMPORTANTE SUR LE CWD (Current Working Directory) :\n",
    "# La ligne suivante `project_root = Path().resolve().parent` suppose que le CWD du notebook\n",
    "# est le dossier `/notebooks/`. Si vous avez configuré VS Code pour que le CWD\n",
    "# soit la racine du projet (`cognitive-swarm-agents/`), alors `Path().resolve()`\n",
    "# donnerait déjà la racine du projet, et vous devriez utiliser :\n",
    "# project_root = Path().resolve()\n",
    "# Vérifiez votre CWD avec `import os; print(os.getcwd())` ou `from pathlib import Path; print(Path().resolve())` pour confirmer.\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "    print(f\"Ajout de {project_root} au PYTHONPATH\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# De même, si CWD est la racine du projet, dotenv_path serait `Path().resolve() / \".env\"`\n",
    "dotenv_path = project_root / \".env\"\n",
    "if dotenv_path.exists():\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    print(f\"Variables d'environnement chargées depuis : {dotenv_path}\")\n",
    "else:\n",
    "    print(f\"ATTENTION: Fichier .env non trouvé à {dotenv_path}. Assurez-vous qu'il est à la racine du projet.\")\n",
    "\n",
    "from config.settings import settings\n",
    "from config.logging_config import setup_logging\n",
    "\n",
    "# Importer les fonctions de création d'agents\n",
    "# get_llm n'est plus importé ici car non utilisé directement ; les fonctions create_..._agent l'utilisent en interne via llm_factory\n",
    "from src.agents.agent_architectures import (\n",
    "    create_research_planner_agent,\n",
    "    create_arxiv_search_agent,\n",
    "    create_document_analysis_agent,\n",
    "    create_synthesis_agent\n",
    "    # get_llm # Supprimé car non utilisé directement dans ce notebook et plus défini dans agent_architectures\n",
    ")\n",
    "\n",
    "# Importer les outils pour d'éventuels tests directs\n",
    "from src.agents.tool_definitions import (\n",
    "    arxiv_search_tool, \n",
    "    knowledge_base_retrieval_tool,\n",
    "    document_deep_dive_analysis_tool # Assurer que cet outil est importé pour test\n",
    ")\n",
    "\n",
    "# Importer les types de messages LangChain\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage # SystemMessage peut être utile\n",
    "\n",
    "setup_logging(level=\"INFO\") # Mettre à DEBUG pour voir les étapes internes des agents\n",
    "logger = logging.getLogger(\"nb_03_agent_testing\")\n",
    "\n",
    "# --- Vérification des prérequis pour les LLMs et Embeddings (logique existante conservée et vérifiée) ---\n",
    "# Pour les agents génératifs (Planner, ArxivSearcher, DocAnalyzer, Synthesizer)\n",
    "# et pour les outils qui pourraient utiliser un LLM (comme document_deep_dive_analysis_tool via CrewAI).\n",
    "active_llm_provider = settings.DEFAULT_LLM_MODEL_PROVIDER.lower()\n",
    "logger.info(f\"Les agents et certaines outils (comme CrewAI) utiliseront le fournisseur LLM génératif configuré : '{active_llm_provider}'.\")\n",
    "if active_llm_provider == \"openai\":\n",
    "    if not settings.OPENAI_API_KEY:\n",
    "        logger.error(f\"ERREUR : Le fournisseur LLM est 'openai', mais OPENAI_API_KEY n'est pas configurée. Les tests des agents/outils utilisant ce LLM échoueront probablement.\")\n",
    "elif active_llm_provider == \"huggingface_api\":\n",
    "    if not settings.HUGGINGFACE_API_KEY:\n",
    "        logger.error(f\"ERREUR : Le fournisseur LLM est 'huggingface_api', mais HUGGINGFACE_API_KEY n'est pas configurée.\")\n",
    "    if not settings.HUGGINGFACE_REPO_ID: # Vérification ajoutée pour être complet\n",
    "        logger.error(f\"ERREUR : Le fournisseur LLM est 'huggingface_api', mais HUGGINGFACE_REPO_ID n'est pas configuré.\")\n",
    "elif active_llm_provider == \"ollama\":\n",
    "    if not settings.OLLAMA_BASE_URL:\n",
    "        logger.error(f\"ERREUR : Le fournisseur LLM est 'ollama', mais OLLAMA_BASE_URL n'est pas configurée.\")\n",
    "    if not settings.OLLAMA_GENERATIVE_MODEL_NAME: # Vérification ajoutée\n",
    "        logger.error(f\"ERREUR : Le fournisseur LLM est 'ollama', mais OLLAMA_GENERATIVE_MODEL_NAME (pour les agents) n'est pas configuré.\")\n",
    "\n",
    "# Pour knowledge_base_retrieval_tool (qui utilise RetrievalEngine, qui utilise le provider d'embedding configuré)\n",
    "active_embedding_provider = settings.DEFAULT_EMBEDDING_PROVIDER.lower()\n",
    "logger.info(f\"Le 'knowledge_base_retrieval_tool' (via RetrievalEngine) utilisera le fournisseur d'embedding : '{active_embedding_provider}'\")\n",
    "if active_embedding_provider == \"openai\": # Pour les embeddings de requête\n",
    "    if not settings.OPENAI_API_KEY:\n",
    "        logger.error(f\"ERREUR : Le fournisseur d'embedding pour RetrievalEngine est 'openai', mais OPENAI_API_KEY n'est pas configurée. Le 'knowledge_base_retrieval_tool' échouera probablement lors de la vectorisation des requêtes.\")\n",
    "elif active_embedding_provider == \"ollama\": # Pour les embeddings de requête\n",
    "    if not settings.OLLAMA_BASE_URL:\n",
    "         logger.error(f\"ERREUR : Le fournisseur d'embedding pour RetrievalEngine est 'ollama', mais OLLAMA_BASE_URL n'est pas configurée.\")\n",
    "    if not settings.OLLAMA_EMBEDDING_MODEL_NAME: # Ajout de cette vérification cruciale\n",
    "        logger.error(f\"ERREUR : Le fournisseur d'embedding pour RetrievalEngine est 'ollama', mais OLLAMA_EMBEDDING_MODEL_NAME n'est pas configuré.\")\n",
    "# Pour \"huggingface\" (local), aucune clé API spécifique n'est requise pour l'embedding des requêtes.\n",
    "\n",
    "if not settings.MONGODB_URI: \n",
    "    logger.error(\"ERREUR : MONGODB_URI non trouvé. Le 'knowledge_base_retrieval_tool' (via RetrievalEngine) ne pourra pas se connecter à la base de données.\")\n",
    "\n",
    "# TAVILY_API_KEY n'est pas utilisé par les outils testés directement dans les cellules suivantes de ce notebook,\n",
    "# mais la vérification est conservée pour information générale si settings.py le liste.\n",
    "if \"TAVILY_API_KEY\" in settings.model_fields and not settings.TAVILY_API_KEY: \n",
    "    logger.warning(\"Clé API TAVILY (TAVILY_API_KEY) non configurée (non critique pour les tests de base de ce notebook).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a730b9e",
   "metadata": {},
   "source": [
    "### 1. Test du `ResearchPlannerAgent`\n",
    "\n",
    "Cet agent est conçu pour prendre une requête utilisateur complexe et la décomposer en un plan de recherche structuré. Il n'utilise pas d'outils pour cette tâche, se basant uniquement sur ses instructions (prompt système) et le LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4dd948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-02 22:53:03 - nb_03_agent_testing - INFO - --- Test du ResearchPlannerAgent ---\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:03 - nb_03_agent_testing - INFO - Tentative d'utilisation du provider LLM configuré par défaut: 'ollama' pour le ResearchPlannerAgent.\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:03 - src.llm_services.llm_factory - INFO - Initializing LLM from llm_factory for provider: 'ollama' with temperature: 0.0\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:03 - src.llm_services.llm_factory - INFO - Using Ollama model (via langchain_ollama): mistral from http://localhost:11434\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:03 - src.agents.agent_architectures - INFO - Research Planner Agent created.\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:03 - nb_03_agent_testing - INFO - Requête pour le planificateur : 'What are the latest trends and key challenges in applying deep reinforcement learning to multi-robot navigation and coordination, particularly for swarm robotics?'\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:05 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\n",
      "--- Plan de Recherche Généré ---\n",
      "1. **Key Questions:**\n",
      "   - What are the current state-of-the-art methods for deep reinforcement learning (DRL) in multi-robot navigation and coordination?\n",
      "   - How have these DRL methods been applied to swarm robotics specifically?\n",
      "   - What are the key challenges encountered when applying DRL to multi-robot navigation and coordination in swarm robotics?\n",
      "   - Are there any recent advancements or trends that address these challenges?\n",
      "\n",
      "2. **Information Sources:**\n",
      "   - ArXiv preprint search: `\"deep reinforcement learning\" AND \"multi-robot navigation\" OR \"swarm robotics\"` (Limit to last 2 years)\n",
      "   - IEEE Xplore Digital Library search: `(\"deep reinforcement learning\" AND \"multi-robot navigation\") OR (\"swarm robotics\")` (Last 5 years)\n",
      "   - Journal of Field Robotics, Autonomous Robots, and IEEE Transactions on Robotics (specific journal searches for recent publications)\n",
      "\n",
      "3. **Search Queries:**\n",
      "   - ArXiv: `\"deep reinforcement learning\" AND \"multi-robot navigation\" OR \"swarm robotics\" AND yyyy..zzzz` (Replace yyyy..zzzz with the appropriate date range)\n",
      "   - IEEE Xplore: `(\"deep reinforcement learning\" AND \"multi-robot navigation\") OR (\"swarm robotics\") AND yyyy..zzzz`\n",
      "\n",
      "4. **Analysis Steps:**\n",
      "   - Extract the methods, challenges, and advancements from each paper.\n",
      "   - Compare and contrast the approaches taken by different researchers in their application of DRL to multi-robot navigation and coordination in swarm robotics.\n",
      "   - Identify common challenges and trends across multiple papers.\n",
      "\n",
      "5. **Final Output Structure:**\n",
      "The final report should provide an overview of the current state-of-the-art methods for applying deep reinforcement learning to multi-robot navigation and coordination, with a focus on swarm robotics. It should detail the key challenges encountered in this field, discuss recent advancements or trends that address these challenges, and offer insights into potential future research directions. The report should be structured in a clear and concise manner, with each section addressing a specific aspect of the user's query.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"--- Test du ResearchPlannerAgent ---\")\n",
    "# La première cellule de ce notebook (ID a57d7cbb) logue déjà des avertissements \n",
    "# si la configuration pour le active_llm_provider (DEFAULT_LLM_MODEL_PROVIDER) est manquante.\n",
    "# Ce test tentera d'utiliser ce provider configuré.\n",
    "logger.info(f\"Tentative d'utilisation du provider LLM configuré par défaut: '{settings.DEFAULT_LLM_MODEL_PROVIDER}' pour le ResearchPlannerAgent.\")\n",
    "\n",
    "try:\n",
    "    # create_research_planner_agent() appelle get_llm() qui utilise \n",
    "    # settings.DEFAULT_LLM_MODEL_PROVIDER et gère la configuration.\n",
    "    # Une ValueError sera levée par get_llm si la configuration est incorrecte pour le provider choisi.\n",
    "    planner_agent_executor = create_research_planner_agent() \n",
    "    \n",
    "    # Requête utilisateur d'exemple (inchangée)\n",
    "    user_query_plan = \"What are the latest trends and key challenges in applying deep reinforcement learning to multi-robot navigation and coordination, particularly for swarm robotics?\"\n",
    "    logger.info(f\"Requête pour le planificateur : '{user_query_plan}'\")\n",
    "    \n",
    "    # Invocation de l'agent\n",
    "    response_planner = planner_agent_executor.invoke({\n",
    "        \"messages\": [HumanMessage(content=user_query_plan)]\n",
    "    })\n",
    "    research_plan = response_planner.get(\"output\")\n",
    "    \n",
    "    print(\"\\n--- Plan de Recherche Généré ---\")\n",
    "    if research_plan:\n",
    "        print(research_plan)\n",
    "    else:\n",
    "        print(\"L'agent planificateur n'a pas retourné de plan (vérifiez les logs).\")\n",
    "        # Afficher la réponse complète peut aider au débogage si 'output' est vide mais la réponse existe\n",
    "        print(f\"Réponse complète de l'agent (si output est vide) : {response_planner}\")\n",
    "            \n",
    "except ValueError as ve:\n",
    "    # Attrape spécifiquement les erreurs de configuration de get_llm()\n",
    "    logger.error(f\"Erreur de configuration LLM pour le ResearchPlannerAgent (provider: {settings.DEFAULT_LLM_MODEL_PROVIDER}): {ve}\", exc_info=True)\n",
    "    print(f\"ERREUR de configuration pour ResearchPlannerAgent : Le provider LLM '{settings.DEFAULT_LLM_MODEL_PROVIDER}' n'est pas correctement configuré (ex: clé API ou URL manquante). Détails : {ve}\")\n",
    "except Exception as e:\n",
    "    # Attrape les autres erreurs pendant l'invocation de l'agent\n",
    "    logger.error(f\"Erreur lors de l'invocation du ResearchPlannerAgent: {e}\", exc_info=True)\n",
    "    print(f\"Erreur inattendue lors du test du ResearchPlannerAgent: {e}\")\n",
    "\n",
    "# L'ancien bloc 'else' qui vérifiait settings.OPENAI_API_KEY est supprimé,\n",
    "# car la logique try/except ci-dessus est plus générale et correcte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a8931e",
   "metadata": {},
   "source": [
    "### 2. Test du `ArxivSearchAgent` (avec `arxiv_search_tool`)\n",
    "\n",
    "Cet agent est spécialisé dans la recherche d'articles sur ArXiv. Il utilise `arxiv_search_tool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da937caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-02 22:53:14 - nb_03_agent_testing - INFO - \n",
      "--- Test du ArxivSearchAgent ---\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:14 - nb_03_agent_testing - INFO - Tentative d'utilisation du provider LLM configuré par défaut: 'ollama' pour l'ArxivSearchAgent.\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:14 - src.llm_services.llm_factory - INFO - Initializing LLM from llm_factory for provider: 'ollama' with temperature: 0.0\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:14 - src.llm_services.llm_factory - INFO - Using Ollama model (via langchain_ollama): mistral from http://localhost:11434\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:14 - src.agents.agent_architectures - INFO - ArXiv Search Agent created with tools: ['arxiv_search_tool']\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:14 - nb_03_agent_testing - INFO - Tâche pour l'agent de recherche ArXiv : 'Find 2 recent papers (last 6 months) on 'explainable reinforcement learning in robotics' sorted by submission date.'\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:15 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:15 - src.agents.tool_definitions - INFO - Executing arxiv_search_tool with query='explainable reinforcement learning in robotics', max_results=2, sort_by='submittedDate', sort_order='descending'\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:15 - arxiv - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=explainable+reinforcement+learning+in+robotics&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:18 - arxiv - INFO - Got first page: 100 of 2586946 total results\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:18 - src.agents.tool_definitions - INFO - arxiv_search_tool found 2 papers.\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:19 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\n",
      "--- Résultats de la Recherche ArXiv (via Agent) ---\n",
      "Impossible de parser la sortie comme JSON. Sortie brute :\n",
      "1. Title: AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion\n",
      "     Authors: Yangyi Huang, Ye Yuan, Xueting Li, Jan Kautz, Umar Iqbal\n",
      "     Summary: This paper introduces AdaHuman, a framework that generates high-fidelity animatable 3D avatars from a single in-the-wild image. The key innovations are a pose-conditioned 3D joint diffusion model and a compositional 3DGS refinement module. AdaHuman significantly outperforms state-of-the-art methods in both avatar reconstruction and reposing.\n",
      "     Published Date: 2025-05-30T17:59:54+00:00\n",
      "     PDF URL: http://arxiv.org/pdf/2505.24877v1\n",
      "\n",
      "  2. Title: Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents\n",
      "     Authors: Yaxin Luo, Zhaoyi Li, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen\n",
      "     Summary: This paper introduces Open CaptchaWorld, a web-based benchmark and platform designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. The benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric: CAPTCHA Reasoning Depth. Experimental results show that humans consistently achieve near-perfect scores, while state-of-the-art MLLM agents struggle significantly.\n",
      "     Published Date: 2025-05-30T17:59:55+00:00\n",
      "     PDF URL: http://arxiv.org/pdf/2505.24878v1\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\n--- Test du ArxivSearchAgent ---\")\n",
    "logger.info(f\"Tentative d'utilisation du provider LLM configuré par défaut: '{settings.DEFAULT_LLM_MODEL_PROVIDER}' pour l'ArxivSearchAgent.\")\n",
    "\n",
    "try:\n",
    "    # create_arxiv_search_agent() appelle get_llm() qui utilise settings.DEFAULT_LLM_MODEL_PROVIDER\n",
    "    arxiv_agent_executor = create_arxiv_search_agent()\n",
    "    \n",
    "    search_task = \"Find 2 recent papers (last 6 months) on 'explainable reinforcement learning in robotics' sorted by submission date.\"\n",
    "    logger.info(f\"Tâche pour l'agent de recherche ArXiv : '{search_task}'\")\n",
    "    \n",
    "    response_arxiv_search = arxiv_agent_executor.invoke({\n",
    "        \"messages\": [HumanMessage(content=search_task)]\n",
    "    })\n",
    "    \n",
    "    arxiv_results_output = response_arxiv_search.get(\"output\")\n",
    "    \n",
    "    print(\"\\n--- Résultats de la Recherche ArXiv (via Agent) ---\")\n",
    "    if arxiv_results_output:\n",
    "        # La sortie de arxiv_search_tool est une liste de dictionnaires (ou un dict d'erreur)\n",
    "        # Essayons de l'afficher de manière lisible\n",
    "        try:\n",
    "            # Si la sortie est une chaîne JSON, la parser. Sinon, l'afficher telle quelle.\n",
    "            # L'outil retourne une liste de dicts, mais l'agent LLM pourrait la wrapper en chaîne.\n",
    "            if isinstance(arxiv_results_output, str):\n",
    "                try:\n",
    "                    # Remplacer les apostrophes simples par des guillemets doubles pour une meilleure compatibilité JSON\n",
    "                    # Attention: ceci est une heuristique et pourrait ne pas marcher pour tous les cas.\n",
    "                    # Une meilleure solution serait que l'agent retourne directement un objet JSON valide ou un type de données structuré.\n",
    "                    corrected_json_string = arxiv_results_output.replace(\"'\", \"\\\"\") \n",
    "                    # Gérer les booléens Python True/False qui ne sont pas valides en JSON (true/false)\n",
    "                    corrected_json_string = corrected_json_string.replace(\"True\", \"true\").replace(\"False\", \"false\")\n",
    "                    # Gérer None qui n'est pas valide en JSON (null)\n",
    "                    corrected_json_string = corrected_json_string.replace(\"None\", \"null\")\n",
    "\n",
    "                    parsed_output = json.loads(corrected_json_string)\n",
    "                    print(json.dumps(parsed_output, indent=2, ensure_ascii=False))\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Impossible de parser la sortie comme JSON. Sortie brute :\\n{arxiv_results_output}\")\n",
    "            elif isinstance(arxiv_results_output, list) or isinstance(arxiv_results_output, dict): # Si c'est déjà un objet Python\n",
    "                print(json.dumps(arxiv_results_output, indent=2, ensure_ascii=False))\n",
    "            else: # Autre type, afficher directement\n",
    "                 print(arxiv_results_output)\n",
    "        except Exception as e_print:\n",
    "            print(f\"Erreur lors de l'affichage formaté des résultats : {e_print}\")\n",
    "            print(\"Sortie brute de l'agent :\")\n",
    "            print(arxiv_results_output)\n",
    "    else:\n",
    "        print(\"L'agent de recherche ArXiv n'a pas retourné de sortie (vérifiez les logs).\")\n",
    "        print(f\"Réponse complète de l'agent (si output est vide) : {response_arxiv_search}\")\n",
    "\n",
    "    # Affichage des étapes intermédiaires (si disponibles et utiles)\n",
    "    if \"intermediate_steps\" in response_arxiv_search and response_arxiv_search[\"intermediate_steps\"]:\n",
    "        print(\"\\nÉtapes intermédiaires de l'agent ArXiv:\")\n",
    "        for step in response_arxiv_search[\"intermediate_steps\"]:\n",
    "            # La structure de 'step' peut varier selon le type d'agent (AgentAction vs ToolCall)\n",
    "            if hasattr(step[0], 'tool') and hasattr(step[0], 'tool_input'): # Pour AgentAction\n",
    "                tool_call_info = f\"Outil: {step[0].tool}, Input: {step[0].tool_input}\"\n",
    "            elif isinstance(step[0], dict) and 'tool' in step[0] and 'tool_input' in step[0]: # Autre format possible\n",
    "                tool_call_info = f\"Outil: {step[0]['tool']}, Input: {step[0]['tool_input']}\"\n",
    "            else: # Fallback\n",
    "                tool_call_info = str(step[0])\n",
    "\n",
    "            tool_result = step[1]\n",
    "            print(f\"  Appel Outil: {tool_call_info}\")\n",
    "            print(f\"  Résultat Outil (extrait): {str(tool_result)[:300]}...\")\n",
    "            \n",
    "except ValueError as ve:\n",
    "    logger.error(f\"Erreur de configuration LLM pour l'ArxivSearchAgent (provider: {settings.DEFAULT_LLM_MODEL_PROVIDER}): {ve}\", exc_info=True)\n",
    "    print(f\"ERREUR de configuration pour ArxivSearchAgent : Le provider LLM '{settings.DEFAULT_LLM_MODEL_PROVIDER}' n'est pas correctement configuré. Détails : {ve}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur lors de l'invocation de l'ArxivSearchAgent: {e}\", exc_info=True)\n",
    "    print(f\"Erreur inattendue lors du test de l'ArxivSearchAgent: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e392505",
   "metadata": {},
   "source": [
    "### 2b. Test Direct de `arxiv_search_tool` (Optionnel)\n",
    "\n",
    "Pour mieux comprendre ce que l'outil `arxiv_search_tool` retourne, nous pouvons l'appeler directement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7871d004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-02 22:53:27 - nb_03_agent_testing - INFO - \n",
      "--- Test Direct de arxiv_search_tool ---\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:27 - src.agents.tool_definitions - INFO - Executing arxiv_search_tool with query='transformer models for robot control', max_results=1, sort_by='submittedDate', sort_order='descending'\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:27 - arxiv - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=transformer+models+for+robot+control&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:30 - arxiv - INFO - Got first page: 100 of 2388778 total results\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:30 - src.agents.tool_definitions - INFO - arxiv_search_tool found 1 papers.\u001b[0m\n",
      "\n",
      "Résultats Directs de arxiv_search_tool:\n",
      "[\n",
      "  {\n",
      "    \"entry_id\": \"http://arxiv.org/abs/2505.24878v1\",\n",
      "    \"title\": \"Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents\",\n",
      "    \"authors\": [\n",
      "      \"Yaxin Luo\",\n",
      "      \"Zhaoyi Li\",\n",
      "      \"Jiacheng Liu\",\n",
      "      \"Jiacheng Cui\",\n",
      "      \"Xiaohan Zhao\",\n",
      "      \"Zhiqiang Shen\"\n",
      "    ],\n",
      "    \"summary\": \"CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL.\",\n",
      "    \"published_date\": \"2025-05-30T17:59:55+00:00\",\n",
      "    \"pdf_url\": \"http://arxiv.org/pdf/2505.24878v1\",\n",
      "    \"primary_category\": \"cs.AI\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\n--- Test Direct de arxiv_search_tool ---\")\n",
    "try:\n",
    "    direct_tool_results = arxiv_search_tool.invoke({\n",
    "        \"query\": \"transformer models for robot control\", \n",
    "        \"max_results\": 1,\n",
    "        \"sort_by\": \"submittedDate\"\n",
    "    })\n",
    "    print(\"\\nRésultats Directs de arxiv_search_tool:\")\n",
    "    # Ajout de ensure_ascii=False pour un meilleur affichage des caractères spéciaux\n",
    "    print(json.dumps(direct_tool_results, indent=2, ensure_ascii=False))\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur lors de l'appel direct à arxiv_search_tool: {e}\", exc_info=True)\n",
    "    print(f\"Erreur lors de l'appel direct à arxiv_search_tool: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f521da5",
   "metadata": {},
   "source": [
    "### 3. Test du `DocumentAnalysisAgent` (avec `knowledge_base_retrieval_tool`)\n",
    "\n",
    "Cet agent analyse les documents récupérés de notre base de connaissances (MongoDB). Son bon fonctionnement dépend de la présence de données pertinentes dans la collection (par exemple, `{COLLECTION_NAME_FOR_RAG_TEST}`) et de la fonctionnalité du `RetrievalEngine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f06850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-02 22:53:30 - nb_03_agent_testing - INFO - \n",
      "--- Test du DocumentAnalysisAgent ---\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:30 - nb_03_agent_testing - INFO - Tentative d'utilisation du provider LLM configuré par défaut: 'ollama' pour le DocumentAnalysisAgent.\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:30 - nb_03_agent_testing - INFO - Ce test dépend aussi du provider d'embedding configuré: 'ollama' (pour knowledge_base_retrieval_tool via RetrievalEngine) et de MongoDB.\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:30 - src.llm_services.llm_factory - INFO - Initializing LLM from llm_factory for provider: 'ollama' with temperature: 0.0\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:30 - src.llm_services.llm_factory - INFO - Using Ollama model (via langchain_ollama): mistral from http://localhost:11434\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:30 - src.agents.agent_architectures - INFO - Document Analysis Agent created with tools: ['knowledge_base_retrieval_tool', 'document_deep_dive_analysis_tool']\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:30 - nb_03_agent_testing - INFO - Tâche pour l'agent d'analyse de documents : 'Based on the knowledge base, what are some common techniques for achieving explainability in RL agents used in robotics? Cite any relevant ArXiv IDs if found.'\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:31 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\n",
      "--- Résultats de l'Analyse de Documents (via Agent) ---\n",
      " To answer your question, I'll use the `knowledge_base_retrieval_tool` to find relevant chunks of text about explainable reinforcement learning (RL) techniques in robotics.\n",
      "\n",
      "Here are some common techniques for achieving explainability in RL agents used in robotics:\n",
      "\n",
      "1. Visualization-based methods: These techniques visualize the learned policies, value functions, or state representations to understand the decision-making process of an RL agent. Examples include:\n",
      "   - \"Visualizing and Understanding Deep Reinforcement Learning\" (arXiv:1707.06347)\n",
      "   - \"Understanding Deep Reinforcement Learning through Cost-Sensitive Analysis\" (arXiv:1802.05905)\n",
      "\n",
      "2. Interpretable models: These methods aim to use interpretable models for RL, such as decision trees or rule lists, which can be easily understood by humans. Examples include:\n",
      "   - \"Interpretable Reinforcement Learning with Decision Trees\" (arXiv:1806.05942)\n",
      "   - \"Rule-based reinforcement learning for robotics\" (arXiv:1703.08887)\n",
      "\n",
      "3. Model-based methods: These techniques learn an internal model of the environment, which can be analyzed to understand the agent's behavior. Examples include:\n",
      "   - \"Model-Based Reinforcement Learning for Robotics\" (arXiv:1906.04587)\n",
      "   - \"Learning and Explaining Dynamics Models with Deep State Representations\" (arXiv:1802.03405)\n",
      "\n",
      "4. Counterfactual explanations: These methods provide explanations for the agent's actions by comparing the actual outcome with alternative outcomes that would have resulted from different decisions. Examples include:\n",
      "   - \"Counterfactual Explanations for Deep Reinforcement Learning\" (arXiv:1906.02752)\n",
      "   - \"Explaining Deep Reinforcement Learning via Counterfactuals\" (arXiv:1804.03692)\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\n--- Test du DocumentAnalysisAgent ---\")\n",
    "logger.info(f\"Tentative d'utilisation du provider LLM configuré par défaut: '{settings.DEFAULT_LLM_MODEL_PROVIDER}' pour le DocumentAnalysisAgent.\")\n",
    "logger.info(f\"Ce test dépend aussi du provider d'embedding configuré: '{settings.DEFAULT_EMBEDDING_PROVIDER}' (pour knowledge_base_retrieval_tool via RetrievalEngine) et de MongoDB.\")\n",
    "\n",
    "# La connexion à MongoDB est essentielle pour les outils de cet agent.\n",
    "if not settings.MONGODB_URI:\n",
    "    logger.error(\"MONGODB_URI non configuré. Test du DocumentAnalysisAgent sauté car ses outils (knowledge_base_retrieval_tool) en dépendent crucialement.\")\n",
    "    print(\"ERREUR: MONGODB_URI non configuré dans .env. Le DocumentAnalysisAgent ne peut pas être testé correctement sans accès à la base de données.\")\n",
    "else:\n",
    "    try:\n",
    "        # La création de l'agent peut échouer si get_llm() (appelé en interne) échoue \n",
    "        # en raison d'une configuration manquante pour le DEFAULT_LLM_MODEL_PROVIDER.\n",
    "        # De même, RetrievalEngine (utilisé par knowledge_base_retrieval_tool) peut échouer à l'init\n",
    "        # si DEFAULT_EMBEDDING_PROVIDER est mal configuré.\n",
    "        doc_analysis_agent_executor = create_document_analysis_agent()\n",
    "        \n",
    "        # Tâche d'analyse exemple.\n",
    "        # Le DocumentAnalysisAgent est maintenant équipé du document_deep_dive_analysis_tool.\n",
    "        # Son prompt système (DOCUMENT_ANALYSIS_SYSTEM_PROMPT_V2) le guide sur quand utiliser quel outil.\n",
    "        # Voici une tâche qui devrait utiliser knowledge_base_retrieval_tool :\n",
    "        analysis_task = \"Based on the knowledge base, what are some common techniques for achieving explainability in RL agents used in robotics? Cite any relevant ArXiv IDs if found.\"\n",
    "        \n",
    "        # Pour tester le document_deep_dive_analysis_tool, la tâche devrait être formulée différemment,\n",
    "        # par exemple, en demandant explicitement une \"deep dive\" sur un document spécifique.\n",
    "        # Exemple :\n",
    "        # analysis_task = \"Perform a detailed, structured deep dive analysis on document 'XXXX.YYYYY' (you will need to fetch its content first if not provided) focusing on its methodology and limitations for sim-to-real transfer.\"\n",
    "        # Pour un tel test, assurez-vous que le document 'XXXX.YYYYY' existe ou que l'agent peut le récupérer.\n",
    "        \n",
    "        logger.info(f\"Tâche pour l'agent d'analyse de documents : '{analysis_task}'\")\n",
    "        \n",
    "        response_doc_analysis = doc_analysis_agent_executor.invoke({\n",
    "            \"messages\": [HumanMessage(content=analysis_task)]\n",
    "        })\n",
    "        analysis_output = response_doc_analysis.get(\"output\")\n",
    "        \n",
    "        print(\"\\n--- Résultats de l'Analyse de Documents (via Agent) ---\")\n",
    "        if analysis_output:\n",
    "            # La sortie peut être une simple chaîne ou un rapport structuré (ex: de CrewAI via le deep_dive_tool)\n",
    "            if isinstance(analysis_output, str) and analysis_output.lower().startswith(\"error:\"):\n",
    "                print(f\"L'agent ou un outil a retourné une ERREUR gérée : {analysis_output}\")\n",
    "            else:\n",
    "                # Tenter d'afficher en Markdown si la sortie est un rapport formaté (souvent le cas pour CrewAI)\n",
    "                # ou en JSON si c'est une structure de données, sinon en texte brut.\n",
    "                if isinstance(analysis_output, str) and (\"\\n## \" in analysis_output or \"\\n### \" in analysis_output or \"```\" in analysis_output) : # Heuristique pour Markdown\n",
    "                    try:\n",
    "                        from IPython.display import display, Markdown\n",
    "                        print(\"Affichage de la sortie comme Markdown:\")\n",
    "                        display(Markdown(analysis_output))\n",
    "                    except ImportError:\n",
    "                        print(analysis_output) # Fallback\n",
    "                elif isinstance(analysis_output, str) and analysis_output.strip().startswith((\"{\", \"[\")): # Heuristique pour JSON\n",
    "                    try:\n",
    "                        parsed_json = json.loads(analysis_output)\n",
    "                        print(\"Sortie formatée comme JSON:\")\n",
    "                        print(json.dumps(parsed_json, indent=2, ensure_ascii=False))\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(analysis_output) # Fallback\n",
    "                else: # Si c'est déjà un dict/list ou une chaîne simple\n",
    "                    if isinstance(analysis_output, (dict, list)):\n",
    "                        print(json.dumps(analysis_output, indent=2, ensure_ascii=False))\n",
    "                    else:\n",
    "                        print(analysis_output)\n",
    "        else:\n",
    "            print(\"L'agent d'analyse de documents n'a pas retourné de sortie ('output' est vide/None).\")\n",
    "            print(f\"Réponse complète de l'agent (pour débogage) : {response_doc_analysis}\")\n",
    "\n",
    "        # Affichage des étapes intermédiaires\n",
    "        if \"intermediate_steps\" in response_doc_analysis and response_doc_analysis[\"intermediate_steps\"]:\n",
    "            print(\"\\nÉtapes intermédiaires de l'agent d'Analyse:\")\n",
    "            for step in response_doc_analysis[\"intermediate_steps\"]:\n",
    "                tool_call_info = \"N/A\"\n",
    "                # La structure de step[0] (l'action) peut varier.\n",
    "                action_part = step[0]\n",
    "                if hasattr(action_part, 'tool') and hasattr(action_part, 'tool_input'): # AgentAction\n",
    "                    tool_call_info = f\"Outil: {action_part.tool}, Input: {action_part.tool_input}\"\n",
    "                elif isinstance(action_part, list) and action_part and isinstance(action_part[0], dict) and 'tool_name' in action_part[0]: # Possible format pour tool_calls multiples\n",
    "                     tool_call_info = f\"Outil(s): {', '.join([tc.get('tool_name', 'unknown') for tc in action_part])}\"\n",
    "                elif isinstance(action_part, dict) and 'tool' in action_part and 'tool_input' in action_part: # Autre format possible\n",
    "                     tool_call_info = f\"Outil: {action_part['tool']}, Input: {action_part['tool_input']}\"\n",
    "                else: # Fallback\n",
    "                    tool_call_info = str(action_part)\n",
    "\n",
    "                tool_result = step[1] # Observation\n",
    "                print(f\"  Appel Outil/Action: {tool_call_info}\")\n",
    "                print(f\"  Résultat Outil (extrait): {str(tool_result)[:300]}...\")\n",
    "                \n",
    "    except ValueError as ve:\n",
    "        logger.error(f\"Erreur de configuration ou de valeur lors de la création/invocation du DocumentAnalysisAgent. Vérifiez les configurations pour le provider LLM '{settings.DEFAULT_LLM_MODEL_PROVIDER}' et le provider d'embedding '{settings.DEFAULT_EMBEDDING_PROVIDER}'. Détails: {ve}\", exc_info=True)\n",
    "        print(f\"ERREUR (DocumentAnalysisAgent): Problème de configuration LLM ou Embedding. Provider LLM: '{settings.DEFAULT_LLM_MODEL_PROVIDER}', Provider Embedding: '{settings.DEFAULT_EMBEDDING_PROVIDER}'. Détails: {ve}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'invocation du DocumentAnalysisAgent: {e}\", exc_info=True)\n",
    "        print(f\"Erreur inattendue lors du test du DocumentAnalysisAgent: {e}. Assurez-vous que RetrievalEngine peut s'initialiser, que MongoDB est accessible et peuplé, et que les configurations LLM/Embedding sont correctes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f5490",
   "metadata": {},
   "source": [
    "### 4. Test du `SynthesisAgent`\n",
    "\n",
    "Cet agent prend des informations analysées (que nous allons simuler ici) et produit une synthèse structurée. Il n'utilise pas d'outils de récupération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e225861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-02 22:53:40 - nb_03_agent_testing - INFO - \n",
      "--- Test du SynthesisAgent ---\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:40 - nb_03_agent_testing - INFO - Tentative d'utilisation du provider LLM configuré par défaut: 'ollama' pour le SynthesisAgent.\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:40 - src.llm_services.llm_factory - INFO - Initializing LLM from llm_factory for provider: 'ollama' with temperature: 0.5\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:40 - src.llm_services.llm_factory - INFO - Using Ollama model (via langchain_ollama): mistral from http://localhost:11434\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:40 - src.agents.agent_architectures - INFO - Synthesis Agent created.\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:40 - nb_03_agent_testing - INFO - Tâche pour l'agent de synthèse (basée sur un contexte simulé).\u001b[0m\n",
      "\u001b[34m2025-06-02 22:53:40 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\n",
      "--- Sortie de Synthèse (via Agent) ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " **Rapport de synthèse sur les considérations clés pour la transfert simulacé-réel dans l'apprentissage renforcé robotique**\n",
       "\n",
       "   Le transfert simulacé-réel dans l'apprentissage renforcé robotique est une étape critique, mais elle peut souffrir de problèmes liés à la randomisation du domaine. Les techniques telles que l'adaptation au contexte et l'identification système sont essentielles. Les paramètres physiques tels que la frottement et les bruits de capteurs sont difficiles à modéliser avec précision.\n",
       "\n",
       "   Selon une étude (ArXiv: 123.4567), les simulateurs réalistes et l'ajout de bruit pendant la formation peuvent améliorer le transfert. L'utilisation d'un rendu photoréaliste aide les politiques basées sur la vision. La distillation de la politique à partir d'un ensemble d'agents formés dans des simulations est une approche prometteuse.\n",
       "\n",
       "   Une étude supplémentaire (ArXiv: 789.0123) souligne l'importance d'utiliser un simulateur réaliste et de prendre en compte les bruits pendant la formation pour améliorer le transfert. Elle met également en avant la distillation de politique à partir d'un ensemble d'agents formés dans des simulations comme une approche prometteuse.\n",
       "\n",
       "   Enfin, un examen récent (ArXiv: 2401.0001) souligne l'importance de développer des algorithmes d'apprentissage robustes et d'améliorer la modélisation des dynamiques pour améliorer le transfert simulacé-réel.\n",
       "\n",
       "   En conclusion, les clés considérations pour un transfert simulacé-réel efficace dans l'apprentissage renforcé robotique comprennent la robustesse des algorithmes d'apprentissage, une modélisation précise des dynamiques, l'ajout de bruit pendant la formation, le rendu photoréaliste et la distillation de politique à partir d'un ensemble d'agents formés dans des simulations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\"\\n--- Test du SynthesisAgent ---\")\n",
    "# create_synthesis_agent() dans agent_architectures.py appelle get_llm() \n",
    "# avec SYNTHESIS_LLM_TEMPERATURE depuis llm_factory.py.\n",
    "# Il utilisera le settings.DEFAULT_LLM_MODEL_PROVIDER.\n",
    "logger.info(f\"Tentative d'utilisation du provider LLM configuré par défaut: '{settings.DEFAULT_LLM_MODEL_PROVIDER}' pour le SynthesisAgent.\")\n",
    "\n",
    "try:\n",
    "    # La création de l'agent peut échouer si get_llm() (appelé en interne) échoue \n",
    "    # en raison d'une configuration manquante pour le DEFAULT_LLM_MODEL_PROVIDER.\n",
    "    synthesis_agent_executor = create_synthesis_agent()\n",
    "    \n",
    "    # Préparer un contexte simulé (inchangé par rapport à la version originale de la cellule)\n",
    "    simulated_context_for_synthesis = \"\"\"\n",
    "    User Query: What are key considerations for sim-to-real transfer in robotic reinforcement learning?\n",
    "\n",
    "    Information from Document Analysis:\n",
    "    - Chunk from ArXiv ID 123.4567: Sim-to-real transfer often suffers from domain randomization issues. Techniques like domain adaptation and system identification are crucial. Physical parameters like friction and sensor noise are hard to model accurately.\n",
    "    - Chunk from ArXiv ID 789.0123: Using realistic simulators and adding noise during training can improve transfer. Photorealistic rendering helps vision-based policies. Policy distillation from an ensemble of simulation-trained agents is a promising approach.\n",
    "    - ArXiv Search found paper 'Recent Advances in Sim-to-Real for Robotics' (ArXiv:2401.0001), summary: This paper reviews state-of-the-art methods, highlighting the importance of robust learning algorithms and accurate dynamics modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Le message pour l'agent de synthèse inclut maintenant le contexte directement.\n",
    "    # Le prompt de l'agent de synthèse (SYNTHESIS_AGENT_SYSTEM_PROMPT) lui indique de travailler\n",
    "    # avec les informations fournies dans les messages.\n",
    "    synthesis_task_message = HumanMessage(\n",
    "        content=f\"Based on the provided information below, write a concise summary report on the key considerations for sim-to-real transfer in robotic reinforcement learning. The report should be well-structured and directly address the initial user query mentioned in the context.\\n\\nProvided Information:\\n{simulated_context_for_synthesis}\"\n",
    "    )\n",
    "    logger.info(\"Tâche pour l'agent de synthèse (basée sur un contexte simulé).\")\n",
    "\n",
    "    response_synthesis = synthesis_agent_executor.invoke({\n",
    "        \"messages\": [synthesis_task_message] # L'historique des messages peut aussi être passé si pertinent\n",
    "    })\n",
    "    synthesized_output = response_synthesis.get(\"output\")\n",
    "    \n",
    "    print(\"\\n--- Sortie de Synthèse (via Agent) ---\")\n",
    "    if synthesized_output:\n",
    "        # Le SynthesisAgent est censé produire du texte formaté (potentiellement Markdown).\n",
    "        if isinstance(synthesized_output, str) and (synthesized_output.lower().startswith(\"error:\") or \"erreur\" in synthesized_output.lower()):\n",
    "             print(f\"L'agent de synthèse semble avoir rencontré une difficulté : {synthesized_output}\")\n",
    "        else:\n",
    "            try:\n",
    "                from IPython.display import display, Markdown\n",
    "                display(Markdown(str(synthesized_output)))\n",
    "            except ImportError:\n",
    "                print(str(synthesized_output))\n",
    "    else:\n",
    "        print(\"L'agent de synthèse n'a pas retourné de sortie ('output' est vide/None).\")\n",
    "        print(f\"Réponse complète de l'agent (pour débogage) : {response_synthesis}\")\n",
    "            \n",
    "except ValueError as ve:\n",
    "    logger.error(f\"Erreur de configuration LLM pour le SynthesisAgent (provider: {settings.DEFAULT_LLM_MODEL_PROVIDER}): {ve}\", exc_info=True)\n",
    "    print(f\"ERREUR de configuration pour SynthesisAgent : Le provider LLM '{settings.DEFAULT_LLM_MODEL_PROVIDER}' n'est pas correctement configuré. Détails : {ve}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur lors de l'invocation du SynthesisAgent: {e}\", exc_info=True)\n",
    "    print(f\"Erreur inattendue lors du test du SynthesisAgent: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3397713f",
   "metadata": {},
   "source": [
    "## Conclusion des Tests d'Agents Individuels\n",
    "\n",
    "Ce notebook a permis de tester chaque agent de manière isolée pour vérifier son comportement de base et son interaction avec les outils.\n",
    "Ces tests unitaires sont importants avant d'orchestrer ces agents dans un workflow LangGraph plus complexe (ce que nous ferons dans le notebook suivant)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognitive-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
