{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688b4077",
   "metadata": {},
   "source": [
    "# Notebook 03: Développement et Test des Agents Individuels et de leurs Outils\n",
    "\n",
    "Ce notebook se concentre sur le test et la démonstration de chaque agent que nous avons défini dans `src/agents/agent_architectures.py`. Nous allons instancier chaque agent, lui soumettre des tâches spécifiques, et observer comment il utilise ses outils (définis dans `src/agents/tool_definitions.py`) et comment il génère ses réponses.\n",
    "\n",
    "**Prérequis :**\n",
    "* Avoir exécuté `00_setup_environment.ipynb` (environnement configuré, clés API dans `.env`).\n",
    "* Pour tester `DocumentAnalysisAgent` efficacement, il est préférable d'avoir une base de données MongoDB peuplée via `01_data_ingestion_and_embedding.ipynb` (ou `scripts/run_ingestion.py`) et que le `RetrievalEngine` (utilisé par `knowledge_base_retrieval_tool`) soit fonctionnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json # Pour un affichage lisible des sorties d'outils\n",
    "\n",
    "# Ajout de la racine du projet au PYTHONPATH\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "    print(f\"Ajout de {project_root} au PYTHONPATH\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "dotenv_path = project_root / \".env\"\n",
    "if dotenv_path.exists():\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    print(f\"Variables d'environnement chargées depuis : {dotenv_path}\")\n",
    "else:\n",
    "    print(f\"ATTENTION: Fichier .env non trouvé à {dotenv_path}.\")\n",
    "\n",
    "from config.settings import settings\n",
    "from config.logging_config import setup_logging\n",
    "\n",
    "# Importer les fonctions de création d'agents\n",
    "from src.agents.agent_architectures import (\n",
    "    create_research_planner_agent,\n",
    "    create_arxiv_search_agent,\n",
    "    create_document_analysis_agent,\n",
    "    create_synthesis_agent,\n",
    "    get_llm \n",
    ")\n",
    "\n",
    "# Importer les outils pour d'éventuels tests directs\n",
    "from src.agents.tool_definitions import arxiv_search_tool, knowledge_base_retrieval_tool\n",
    "# document_deep_dive_analysis_tool est aussi disponible mais moins susceptible d'être testé directement ici\n",
    "\n",
    "# Importer les types de messages LangChain\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "setup_logging(level=\"INFO\") # Mettre à DEBUG pour voir les étapes internes des agents\n",
    "logger = logging.getLogger(\"nb_03_agent_testing\")\n",
    "\n",
    "# --- MODIFIÉ : Vérification des prérequis pour les LLMs et Embeddings ---\n",
    "# Pour les agents génératifs (Planner, ArxivSearcher, DocAnalyzer, Synthesizer, CrewAI via get_llm)\n",
    "active_llm_provider = settings.DEFAULT_LLM_MODEL_PROVIDER.lower()\n",
    "logger.info(f\"Les agents utiliseront le fournisseur LLM génératif : '{active_llm_provider}'\")\n",
    "if active_llm_provider == \"openai\" and not settings.OPENAI_API_KEY:\n",
    "    logger.error(f\"ERREUR : Le fournisseur LLM est 'openai', mais OPENAI_API_KEY n'est pas configurée. Les tests des agents utilisant ce LLM échoueront.\")\n",
    "elif active_llm_provider == \"huggingface_api\" and not settings.HUGGINGFACE_API_KEY:\n",
    "    logger.error(f\"ERREUR : Le fournisseur LLM est 'huggingface_api', mais HUGGINGFACE_API_KEY n'est pas configurée.\")\n",
    "elif active_llm_provider == \"ollama\" and not settings.OLLAMA_BASE_URL:\n",
    "    logger.error(f\"ERREUR : Le fournisseur LLM est 'ollama', mais OLLAMA_BASE_URL n'est pas configurée.\")\n",
    "\n",
    "# Pour knowledge_base_retrieval_tool (qui utilise RetrievalEngine, qui utilise le provider d'embedding configuré)\n",
    "active_embedding_provider = settings.DEFAULT_EMBEDDING_PROVIDER.lower()\n",
    "logger.info(f\"Le 'knowledge_base_retrieval_tool' utilisera le fournisseur d'embedding : '{active_embedding_provider}'\")\n",
    "if active_embedding_provider == \"openai\" and not settings.OPENAI_API_KEY:\n",
    "    logger.error(f\"ERREUR : Le fournisseur d'embedding pour RetrievalEngine est 'openai', mais OPENAI_API_KEY n'est pas configurée. Le 'knowledge_base_retrieval_tool' échouera probablement.\")\n",
    "elif active_embedding_provider == \"ollama\" and not settings.OLLAMA_BASE_URL:\n",
    "     logger.error(f\"ERREUR : Le fournisseur d'embedding pour RetrievalEngine est 'ollama', mais OLLAMA_BASE_URL n'est pas configurée.\")\n",
    "# Pas de vérification de clé pour HuggingFace embeddings locaux.\n",
    "\n",
    "if not settings.MONGO_URI: # Nécessaire pour knowledge_base_retrieval_tool via RetrievalEngine\n",
    "    logger.error(\"ERREUR : MONGO_URI non trouvé. Le 'knowledge_base_retrieval_tool' ne pourra pas se connecter à la base de données.\")\n",
    "\n",
    "if not settings.TAVILY_API_KEY: \n",
    "    logger.warning(\"Clé API TAVILY non configurée (non utilisée par les outils actuels de ce notebook, mais bon à savoir).\")\n",
    "# --- FIN MODIFIÉ ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87b2b5",
   "metadata": {},
   "source": [
    "### 1. Test du `ResearchPlannerAgent`\n",
    "\n",
    "Cet agent est conçu pour prendre une requête utilisateur complexe et la décomposer en un plan de recherche structuré. Il n'utilise pas d'outils pour cette tâche, se basant uniquement sur ses instructions (prompt système) et le LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf123821",
   "metadata": {},
   "outputs": [],
   "source": [
    "if settings.OPENAI_API_KEY:\n",
    "    logger.info(\"--- Test du ResearchPlannerAgent ---\")\n",
    "    planner_agent_executor = create_research_planner_agent()\n",
    "    \n",
    "    # Requête utilisateur d'exemple\n",
    "    user_query_plan = \"What are the latest trends and key challenges in applying deep reinforcement learning to multi-robot navigation and coordination, particularly for swarm robotics?\"\n",
    "    logger.info(f\"Requête pour le planificateur : '{user_query_plan}'\")\n",
    "    \n",
    "    # Invocation de l'agent\n",
    "    # Les agents attendent une liste de messages. Pour une nouvelle tâche, c'est souvent un HumanMessage.\n",
    "    try:\n",
    "        response_planner = planner_agent_executor.invoke({\n",
    "            \"messages\": [HumanMessage(content=user_query_plan)]\n",
    "        })\n",
    "        research_plan = response_planner.get(\"output\")\n",
    "        \n",
    "        print(\"\\n--- Plan de Recherche Généré ---\")\n",
    "        if research_plan:\n",
    "            print(research_plan)\n",
    "        else:\n",
    "            print(\"L'agent planificateur n'a pas retourné de plan (vérifiez les logs).\")\n",
    "            print(f\"Réponse complète de l'agent : {response_planner}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'invocation du ResearchPlannerAgent: {e}\", exc_info=True)\n",
    "        print(f\"Erreur lors du test du ResearchPlannerAgent: {e}\")\n",
    "else:\n",
    "    logger.warning(\"Clé API OpenAI manquante, test du ResearchPlannerAgent sauté.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e315f61e",
   "metadata": {},
   "source": [
    "### 2. Test du `ArxivSearchAgent` (avec `arxiv_search_tool`)\n",
    "\n",
    "Cet agent est spécialisé dans la recherche d'articles sur ArXiv. Il utilise `arxiv_search_tool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da937caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if settings.OPENAI_API_KEY:\n",
    "    logger.info(\"\\n--- Test du ArxivSearchAgent ---\")\n",
    "    arxiv_agent_executor = create_arxiv_search_agent()\n",
    "    \n",
    "    # Tâche de recherche pour l'agent (pourrait être une directive issue d'un plan)\n",
    "    search_task = \"Find 2 recent papers (last 6 months) on 'explainable reinforcement learning in robotics' sorted by submission date.\"\n",
    "    # L'agent doit comprendre cette tâche et formuler la bonne requête pour son outil.\n",
    "    logger.info(f\"Tâche pour l'agent de recherche ArXiv : '{search_task}'\")\n",
    "    \n",
    "    try:\n",
    "        response_arxiv_search = arxiv_agent_executor.invoke({\n",
    "            \"messages\": [HumanMessage(content=search_task)]\n",
    "        })\n",
    "        \n",
    "        # La sortie de l'agent devrait être le résultat de l'appel à l'outil,\n",
    "        # ou un message indiquant qu'il a utilisé l'outil.\n",
    "        # Si l'agent est bien conçu, son \"output\" final devrait être les résultats de l'outil.\n",
    "        arxiv_results_output = response_arxiv_search.get(\"output\")\n",
    "        \n",
    "        print(\"\\n--- Résultats de la Recherche ArXiv (via Agent) ---\")\n",
    "        if arxiv_results_output:\n",
    "            # La sortie de arxiv_search_tool est une liste de dictionnaires (ou un dict d'erreur)\n",
    "            # Essayons de l'afficher de manière lisible\n",
    "            try:\n",
    "                # Si la sortie est une chaîne JSON, la parser. Sinon, l'afficher telle quelle.\n",
    "                # L'outil retourne une liste de dicts, mais l'agent LLM pourrait la wrapper en chaîne.\n",
    "                if isinstance(arxiv_results_output, str):\n",
    "                    try:\n",
    "                        parsed_output = json.loads(arxiv_results_output.replace(\"'\", \"\\\"\")) # Tentative de correction des apostrophes\n",
    "                        print(json.dumps(parsed_output, indent=2))\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(arxiv_results_output) # Afficher comme chaîne si ce n'est pas du JSON valide\n",
    "                else: # Supposons que c'est déjà la liste de dicts\n",
    "                     print(json.dumps(arxiv_results_output, indent=2))\n",
    "            except Exception as e_print:\n",
    "                print(f\"Erreur lors de l'affichage des résultats : {e_print}\")\n",
    "                print(\"Sortie brute de l'agent :\")\n",
    "                print(arxiv_results_output)\n",
    "        else:\n",
    "            print(\"L'agent de recherche ArXiv n'a pas retourné de sortie (vérifiez les logs).\")\n",
    "            print(f\"Réponse complète de l'agent : {response_arxiv_search}\")\n",
    "\n",
    "        # Pour voir les étapes intermédiaires (appels d'outils) si verbose=True est actif pour l'AgentExecutor\n",
    "        # et si l'AgentExecutor retourne `intermediate_steps`.\n",
    "        # Les agents créés avec `create_openai_tools_agent` mettent les `tool_calls` et `tool_messages`\n",
    "        # dans la clé \"messages\" de la sortie de `invoke` si on les gère dans un cycle de LangGraph.\n",
    "        # En invocation directe, `intermediate_steps` peut être disponible.\n",
    "        if \"intermediate_steps\" in response_arxiv_search:\n",
    "            print(\"\\nÉtapes intermédiaires de l'agent ArXiv:\")\n",
    "            for step in response_arxiv_search[\"intermediate_steps\"]:\n",
    "                tool_call = step[0] # AgentAction (ou equivalent pour tool_calls)\n",
    "                tool_result = step[1] # Observation (résultat de l'outil)\n",
    "                print(f\"  Appel Outil: {tool_call.tool} avec input {tool_call.tool_input}\")\n",
    "                print(f\"  Résultat Outil: {str(tool_result)[:300]}...\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'invocation du ArxivSearchAgent: {e}\", exc_info=True)\n",
    "        print(f\"Erreur lors du test du ArxivSearchAgent: {e}\")\n",
    "else:\n",
    "    logger.warning(\"Clé API OpenAI manquante, test du ArxivSearchAgent sauté.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32918fbb",
   "metadata": {},
   "source": [
    "### 2b. Test Direct de `arxiv_search_tool` (Optionnel)\n",
    "\n",
    "Pour mieux comprendre ce que l'outil `arxiv_search_tool` retourne, nous pouvons l'appeler directement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"\\n--- Test Direct de arxiv_search_tool ---\")\n",
    "try:\n",
    "    direct_tool_results = arxiv_search_tool.invoke({\n",
    "        \"query\": \"transformer models for robot control\", \n",
    "        \"max_results\": 1,\n",
    "        \"sort_by\": \"submittedDate\"\n",
    "    })\n",
    "    print(\"\\nRésultats Directs de arxiv_search_tool:\")\n",
    "    print(json.dumps(direct_tool_results, indent=2))\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur lors de l'appel direct à arxiv_search_tool: {e}\", exc_info=True)\n",
    "    print(f\"Erreur lors de l'appel direct à arxiv_search_tool: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f521da5",
   "metadata": {},
   "source": [
    "### 3. Test du `DocumentAnalysisAgent` (avec `knowledge_base_retrieval_tool`)\n",
    "\n",
    "Cet agent analyse les documents récupérés de notre base de connaissances (MongoDB). Son bon fonctionnement dépend de la présence de données pertinentes dans la collection (par exemple, `{COLLECTION_NAME_FOR_RAG_TEST}`) et de la fonctionnalité du `RetrievalEngine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f06850",
   "metadata": {},
   "outputs": [],
   "source": [
    "if settings.OPENAI_API_KEY and settings.MONGO_URI:\n",
    "    logger.info(\"\\n--- Test du DocumentAnalysisAgent ---\")\n",
    "    doc_analysis_agent_executor = create_document_analysis_agent()\n",
    "    \n",
    "    # Tâche d'analyse pour l'agent.\n",
    "    # Supposons que nous ayons ingéré des articles sur \"explainable artificial intelligence for robotics\".\n",
    "    analysis_task = \"Based on the knowledge base, what are some common techniques for achieving explainability in RL agents used in robotics? Cite any relevant ArXiv IDs if found.\"\n",
    "    logger.info(f\"Tâche pour l'agent d'analyse de documents : '{analysis_task}'\")\n",
    "    \n",
    "    try:\n",
    "        response_doc_analysis = doc_analysis_agent_executor.invoke({\n",
    "            \"messages\": [HumanMessage(content=analysis_task)]\n",
    "        })\n",
    "        analysis_output = response_doc_analysis.get(\"output\")\n",
    "        \n",
    "        print(\"\\n--- Résultats de l'Analyse de Documents (via Agent) ---\")\n",
    "        if analysis_output:\n",
    "            print(analysis_output)\n",
    "        else:\n",
    "            print(\"L'agent d'analyse de documents n'a pas retourné de sortie (vérifiez les logs).\")\n",
    "            print(f\"Réponse complète de l'agent : {response_doc_analysis}\")\n",
    "\n",
    "        if \"intermediate_steps\" in response_doc_analysis:\n",
    "            print(\"\\nÉtapes intermédiaires de l'agent d'Analyse:\")\n",
    "            for step in response_doc_analysis[\"intermediate_steps\"]:\n",
    "                tool_call, tool_result = step\n",
    "                print(f\"  Appel Outil: {tool_call.tool} avec input {tool_call.tool_input}\")\n",
    "                print(f\"  Résultat Outil (extrait): {str(tool_result)[:300]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'invocation du DocumentAnalysisAgent: {e}\", exc_info=True)\n",
    "        print(f\"Erreur lors du test du DocumentAnalysisAgent: {e}. Assurez-vous que RetrievalEngine peut s'initialiser et que la base de données est peuplée et accessible.\")\n",
    "else:\n",
    "    logger.warning(\"Clé API OpenAI ou MONGO_URI manquante, test du DocumentAnalysisAgent sauté.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f5490",
   "metadata": {},
   "source": [
    "### 4. Test du `SynthesisAgent`\n",
    "\n",
    "Cet agent prend des informations analysées (que nous allons simuler ici) et produit une synthèse structurée. Il n'utilise pas d'outils de récupération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4214b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "if settings.OPENAI_API_KEY:\n",
    "    logger.info(\"\\n--- Test du SynthesisAgent ---\")\n",
    "    synthesis_agent_executor = create_synthesis_agent()\n",
    "    \n",
    "    # Préparer un contexte simulé (ce que les agents précédents auraient pu fournir)\n",
    "    simulated_context_for_synthesis = \"\"\"\n",
    "    User Query: What are key considerations for sim-to-real transfer in robotic reinforcement learning?\n",
    "\n",
    "    Information from Document Analysis:\n",
    "    - Chunk from ArXiv ID 123.4567: Sim-to-real transfer often suffers from domain randomization issues. Techniques like domain adaptation and system identification are crucial. Physical parameters like friction and sensor noise are hard to model accurately.\n",
    "    - Chunk from ArXiv ID 789.0123: Using realistic simulators and adding noise during training can improve transfer. Photorealistic rendering helps vision-based policies. Policy distillation from an ensemble of simulation-trained agents is a promising approach.\n",
    "    - ArXiv Search found paper 'Recent Advances in Sim-to-Real for Robotics' (ArXiv:2401.0001), summary: This paper reviews state-of-the-art methods, highlighting the importance of robust learning algorithms and accurate dynamics modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    synthesis_task_message = HumanMessage(content=f\"Based on the provided information below, write a concise summary report on key considerations for sim-to-real transfer in robotic reinforcement learning.\\n\\nProvided Information:\\n{simulated_context_for_synthesis}\")\n",
    "    logger.info(f\"Tâche pour l'agent de synthèse (basée sur contexte simulé).\")\n",
    "\n",
    "    try:\n",
    "        response_synthesis = synthesis_agent_executor.invoke({\n",
    "            \"messages\": [synthesis_task_message] # L'agent doit extraire le contexte et la tâche de ce message\n",
    "        })\n",
    "        synthesized_output = response_synthesis.get(\"output\")\n",
    "        \n",
    "        print(\"\\n--- Sortie de Synthèse (via Agent) ---\")\n",
    "        if synthesized_output:\n",
    "            print(synthesized_output)\n",
    "        else:\n",
    "            print(\"L'agent de synthèse n'a pas retourné de sortie (vérifiez les logs).\")\n",
    "            print(f\"Réponse complète de l'agent : {response_synthesis}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'invocation du SynthesisAgent: {e}\", exc_info=True)\n",
    "        print(f\"Erreur lors du test du SynthesisAgent: {e}\")\n",
    "else:\n",
    "    logger.warning(\"Clé API OpenAI manquante, test du SynthesisAgent sauté.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3397713f",
   "metadata": {},
   "source": [
    "## Conclusion des Tests d'Agents Individuels\n",
    "\n",
    "Ce notebook a permis de tester chaque agent de manière isolée pour vérifier son comportement de base et son interaction avec les outils.\n",
    "Ces tests unitaires sont importants avant d'orchestrer ces agents dans un workflow LangGraph plus complexe (ce que nous ferons dans le notebook suivant)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognitive-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
