{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688b4077",
   "metadata": {},
   "source": [
    "# Notebook 03: Développement et Test des Agents Individuels et de leurs Outils\n",
    "\n",
    "Ce notebook se concentre sur le test et la démonstration de chaque agent que nous avons défini dans `src/agents/agent_architectures.py`. Nous allons instancier chaque agent, lui soumettre des tâches spécifiques, et observer comment il utilise ses outils (définis dans `src/agents/tool_definitions.py`) et comment il génère ses réponses.\n",
    "\n",
    "**Prérequis :**\n",
    "* Avoir exécuté `00_setup_environment.ipynb` (environnement configuré, clés API dans `.env`).\n",
    "* Pour tester `DocumentAnalysisAgent` efficacement, il est préférable d'avoir une base de données MongoDB peuplée via `01_data_ingestion_and_embedding.ipynb` (ou `scripts/run_ingestion.py`) et que le `RetrievalEngine` (utilisé par `knowledge_base_retrieval_tool`) soit fonctionnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63c5c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables d'environnement chargées depuis : /home/facetoface/cognitive-swarm-agents/.env\n",
      "\u001b[34m2025-06-01 22:33:17 - nb_03_agent_testing - INFO - Les agents et certaines outils (comme CrewAI) utiliseront le fournisseur LLM génératif configuré : 'ollama'.\u001b[0m\n",
      "\u001b[34m2025-06-01 22:33:17 - nb_03_agent_testing - INFO - Le 'knowledge_base_retrieval_tool' (via RetrievalEngine) utilisera le fournisseur d'embedding : 'ollama'\u001b[0m\n",
      "\u001b[33m2025-06-01 22:33:17 - nb_03_agent_testing - WARNING - Clé API TAVILY (TAVILY_API_KEY) non configurée (non critique pour les tests de base de ce notebook).\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31662/935265697.py:93: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  if \"TAVILY_API_KEY\" in settings.model_fields and not settings.TAVILY_API_KEY:\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json # Pour un affichage lisible des sorties d'outils\n",
    "from typing import Optional, List # Ajout pour robustesse\n",
    "\n",
    "# --- Configuration du PYTHONPATH et chargement de .env ---\n",
    "# NOTE IMPORTANTE SUR LE CWD (Current Working Directory) :\n",
    "# La ligne suivante `project_root = Path().resolve().parent` suppose que le CWD du notebook\n",
    "# est le dossier `/notebooks/`. Si vous avez configuré VS Code pour que le CWD\n",
    "# soit la racine du projet (`cognitive-swarm-agents/`), alors `Path().resolve()`\n",
    "# donnerait déjà la racine du projet, et vous devriez utiliser :\n",
    "# project_root = Path().resolve()\n",
    "# Vérifiez votre CWD avec `import os; print(os.getcwd())` ou `from pathlib import Path; print(Path().resolve())` pour confirmer.\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "    print(f\"Ajout de {project_root} au PYTHONPATH\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# De même, si CWD est la racine du projet, dotenv_path serait `Path().resolve() / \".env\"`\n",
    "dotenv_path = project_root / \".env\"\n",
    "if dotenv_path.exists():\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    print(f\"Variables d'environnement chargées depuis : {dotenv_path}\")\n",
    "else:\n",
    "    print(f\"ATTENTION: Fichier .env non trouvé à {dotenv_path}. Assurez-vous qu'il est à la racine du projet.\")\n",
    "\n",
    "from config.settings import settings\n",
    "from config.logging_config import setup_logging\n",
    "\n",
    "# Importer les fonctions de création d'agents\n",
    "# get_llm n'est plus importé ici car non utilisé directement ; les fonctions create_..._agent l'utilisent en interne via llm_factory\n",
    "from src.agents.agent_architectures import (\n",
    "    create_research_planner_agent,\n",
    "    create_arxiv_search_agent,\n",
    "    create_document_analysis_agent,\n",
    "    create_synthesis_agent\n",
    "    # get_llm # Supprimé car non utilisé directement dans ce notebook et plus défini dans agent_architectures\n",
    ")\n",
    "\n",
    "# Importer les outils pour d'éventuels tests directs\n",
    "from src.agents.tool_definitions import (\n",
    "    arxiv_search_tool, \n",
    "    knowledge_base_retrieval_tool,\n",
    "    document_deep_dive_analysis_tool # Assurer que cet outil est importé pour test\n",
    ")\n",
    "\n",
    "# Importer les types de messages LangChain\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage # SystemMessage peut être utile\n",
    "\n",
    "setup_logging(level=\"INFO\") # Mettre à DEBUG pour voir les étapes internes des agents\n",
    "logger = logging.getLogger(\"nb_03_agent_testing\")\n",
    "\n",
    "# --- Vérification des prérequis pour les LLMs et Embeddings (logique existante conservée et vérifiée) ---\n",
    "# Pour les agents génératifs (Planner, ArxivSearcher, DocAnalyzer, Synthesizer)\n",
    "# et pour les outils qui pourraient utiliser un LLM (comme document_deep_dive_analysis_tool via CrewAI).\n",
    "active_llm_provider = settings.DEFAULT_LLM_MODEL_PROVIDER.lower()\n",
    "logger.info(f\"Les agents et certaines outils (comme CrewAI) utiliseront le fournisseur LLM génératif configuré : '{active_llm_provider}'.\")\n",
    "if active_llm_provider == \"openai\":\n",
    "    if not settings.OPENAI_API_KEY:\n",
    "        logger.error(f\"ERREUR : Le fournisseur LLM est 'openai', mais OPENAI_API_KEY n'est pas configurée. Les tests des agents/outils utilisant ce LLM échoueront probablement.\")\n",
    "elif active_llm_provider == \"huggingface_api\":\n",
    "    if not settings.HUGGINGFACE_API_KEY:\n",
    "        logger.error(f\"ERREUR : Le fournisseur LLM est 'huggingface_api', mais HUGGINGFACE_API_KEY n'est pas configurée.\")\n",
    "    if not settings.HUGGINGFACE_REPO_ID: # Vérification ajoutée pour être complet\n",
    "        logger.error(f\"ERREUR : Le fournisseur LLM est 'huggingface_api', mais HUGGINGFACE_REPO_ID n'est pas configuré.\")\n",
    "elif active_llm_provider == \"ollama\":\n",
    "    if not settings.OLLAMA_BASE_URL:\n",
    "        logger.error(f\"ERREUR : Le fournisseur LLM est 'ollama', mais OLLAMA_BASE_URL n'est pas configurée.\")\n",
    "    if not settings.OLLAMA_GENERATIVE_MODEL_NAME: # Vérification ajoutée\n",
    "        logger.error(f\"ERREUR : Le fournisseur LLM est 'ollama', mais OLLAMA_GENERATIVE_MODEL_NAME (pour les agents) n'est pas configuré.\")\n",
    "\n",
    "# Pour knowledge_base_retrieval_tool (qui utilise RetrievalEngine, qui utilise le provider d'embedding configuré)\n",
    "active_embedding_provider = settings.DEFAULT_EMBEDDING_PROVIDER.lower()\n",
    "logger.info(f\"Le 'knowledge_base_retrieval_tool' (via RetrievalEngine) utilisera le fournisseur d'embedding : '{active_embedding_provider}'\")\n",
    "if active_embedding_provider == \"openai\": # Pour les embeddings de requête\n",
    "    if not settings.OPENAI_API_KEY:\n",
    "        logger.error(f\"ERREUR : Le fournisseur d'embedding pour RetrievalEngine est 'openai', mais OPENAI_API_KEY n'est pas configurée. Le 'knowledge_base_retrieval_tool' échouera probablement lors de la vectorisation des requêtes.\")\n",
    "elif active_embedding_provider == \"ollama\": # Pour les embeddings de requête\n",
    "    if not settings.OLLAMA_BASE_URL:\n",
    "         logger.error(f\"ERREUR : Le fournisseur d'embedding pour RetrievalEngine est 'ollama', mais OLLAMA_BASE_URL n'est pas configurée.\")\n",
    "    if not settings.OLLAMA_EMBEDDING_MODEL_NAME: # Ajout de cette vérification cruciale\n",
    "        logger.error(f\"ERREUR : Le fournisseur d'embedding pour RetrievalEngine est 'ollama', mais OLLAMA_EMBEDDING_MODEL_NAME n'est pas configuré.\")\n",
    "# Pour \"huggingface\" (local), aucune clé API spécifique n'est requise pour l'embedding des requêtes.\n",
    "\n",
    "if not settings.MONGO_URI: \n",
    "    logger.error(\"ERREUR : MONGO_URI non trouvé. Le 'knowledge_base_retrieval_tool' (via RetrievalEngine) ne pourra pas se connecter à la base de données.\")\n",
    "\n",
    "# TAVILY_API_KEY n'est pas utilisé par les outils testés directement dans les cellules suivantes de ce notebook,\n",
    "# mais la vérification est conservée pour information générale si settings.py le liste.\n",
    "if \"TAVILY_API_KEY\" in settings.model_fields and not settings.TAVILY_API_KEY: \n",
    "    logger.warning(\"Clé API TAVILY (TAVILY_API_KEY) non configurée (non critique pour les tests de base de ce notebook).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a730b9e",
   "metadata": {},
   "source": [
    "### 1. Test du `ResearchPlannerAgent`\n",
    "\n",
    "Cet agent est conçu pour prendre une requête utilisateur complexe et la décomposer en un plan de recherche structuré. Il n'utilise pas d'outils pour cette tâche, se basant uniquement sur ses instructions (prompt système) et le LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4dd948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-01 22:33:17 - nb_03_agent_testing - INFO - --- Test du ResearchPlannerAgent ---\u001b[0m\n",
      "\u001b[34m2025-06-01 22:33:17 - nb_03_agent_testing - INFO - Tentative d'utilisation du provider LLM configuré par défaut: 'ollama' pour le ResearchPlannerAgent.\u001b[0m\n",
      "\u001b[34m2025-06-01 22:33:17 - src.llm_services.llm_factory - INFO - Initializing LLM from llm_factory for provider: 'ollama' with temperature: 0.0\u001b[0m\n",
      "\u001b[34m2025-06-01 22:33:17 - src.llm_services.llm_factory - INFO - Using Ollama model: mistral via http://localhost:11434\u001b[0m\n",
      "\u001b[34m2025-06-01 22:33:17 - src.agents.agent_architectures - INFO - Research Planner Agent created.\u001b[0m\n",
      "\u001b[34m2025-06-01 22:33:17 - nb_03_agent_testing - INFO - Requête pour le planificateur : 'What are the latest trends and key challenges in applying deep reinforcement learning to multi-robot navigation and coordination, particularly for swarm robotics?'\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/facetoface/cognitive-swarm-agents/src/llm_services/llm_factory.py:146: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  return ChatOllama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Plan de Recherche Généré ---\n",
      "1. **Key Questions:**\n",
      "   - What are the current state-of-the-art methods in deep reinforcement learning (DRL) for multi-robot navigation and coordination?\n",
      "   - How are these DRL methods applied specifically to swarm robotics?\n",
      "   - What are the key challenges encountered when applying DRL to multi-robot navigation and coordination in swarm robotics?\n",
      "   - Are there any recent advancements or solutions proposed to address these challenges?\n",
      "   - How effective are these solutions in improving the performance of DRL for multi-robot navigation and coordination in swarm robotics?\n",
      "\n",
      "2. **Information Sources:**\n",
      "   - ArXiv preprints related to \"Deep Reinforcement Learning\" AND \"Multi-Robot Navigation\" AND \"Swarm Robotics\".\n",
      "   - Recent peer-reviewed journal articles from IEEE Transactions on Robotics, Journal of Field Robotics, and Autonomous Agents and Multi-Agent Systems.\n",
      "   - Relevant conference proceedings such as ICRA (IEEE International Conference on Robotics and Automation), RSS (Robotics: Science and Systems), and AAAI (Association for Advancement of Artificial Intelligence) conferences.\n",
      "\n",
      "3. **Search Queries:**\n",
      "   - \"Deep Reinforcement Learning AND Multi-Robot Navigation AND Swarm Robotics\" site:arxiv.org\n",
      "   - \"Deep reinforcement learning in swarm robotics\" (journal name: IEEE Transactions on Robotics OR Journal of Field Robotics OR Autonomous Agents and Multi-Agent Systems)\n",
      "   - \"Multi-robot navigation using deep reinforcement learning in swarm robotics\" (conference name: ICRA OR RSS OR AAAI)\n",
      "\n",
      "4. **Analysis Steps:**\n",
      "   - Extract the methods, challenges, and solutions from each source.\n",
      "   - Compare and contrast the similarities and differences between the methods and their effectiveness in addressing the challenges.\n",
      "   - Identify trends and patterns in the approaches used for DRL in multi-robot navigation and coordination for swarm robotics.\n",
      "   - Evaluate the robustness, scalability, and generalizability of these methods in real-world applications.\n",
      "\n",
      "5. **Final Output Structure:**\n",
      "The final report should provide an overview of the current trends and challenges in applying DRL to multi-robot navigation and coordination for swarm robotics. It should include a summary of the state-of-the-art methods, their key features, and their effectiveness in addressing the identified challenges. The report should also discuss recent advancements or solutions proposed to improve the performance of these methods and provide recommendations for future research directions. The report should be structured in a clear and concise manner, with each section focusing on a specific aspect of the topic. It should include relevant citations and references for further reading.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"--- Test du ResearchPlannerAgent ---\")\n",
    "# La première cellule de ce notebook (ID a57d7cbb) logue déjà des avertissements \n",
    "# si la configuration pour le active_llm_provider (DEFAULT_LLM_MODEL_PROVIDER) est manquante.\n",
    "# Ce test tentera d'utiliser ce provider configuré.\n",
    "logger.info(f\"Tentative d'utilisation du provider LLM configuré par défaut: '{settings.DEFAULT_LLM_MODEL_PROVIDER}' pour le ResearchPlannerAgent.\")\n",
    "\n",
    "try:\n",
    "    # create_research_planner_agent() appelle get_llm() qui utilise \n",
    "    # settings.DEFAULT_LLM_MODEL_PROVIDER et gère la configuration.\n",
    "    # Une ValueError sera levée par get_llm si la configuration est incorrecte pour le provider choisi.\n",
    "    planner_agent_executor = create_research_planner_agent() \n",
    "    \n",
    "    # Requête utilisateur d'exemple (inchangée)\n",
    "    user_query_plan = \"What are the latest trends and key challenges in applying deep reinforcement learning to multi-robot navigation and coordination, particularly for swarm robotics?\"\n",
    "    logger.info(f\"Requête pour le planificateur : '{user_query_plan}'\")\n",
    "    \n",
    "    # Invocation de l'agent\n",
    "    response_planner = planner_agent_executor.invoke({\n",
    "        \"messages\": [HumanMessage(content=user_query_plan)]\n",
    "    })\n",
    "    research_plan = response_planner.get(\"output\")\n",
    "    \n",
    "    print(\"\\n--- Plan de Recherche Généré ---\")\n",
    "    if research_plan:\n",
    "        print(research_plan)\n",
    "    else:\n",
    "        print(\"L'agent planificateur n'a pas retourné de plan (vérifiez les logs).\")\n",
    "        # Afficher la réponse complète peut aider au débogage si 'output' est vide mais la réponse existe\n",
    "        print(f\"Réponse complète de l'agent (si output est vide) : {response_planner}\")\n",
    "            \n",
    "except ValueError as ve:\n",
    "    # Attrape spécifiquement les erreurs de configuration de get_llm()\n",
    "    logger.error(f\"Erreur de configuration LLM pour le ResearchPlannerAgent (provider: {settings.DEFAULT_LLM_MODEL_PROVIDER}): {ve}\", exc_info=True)\n",
    "    print(f\"ERREUR de configuration pour ResearchPlannerAgent : Le provider LLM '{settings.DEFAULT_LLM_MODEL_PROVIDER}' n'est pas correctement configuré (ex: clé API ou URL manquante). Détails : {ve}\")\n",
    "except Exception as e:\n",
    "    # Attrape les autres erreurs pendant l'invocation de l'agent\n",
    "    logger.error(f\"Erreur lors de l'invocation du ResearchPlannerAgent: {e}\", exc_info=True)\n",
    "    print(f\"Erreur inattendue lors du test du ResearchPlannerAgent: {e}\")\n",
    "\n",
    "# L'ancien bloc 'else' qui vérifiait settings.OPENAI_API_KEY est supprimé,\n",
    "# car la logique try/except ci-dessus est plus générale et correcte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a8931e",
   "metadata": {},
   "source": [
    "### 2. Test du `ArxivSearchAgent` (avec `arxiv_search_tool`)\n",
    "\n",
    "Cet agent est spécialisé dans la recherche d'articles sur ArXiv. Il utilise `arxiv_search_tool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da937caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-01 22:34:25 - nb_03_agent_testing - INFO - \n",
      "--- Test du ArxivSearchAgent ---\u001b[0m\n",
      "\u001b[34m2025-06-01 22:34:25 - nb_03_agent_testing - INFO - Tentative d'utilisation du provider LLM configuré par défaut: 'ollama' pour l'ArxivSearchAgent.\u001b[0m\n",
      "\u001b[34m2025-06-01 22:34:25 - src.llm_services.llm_factory - INFO - Initializing LLM from llm_factory for provider: 'ollama' with temperature: 0.0\u001b[0m\n",
      "\u001b[34m2025-06-01 22:34:25 - src.llm_services.llm_factory - INFO - Using Ollama model: mistral via http://localhost:11434\u001b[0m\n",
      "\u001b[34m2025-06-01 22:34:25 - src.agents.agent_architectures - INFO - ArXiv Search Agent created with tools: ['arxiv_search_tool']\u001b[0m\n",
      "\u001b[34m2025-06-01 22:34:25 - nb_03_agent_testing - INFO - Tâche pour l'agent de recherche ArXiv : 'Find 2 recent papers (last 6 months) on 'explainable reinforcement learning in robotics' sorted by submission date.'\u001b[0m\n",
      "\n",
      "--- Résultats de la Recherche ArXiv (via Agent) ---\n",
      "Impossible de parser la sortie comme JSON. Sortie brute :\n",
      " I have used the `arxiv_search_tool` to perform a search for recent papers (within the last 6 months) on 'explainable reinforcement learning in robotics'. Here are the results:\n",
      "\n",
      "1. Title: Explaining Deep Reinforcement Learning for Robotics: A Survey\n",
      "   Abstract: This paper provides an overview of explainability methods for deep reinforcement learning (DRL) in robotics, discussing their applications and challenges. It also presents a taxonomy of existing explainability techniques and discusses future research directions.\n",
      "\n",
      "2. Title: Interpretable Reinforcement Learning for Robotics: A Case Study on Human-Robot Collaboration\n",
      "   Abstract: This paper proposes an interpretable reinforcement learning (IRL) approach for human-robot collaboration tasks. The method uses a combination of inverse reinforcement learning and model-based RL to learn policies that are both effective and interpretable.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\n--- Test du ArxivSearchAgent ---\")\n",
    "logger.info(f\"Tentative d'utilisation du provider LLM configuré par défaut: '{settings.DEFAULT_LLM_MODEL_PROVIDER}' pour l'ArxivSearchAgent.\")\n",
    "\n",
    "try:\n",
    "    # create_arxiv_search_agent() appelle get_llm() qui utilise settings.DEFAULT_LLM_MODEL_PROVIDER\n",
    "    arxiv_agent_executor = create_arxiv_search_agent()\n",
    "    \n",
    "    search_task = \"Find 2 recent papers (last 6 months) on 'explainable reinforcement learning in robotics' sorted by submission date.\"\n",
    "    logger.info(f\"Tâche pour l'agent de recherche ArXiv : '{search_task}'\")\n",
    "    \n",
    "    response_arxiv_search = arxiv_agent_executor.invoke({\n",
    "        \"messages\": [HumanMessage(content=search_task)]\n",
    "    })\n",
    "    \n",
    "    arxiv_results_output = response_arxiv_search.get(\"output\")\n",
    "    \n",
    "    print(\"\\n--- Résultats de la Recherche ArXiv (via Agent) ---\")\n",
    "    if arxiv_results_output:\n",
    "        # La sortie de arxiv_search_tool est une liste de dictionnaires (ou un dict d'erreur)\n",
    "        # Essayons de l'afficher de manière lisible\n",
    "        try:\n",
    "            # Si la sortie est une chaîne JSON, la parser. Sinon, l'afficher telle quelle.\n",
    "            # L'outil retourne une liste de dicts, mais l'agent LLM pourrait la wrapper en chaîne.\n",
    "            if isinstance(arxiv_results_output, str):\n",
    "                try:\n",
    "                    # Remplacer les apostrophes simples par des guillemets doubles pour une meilleure compatibilité JSON\n",
    "                    # Attention: ceci est une heuristique et pourrait ne pas marcher pour tous les cas.\n",
    "                    # Une meilleure solution serait que l'agent retourne directement un objet JSON valide ou un type de données structuré.\n",
    "                    corrected_json_string = arxiv_results_output.replace(\"'\", \"\\\"\") \n",
    "                    # Gérer les booléens Python True/False qui ne sont pas valides en JSON (true/false)\n",
    "                    corrected_json_string = corrected_json_string.replace(\"True\", \"true\").replace(\"False\", \"false\")\n",
    "                    # Gérer None qui n'est pas valide en JSON (null)\n",
    "                    corrected_json_string = corrected_json_string.replace(\"None\", \"null\")\n",
    "\n",
    "                    parsed_output = json.loads(corrected_json_string)\n",
    "                    print(json.dumps(parsed_output, indent=2, ensure_ascii=False))\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Impossible de parser la sortie comme JSON. Sortie brute :\\n{arxiv_results_output}\")\n",
    "            elif isinstance(arxiv_results_output, list) or isinstance(arxiv_results_output, dict): # Si c'est déjà un objet Python\n",
    "                print(json.dumps(arxiv_results_output, indent=2, ensure_ascii=False))\n",
    "            else: # Autre type, afficher directement\n",
    "                 print(arxiv_results_output)\n",
    "        except Exception as e_print:\n",
    "            print(f\"Erreur lors de l'affichage formaté des résultats : {e_print}\")\n",
    "            print(\"Sortie brute de l'agent :\")\n",
    "            print(arxiv_results_output)\n",
    "    else:\n",
    "        print(\"L'agent de recherche ArXiv n'a pas retourné de sortie (vérifiez les logs).\")\n",
    "        print(f\"Réponse complète de l'agent (si output est vide) : {response_arxiv_search}\")\n",
    "\n",
    "    # Affichage des étapes intermédiaires (si disponibles et utiles)\n",
    "    if \"intermediate_steps\" in response_arxiv_search and response_arxiv_search[\"intermediate_steps\"]:\n",
    "        print(\"\\nÉtapes intermédiaires de l'agent ArXiv:\")\n",
    "        for step in response_arxiv_search[\"intermediate_steps\"]:\n",
    "            # La structure de 'step' peut varier selon le type d'agent (AgentAction vs ToolCall)\n",
    "            if hasattr(step[0], 'tool') and hasattr(step[0], 'tool_input'): # Pour AgentAction\n",
    "                tool_call_info = f\"Outil: {step[0].tool}, Input: {step[0].tool_input}\"\n",
    "            elif isinstance(step[0], dict) and 'tool' in step[0] and 'tool_input' in step[0]: # Autre format possible\n",
    "                tool_call_info = f\"Outil: {step[0]['tool']}, Input: {step[0]['tool_input']}\"\n",
    "            else: # Fallback\n",
    "                tool_call_info = str(step[0])\n",
    "\n",
    "            tool_result = step[1]\n",
    "            print(f\"  Appel Outil: {tool_call_info}\")\n",
    "            print(f\"  Résultat Outil (extrait): {str(tool_result)[:300]}...\")\n",
    "            \n",
    "except ValueError as ve:\n",
    "    logger.error(f\"Erreur de configuration LLM pour l'ArxivSearchAgent (provider: {settings.DEFAULT_LLM_MODEL_PROVIDER}): {ve}\", exc_info=True)\n",
    "    print(f\"ERREUR de configuration pour ArxivSearchAgent : Le provider LLM '{settings.DEFAULT_LLM_MODEL_PROVIDER}' n'est pas correctement configuré. Détails : {ve}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur lors de l'invocation de l'ArxivSearchAgent: {e}\", exc_info=True)\n",
    "    print(f\"Erreur inattendue lors du test de l'ArxivSearchAgent: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e392505",
   "metadata": {},
   "source": [
    "### 2b. Test Direct de `arxiv_search_tool` (Optionnel)\n",
    "\n",
    "Pour mieux comprendre ce que l'outil `arxiv_search_tool` retourne, nous pouvons l'appeler directement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7871d004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-01 22:34:50 - nb_03_agent_testing - INFO - \n",
      "--- Test Direct de arxiv_search_tool ---\u001b[0m\n",
      "\u001b[34m2025-06-01 22:34:50 - src.agents.tool_definitions - INFO - Executing arxiv_search_tool with query='transformer models for robot control', max_results=1, sort_by='submittedDate', sort_order='descending'\u001b[0m\n",
      "\u001b[34m2025-06-01 22:34:50 - arxiv - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=transformer+models+for+robot+control&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100\u001b[0m\n",
      "\u001b[34m2025-06-01 22:35:00 - arxiv - INFO - Got first page: 100 of 2387721 total results\u001b[0m\n",
      "\u001b[34m2025-06-01 22:35:00 - src.agents.tool_definitions - INFO - arxiv_search_tool found 1 papers.\u001b[0m\n",
      "\n",
      "Résultats Directs de arxiv_search_tool:\n",
      "[\n",
      "  {\n",
      "    \"entry_id\": \"http://arxiv.org/abs/2505.23769v1\",\n",
      "    \"title\": \"TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models\",\n",
      "    \"authors\": [\n",
      "      \"Yao Xiao\",\n",
      "      \"Qiqian Fu\",\n",
      "      \"Heyi Tao\",\n",
      "      \"Yuqun Wu\",\n",
      "      \"Zhen Zhu\",\n",
      "      \"Derek Hoiem\"\n",
      "    ],\n",
      "    \"summary\": \"Image-text models excel at image-level tasks but struggle with detailed visual understanding. While these models provide strong visual-language alignment, segmentation models like SAM2 offer precise spatial boundaries for objects. To this end, we propose TextRegion, a simple, effective, and training-free framework that combines the strengths of image-text models and SAM2 to generate powerful text-aligned region tokens. These tokens enable detailed visual understanding while preserving open-vocabulary capabilities. They can be directly applied to various downstream tasks, including open-world semantic segmentation, referring expression comprehension, and grounding. We conduct extensive evaluations and consistently achieve superior or competitive performance compared to state-of-the-art training-free methods. Additionally, our framework is compatible with many image-text models, making it highly practical and easily extensible as stronger models emerge. Code is available at: https://github.com/avaxiao/TextRegion.\",\n",
      "    \"published_date\": \"2025-05-29T17:59:59+00:00\",\n",
      "    \"pdf_url\": \"http://arxiv.org/pdf/2505.23769v1\",\n",
      "    \"primary_category\": \"cs.CV\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\n--- Test Direct de arxiv_search_tool ---\")\n",
    "try:\n",
    "    direct_tool_results = arxiv_search_tool.invoke({\n",
    "        \"query\": \"transformer models for robot control\", \n",
    "        \"max_results\": 1,\n",
    "        \"sort_by\": \"submittedDate\"\n",
    "    })\n",
    "    print(\"\\nRésultats Directs de arxiv_search_tool:\")\n",
    "    # Ajout de ensure_ascii=False pour un meilleur affichage des caractères spéciaux\n",
    "    print(json.dumps(direct_tool_results, indent=2, ensure_ascii=False))\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur lors de l'appel direct à arxiv_search_tool: {e}\", exc_info=True)\n",
    "    print(f\"Erreur lors de l'appel direct à arxiv_search_tool: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f521da5",
   "metadata": {},
   "source": [
    "### 3. Test du `DocumentAnalysisAgent` (avec `knowledge_base_retrieval_tool`)\n",
    "\n",
    "Cet agent analyse les documents récupérés de notre base de connaissances (MongoDB). Son bon fonctionnement dépend de la présence de données pertinentes dans la collection (par exemple, `{COLLECTION_NAME_FOR_RAG_TEST}`) et de la fonctionnalité du `RetrievalEngine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f06850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-01 22:35:00 - nb_03_agent_testing - INFO - \n",
      "--- Test du DocumentAnalysisAgent ---\u001b[0m\n",
      "\u001b[34m2025-06-01 22:35:00 - nb_03_agent_testing - INFO - Tentative d'utilisation du provider LLM configuré par défaut: 'ollama' pour le DocumentAnalysisAgent.\u001b[0m\n",
      "\u001b[34m2025-06-01 22:35:00 - nb_03_agent_testing - INFO - Ce test dépend aussi du provider d'embedding configuré: 'ollama' (pour knowledge_base_retrieval_tool via RetrievalEngine) et de MongoDB.\u001b[0m\n",
      "\u001b[34m2025-06-01 22:35:00 - src.llm_services.llm_factory - INFO - Initializing LLM from llm_factory for provider: 'ollama' with temperature: 0.0\u001b[0m\n",
      "\u001b[34m2025-06-01 22:35:00 - src.llm_services.llm_factory - INFO - Using Ollama model: mistral via http://localhost:11434\u001b[0m\n",
      "\u001b[34m2025-06-01 22:35:00 - src.agents.agent_architectures - INFO - Document Analysis Agent created with tools: ['knowledge_base_retrieval_tool', 'document_deep_dive_analysis_tool']\u001b[0m\n",
      "\u001b[34m2025-06-01 22:35:00 - nb_03_agent_testing - INFO - Tâche pour l'agent d'analyse de documents : 'Based on the knowledge base, what are some common techniques for achieving explainability in RL agents used in robotics? Cite any relevant ArXiv IDs if found.'\u001b[0m\n",
      "\n",
      "--- Résultats de l'Analyse de Documents (via Agent) ---\n",
      " To answer your question, I'll use the `knowledge_base_retrieval_tool` to fetch relevant chunks of text from the ingested knowledge base based on the topic \"common techniques for achieving explainability in RL agents used in robotics\".\n",
      "\n",
      "Here are some common techniques found:\n",
      "\n",
      "1. Visualization-based methods: These techniques help visualize the internal states, actions, and rewards of an RL agent to understand its decision-making process. Examples include:\n",
      "   - \"Visualizing Decisions in Deep Reinforcement Learning for Robotics\" (arXiv:1806.05943)\n",
      "   - \"Explainable Reinforcement Learning for Robotic Manipulation\" (arXiv:2007.08822)\n",
      "\n",
      "2. Interpretability methods: These techniques aim to make the learned policies and value functions of an RL agent interpretable by humans. Examples include:\n",
      "   - \"Interpretable Reinforcement Learning for Robotics\" (arXiv:1906.07584)\n",
      "   - \"Towards Interpretable Deep Reinforcement Learning for Robotics\" (arXiv:2103.06838)\n",
      "\n",
      "3. Model-based methods: These techniques involve learning an explicit model of the environment, which can be used to explain the agent's behavior. Examples include:\n",
      "   - \"Model-Based Reinforcement Learning for Robotics\" (arXiv:1709.05864)\n",
      "   - \"Learning and Explaining Dynamics Models with Deep Reinforcement Learning\" (arXiv:2003.08038)\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\n--- Test du DocumentAnalysisAgent ---\")\n",
    "logger.info(f\"Tentative d'utilisation du provider LLM configuré par défaut: '{settings.DEFAULT_LLM_MODEL_PROVIDER}' pour le DocumentAnalysisAgent.\")\n",
    "logger.info(f\"Ce test dépend aussi du provider d'embedding configuré: '{settings.DEFAULT_EMBEDDING_PROVIDER}' (pour knowledge_base_retrieval_tool via RetrievalEngine) et de MongoDB.\")\n",
    "\n",
    "# La connexion à MongoDB est essentielle pour les outils de cet agent.\n",
    "if not settings.MONGO_URI:\n",
    "    logger.error(\"MONGO_URI non configuré. Test du DocumentAnalysisAgent sauté car ses outils (knowledge_base_retrieval_tool) en dépendent crucialement.\")\n",
    "    print(\"ERREUR: MONGO_URI non configuré dans .env. Le DocumentAnalysisAgent ne peut pas être testé correctement sans accès à la base de données.\")\n",
    "else:\n",
    "    try:\n",
    "        # La création de l'agent peut échouer si get_llm() (appelé en interne) échoue \n",
    "        # en raison d'une configuration manquante pour le DEFAULT_LLM_MODEL_PROVIDER.\n",
    "        # De même, RetrievalEngine (utilisé par knowledge_base_retrieval_tool) peut échouer à l'init\n",
    "        # si DEFAULT_EMBEDDING_PROVIDER est mal configuré.\n",
    "        doc_analysis_agent_executor = create_document_analysis_agent()\n",
    "        \n",
    "        # Tâche d'analyse exemple.\n",
    "        # Le DocumentAnalysisAgent est maintenant équipé du document_deep_dive_analysis_tool.\n",
    "        # Son prompt système (DOCUMENT_ANALYSIS_SYSTEM_PROMPT_V2) le guide sur quand utiliser quel outil.\n",
    "        # Voici une tâche qui devrait utiliser knowledge_base_retrieval_tool :\n",
    "        analysis_task = \"Based on the knowledge base, what are some common techniques for achieving explainability in RL agents used in robotics? Cite any relevant ArXiv IDs if found.\"\n",
    "        \n",
    "        # Pour tester le document_deep_dive_analysis_tool, la tâche devrait être formulée différemment,\n",
    "        # par exemple, en demandant explicitement une \"deep dive\" sur un document spécifique.\n",
    "        # Exemple :\n",
    "        # analysis_task = \"Perform a detailed, structured deep dive analysis on document 'XXXX.YYYYY' (you will need to fetch its content first if not provided) focusing on its methodology and limitations for sim-to-real transfer.\"\n",
    "        # Pour un tel test, assurez-vous que le document 'XXXX.YYYYY' existe ou que l'agent peut le récupérer.\n",
    "        \n",
    "        logger.info(f\"Tâche pour l'agent d'analyse de documents : '{analysis_task}'\")\n",
    "        \n",
    "        response_doc_analysis = doc_analysis_agent_executor.invoke({\n",
    "            \"messages\": [HumanMessage(content=analysis_task)]\n",
    "        })\n",
    "        analysis_output = response_doc_analysis.get(\"output\")\n",
    "        \n",
    "        print(\"\\n--- Résultats de l'Analyse de Documents (via Agent) ---\")\n",
    "        if analysis_output:\n",
    "            # La sortie peut être une simple chaîne ou un rapport structuré (ex: de CrewAI via le deep_dive_tool)\n",
    "            if isinstance(analysis_output, str) and analysis_output.lower().startswith(\"error:\"):\n",
    "                print(f\"L'agent ou un outil a retourné une ERREUR gérée : {analysis_output}\")\n",
    "            else:\n",
    "                # Tenter d'afficher en Markdown si la sortie est un rapport formaté (souvent le cas pour CrewAI)\n",
    "                # ou en JSON si c'est une structure de données, sinon en texte brut.\n",
    "                if isinstance(analysis_output, str) and (\"\\n## \" in analysis_output or \"\\n### \" in analysis_output or \"```\" in analysis_output) : # Heuristique pour Markdown\n",
    "                    try:\n",
    "                        from IPython.display import display, Markdown\n",
    "                        print(\"Affichage de la sortie comme Markdown:\")\n",
    "                        display(Markdown(analysis_output))\n",
    "                    except ImportError:\n",
    "                        print(analysis_output) # Fallback\n",
    "                elif isinstance(analysis_output, str) and analysis_output.strip().startswith((\"{\", \"[\")): # Heuristique pour JSON\n",
    "                    try:\n",
    "                        parsed_json = json.loads(analysis_output)\n",
    "                        print(\"Sortie formatée comme JSON:\")\n",
    "                        print(json.dumps(parsed_json, indent=2, ensure_ascii=False))\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(analysis_output) # Fallback\n",
    "                else: # Si c'est déjà un dict/list ou une chaîne simple\n",
    "                    if isinstance(analysis_output, (dict, list)):\n",
    "                        print(json.dumps(analysis_output, indent=2, ensure_ascii=False))\n",
    "                    else:\n",
    "                        print(analysis_output)\n",
    "        else:\n",
    "            print(\"L'agent d'analyse de documents n'a pas retourné de sortie ('output' est vide/None).\")\n",
    "            print(f\"Réponse complète de l'agent (pour débogage) : {response_doc_analysis}\")\n",
    "\n",
    "        # Affichage des étapes intermédiaires\n",
    "        if \"intermediate_steps\" in response_doc_analysis and response_doc_analysis[\"intermediate_steps\"]:\n",
    "            print(\"\\nÉtapes intermédiaires de l'agent d'Analyse:\")\n",
    "            for step in response_doc_analysis[\"intermediate_steps\"]:\n",
    "                tool_call_info = \"N/A\"\n",
    "                # La structure de step[0] (l'action) peut varier.\n",
    "                action_part = step[0]\n",
    "                if hasattr(action_part, 'tool') and hasattr(action_part, 'tool_input'): # AgentAction\n",
    "                    tool_call_info = f\"Outil: {action_part.tool}, Input: {action_part.tool_input}\"\n",
    "                elif isinstance(action_part, list) and action_part and isinstance(action_part[0], dict) and 'tool_name' in action_part[0]: # Possible format pour tool_calls multiples\n",
    "                     tool_call_info = f\"Outil(s): {', '.join([tc.get('tool_name', 'unknown') for tc in action_part])}\"\n",
    "                elif isinstance(action_part, dict) and 'tool' in action_part and 'tool_input' in action_part: # Autre format possible\n",
    "                     tool_call_info = f\"Outil: {action_part['tool']}, Input: {action_part['tool_input']}\"\n",
    "                else: # Fallback\n",
    "                    tool_call_info = str(action_part)\n",
    "\n",
    "                tool_result = step[1] # Observation\n",
    "                print(f\"  Appel Outil/Action: {tool_call_info}\")\n",
    "                print(f\"  Résultat Outil (extrait): {str(tool_result)[:300]}...\")\n",
    "                \n",
    "    except ValueError as ve:\n",
    "        logger.error(f\"Erreur de configuration ou de valeur lors de la création/invocation du DocumentAnalysisAgent. Vérifiez les configurations pour le provider LLM '{settings.DEFAULT_LLM_MODEL_PROVIDER}' et le provider d'embedding '{settings.DEFAULT_EMBEDDING_PROVIDER}'. Détails: {ve}\", exc_info=True)\n",
    "        print(f\"ERREUR (DocumentAnalysisAgent): Problème de configuration LLM ou Embedding. Provider LLM: '{settings.DEFAULT_LLM_MODEL_PROVIDER}', Provider Embedding: '{settings.DEFAULT_EMBEDDING_PROVIDER}'. Détails: {ve}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'invocation du DocumentAnalysisAgent: {e}\", exc_info=True)\n",
    "        print(f\"Erreur inattendue lors du test du DocumentAnalysisAgent: {e}. Assurez-vous que RetrievalEngine peut s'initialiser, que MongoDB est accessible et peuplé, et que les configurations LLM/Embedding sont correctes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f5490",
   "metadata": {},
   "source": [
    "### 4. Test du `SynthesisAgent`\n",
    "\n",
    "Cet agent prend des informations analysées (que nous allons simuler ici) et produit une synthèse structurée. Il n'utilise pas d'outils de récupération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e225861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-01 22:35:52 - nb_03_agent_testing - INFO - \n",
      "--- Test du SynthesisAgent ---\u001b[0m\n",
      "\u001b[34m2025-06-01 22:35:52 - nb_03_agent_testing - INFO - Tentative d'utilisation du provider LLM configuré par défaut: 'ollama' pour le SynthesisAgent.\u001b[0m\n",
      "\u001b[34m2025-06-01 22:35:52 - src.llm_services.llm_factory - INFO - Initializing LLM from llm_factory for provider: 'ollama' with temperature: 0.5\u001b[0m\n",
      "\u001b[34m2025-06-01 22:35:52 - src.llm_services.llm_factory - INFO - Using Ollama model: mistral via http://localhost:11434\u001b[0m\n",
      "\u001b[34m2025-06-01 22:35:52 - src.agents.agent_architectures - INFO - Synthesis Agent created.\u001b[0m\n",
      "\u001b[34m2025-06-01 22:35:52 - nb_03_agent_testing - INFO - Tâche pour l'agent de synthèse (basée sur un contexte simulé).\u001b[0m\n",
      "\n",
      "--- Sortie de Synthèse (via Agent) ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " Title: Key Considerations for Sim-to-Real Transfer in Robotic Reinforcement Learning\n",
       "\n",
       "   Introduction:\n",
       "      The process of transferring learned policies from simulation to real-world robotic environments (sim-to-real transfer) is a critical aspect of robotic reinforcement learning. This report summarizes key considerations based on the provided analysis of relevant literature.\n",
       "\n",
       "   Domain Adaptation and System Identification:\n",
       "      One of the primary challenges in sim-to-real transfer is domain randomization issues. Techniques such as domain adaptation and system identification are crucial for bridging the gap between simulation and reality (ArXiv ID 123.4567). These methods help to account for differences in physical parameters like friction and sensor noise, which are challenging to model accurately.\n",
       "\n",
       "   Realistic Simulators and Noise Addition:\n",
       "      Using realistic simulators during training can significantly improve the transfer of policies to real-world environments (ArXiv ID 789.0123). Additionally, adding noise during training can help make the learned policy more robust to real-world variability. Photorealistic rendering is particularly beneficial for vision-based policies.\n",
       "\n",
       "   Robust Learning Algorithms and Accurate Dynamics Modeling:\n",
       "      A comprehensive review of state-of-the-art methods in sim-to-real transfer for robotics highlights the importance of robust learning algorithms and accurate dynamics modeling (ArXiv:2401.0001). This implies that the choice of reinforcement learning algorithm should be based on its ability to handle real-world uncertainties and variations.\n",
       "\n",
       "   Policy Distillation:\n",
       "      A promising approach for sim-to-real transfer is policy distillation from an ensemble of simulation-trained agents (ArXiv ID 789.0123). This method leverages the diversity in policies learned by multiple agents to create a more robust and adaptable policy for real-world deployment.\n",
       "\n",
       "   Conclusion:\n",
       "      To achieve successful sim-to-real transfer in robotic reinforcement learning, it is essential to consider domain adaptation, system identification, realistic simulation environments, noise addition, robust learning algorithms, accurate dynamics modeling, and policy distillation from an ensemble of agents. These key considerations will help ensure that learned policies can effectively adapt to the real-world variability and uncertainties they encounter during deployment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\"\\n--- Test du SynthesisAgent ---\")\n",
    "# create_synthesis_agent() dans agent_architectures.py appelle get_llm() \n",
    "# avec SYNTHESIS_LLM_TEMPERATURE depuis llm_factory.py.\n",
    "# Il utilisera le settings.DEFAULT_LLM_MODEL_PROVIDER.\n",
    "logger.info(f\"Tentative d'utilisation du provider LLM configuré par défaut: '{settings.DEFAULT_LLM_MODEL_PROVIDER}' pour le SynthesisAgent.\")\n",
    "\n",
    "try:\n",
    "    # La création de l'agent peut échouer si get_llm() (appelé en interne) échoue \n",
    "    # en raison d'une configuration manquante pour le DEFAULT_LLM_MODEL_PROVIDER.\n",
    "    synthesis_agent_executor = create_synthesis_agent()\n",
    "    \n",
    "    # Préparer un contexte simulé (inchangé par rapport à la version originale de la cellule)\n",
    "    simulated_context_for_synthesis = \"\"\"\n",
    "    User Query: What are key considerations for sim-to-real transfer in robotic reinforcement learning?\n",
    "\n",
    "    Information from Document Analysis:\n",
    "    - Chunk from ArXiv ID 123.4567: Sim-to-real transfer often suffers from domain randomization issues. Techniques like domain adaptation and system identification are crucial. Physical parameters like friction and sensor noise are hard to model accurately.\n",
    "    - Chunk from ArXiv ID 789.0123: Using realistic simulators and adding noise during training can improve transfer. Photorealistic rendering helps vision-based policies. Policy distillation from an ensemble of simulation-trained agents is a promising approach.\n",
    "    - ArXiv Search found paper 'Recent Advances in Sim-to-Real for Robotics' (ArXiv:2401.0001), summary: This paper reviews state-of-the-art methods, highlighting the importance of robust learning algorithms and accurate dynamics modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Le message pour l'agent de synthèse inclut maintenant le contexte directement.\n",
    "    # Le prompt de l'agent de synthèse (SYNTHESIS_AGENT_SYSTEM_PROMPT) lui indique de travailler\n",
    "    # avec les informations fournies dans les messages.\n",
    "    synthesis_task_message = HumanMessage(\n",
    "        content=f\"Based on the provided information below, write a concise summary report on the key considerations for sim-to-real transfer in robotic reinforcement learning. The report should be well-structured and directly address the initial user query mentioned in the context.\\n\\nProvided Information:\\n{simulated_context_for_synthesis}\"\n",
    "    )\n",
    "    logger.info(\"Tâche pour l'agent de synthèse (basée sur un contexte simulé).\")\n",
    "\n",
    "    response_synthesis = synthesis_agent_executor.invoke({\n",
    "        \"messages\": [synthesis_task_message] # L'historique des messages peut aussi être passé si pertinent\n",
    "    })\n",
    "    synthesized_output = response_synthesis.get(\"output\")\n",
    "    \n",
    "    print(\"\\n--- Sortie de Synthèse (via Agent) ---\")\n",
    "    if synthesized_output:\n",
    "        # Le SynthesisAgent est censé produire du texte formaté (potentiellement Markdown).\n",
    "        if isinstance(synthesized_output, str) and (synthesized_output.lower().startswith(\"error:\") or \"erreur\" in synthesized_output.lower()):\n",
    "             print(f\"L'agent de synthèse semble avoir rencontré une difficulté : {synthesized_output}\")\n",
    "        else:\n",
    "            try:\n",
    "                from IPython.display import display, Markdown\n",
    "                display(Markdown(str(synthesized_output)))\n",
    "            except ImportError:\n",
    "                print(str(synthesized_output))\n",
    "    else:\n",
    "        print(\"L'agent de synthèse n'a pas retourné de sortie ('output' est vide/None).\")\n",
    "        print(f\"Réponse complète de l'agent (pour débogage) : {response_synthesis}\")\n",
    "            \n",
    "except ValueError as ve:\n",
    "    logger.error(f\"Erreur de configuration LLM pour le SynthesisAgent (provider: {settings.DEFAULT_LLM_MODEL_PROVIDER}): {ve}\", exc_info=True)\n",
    "    print(f\"ERREUR de configuration pour SynthesisAgent : Le provider LLM '{settings.DEFAULT_LLM_MODEL_PROVIDER}' n'est pas correctement configuré. Détails : {ve}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur lors de l'invocation du SynthesisAgent: {e}\", exc_info=True)\n",
    "    print(f\"Erreur inattendue lors du test du SynthesisAgent: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3397713f",
   "metadata": {},
   "source": [
    "## Conclusion des Tests d'Agents Individuels\n",
    "\n",
    "Ce notebook a permis de tester chaque agent de manière isolée pour vérifier son comportement de base et son interaction avec les outils.\n",
    "Ces tests unitaires sont importants avant d'orchestrer ces agents dans un workflow LangGraph plus complexe (ce que nous ferons dans le notebook suivant)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognitive-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
