{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5cad9a",
   "metadata": {},
   "source": [
    "# Notebook 06: Test Approfondi du Pipeline de Bout en Bout\n",
    "\n",
    "Ce notebook est dédié à un test complet et une analyse détaillée du workflow \"Cognitive Swarm\" sur une requête utilisateur complexe. Nous allons observer les sorties intermédiaires des agents, le flux de décision, et la qualité de la synthèse finale. Nous explorerons également comment inspecter les états sauvegardés par le checkpointer MongoDB.\n",
    "\n",
    "**Prérequis :**\n",
    "* **Environnement de Base :** Avoir exécuté le notebook `00_setup_environment.ipynb` pour configurer l'environnement Conda, les dépendances Python, et s'assurer que le fichier `.env` à la racine du projet est correctement rempli avec toutes les configurations nécessaires.\n",
    "* **Base de Données MongoDB :**\n",
    "    * `MONGO_URI` doit être correctement configuré dans `.env` et votre instance MongoDB doit être accessible. Ceci est fondamental pour le checkpointer LangGraph (`MongoDBSaver`) et les outils RAG.\n",
    "    * La base de données doit être peuplée (via `01_data_ingestion_and_embedding.ipynb` ou `scripts/run_ingestion.py`) avec des documents dont les embeddings ont été générés en utilisant le fournisseur spécifié par `DEFAULT_EMBEDDING_PROVIDER` dans vos paramètres. Assurez-vous que les embeddings sont cohérents avec le fournisseur que vous comptez utiliser pour les requêtes dans ce notebook.\n",
    "* **Configuration des Fournisseurs de Modèles (dans `.env`) :** Le workflow `CognitiveSwarm` (exécuté par `run_cognitive_swarm_v2_1` dans ce notebook) utilisera les fournisseurs configurés via `DEFAULT_LLM_MODEL_PROVIDER` (pour les agents) et `DEFAULT_EMBEDDING_PROVIDER` (pour la RAG). Vérifiez que les configurations correspondantes sont correctement en place :\n",
    "    * **Pour les LLMs des Agents (`DEFAULT_LLM_MODEL_PROVIDER`) :**\n",
    "        * Si réglé sur `\"openai\"` : `OPENAI_API_KEY` et `DEFAULT_OPENAI_GENERATIVE_MODEL` sont requis.\n",
    "        * Si réglé sur `\"huggingface_api\"` : `HUGGINGFACE_API_KEY` et `HUGGINGFACE_REPO_ID` (pour le modèle génératif) sont requis.\n",
    "        * Si réglé sur `\"ollama\"` : `OLLAMA_BASE_URL` doit pointer vers votre instance Ollama en cours d'exécution, et que `OLLAMA_GENERATIVE_MODEL_NAME` doit être un modèle que vous avez téléchargé via `ollama pull` et qui est servi par Ollama.\n",
    "    * **Pour les Embeddings (`DEFAULT_EMBEDDING_PROVIDER`, utilisé par `RetrievalEngine` dans les outils) :**\n",
    "        * Si réglé sur `\"openai\"` : `OPENAI_API_KEY` et `OPENAI_EMBEDDING_MODEL_NAME` sont requis.\n",
    "        * Si réglé sur `\"huggingface\"` (local Sentence Transformers) : `HUGGINGFACE_EMBEDDING_MODEL_NAME` doit être configuré (aucune clé API spécifique n'est généralement nécessaire pour cette partie).\n",
    "        * Si réglé sur `\"ollama\"` : `OLLAMA_BASE_URL` et `OLLAMA_EMBEDDING_MODEL_NAME` (un modèle d'embedding approprié) sont requis et le modèle doit être servi par Ollama.\n",
    "* **Checkpointer LangGraph :** Le `MongoDBSaver` est utilisé par défaut par le workflow (`graph_app_v2_1` dans `main_workflow.py`) ; son bon fonctionnement dépend de la configuration correcte de `MONGO_URI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a65b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import uuid \n",
    "import pprint \n",
    "from typing import Dict, Any, Optional, List \n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "    print(f\"Ajout de {project_root} au PYTHONPATH\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "dotenv_path = project_root / \".env\"\n",
    "if dotenv_path.exists():\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    print(f\"Variables d'environnement chargées depuis : {dotenv_path}\")\n",
    "else:\n",
    "    print(f\"ATTENTION: Fichier .env non trouvé à {dotenv_path}.\")\n",
    "\n",
    "from config.settings import settings\n",
    "from config.logging_config import setup_logging\n",
    "from src.graph.main_workflow import run_cognitive_swarm_v2_1\n",
    "from src.graph.checkpointer import MongoDBSaver \n",
    "# --- AJOUT DE L'IMPORT MANQUANT ---\n",
    "from src.vector_store.mongodb_manager import MongoDBManager \n",
    "\n",
    "LOG_LEVEL_NOTEBOOK = \"INFO\" \n",
    "setup_logging(level=LOG_LEVEL_NOTEBOOK) \n",
    "logger = logging.getLogger(\"nb_06_e2e_test\")\n",
    "\n",
    "logger.info(f\"--- Configuration Active pour le Test de Bout en Bout (depuis settings.py et .env) ---\")\n",
    "\n",
    "generative_llm_provider = settings.DEFAULT_LLM_MODEL_PROVIDER.lower()\n",
    "logger.info(f\"Fournisseur LLM génératif principal pour les agents : '{generative_llm_provider}'\")\n",
    "config_llm_ok = False\n",
    "if generative_llm_provider == \"openai\":\n",
    "    if settings.OPENAI_API_KEY and settings.DEFAULT_OPENAI_GENERATIVE_MODEL:\n",
    "        config_llm_ok = True\n",
    "        logger.info(f\"  OpenAI: Clé API trouvée, Modèle: {settings.DEFAULT_OPENAI_GENERATIVE_MODEL}\")\n",
    "    else:\n",
    "        logger.error(\"  ERREUR OpenAI: OPENAI_API_KEY et/ou DEFAULT_OPENAI_GENERATIVE_MODEL manquants.\")\n",
    "elif generative_llm_provider == \"huggingface_api\":\n",
    "    if settings.HUGGINGFACE_API_KEY and settings.HUGGINGFACE_REPO_ID:\n",
    "        config_llm_ok = True\n",
    "        logger.info(f\"  HuggingFace API: Clé API trouvée, Repo ID: {settings.HUGGINGFACE_REPO_ID}\")\n",
    "    else:\n",
    "        logger.error(\"  ERREUR HuggingFace API: HUGGINGFACE_API_KEY et/ou HUGGINGFACE_REPO_ID manquants.\")\n",
    "elif generative_llm_provider == \"ollama\":\n",
    "    if settings.OLLAMA_BASE_URL and settings.OLLAMA_GENERATIVE_MODEL_NAME:\n",
    "        config_llm_ok = True\n",
    "        logger.info(f\"  Ollama: URL Base: {settings.OLLAMA_BASE_URL}, Modèle Génératif: {settings.OLLAMA_GENERATIVE_MODEL_NAME}\")\n",
    "        logger.info(f\"    (Assurez-vous que le modèle '{settings.OLLAMA_GENERATIVE_MODEL_NAME}' est servi par Ollama via 'ollama pull ...')\")\n",
    "    else:\n",
    "        logger.error(\"  ERREUR Ollama: OLLAMA_BASE_URL et/ou OLLAMA_GENERATIVE_MODEL_NAME manquants.\")\n",
    "else:\n",
    "    logger.error(f\"  ERREUR: Fournisseur LLM génératif inconnu : '{generative_llm_provider}'\")\n",
    "\n",
    "if not config_llm_ok:\n",
    "     logger.warning(f\"  AVERTISSEMENT: Configuration LLM pour '{generative_llm_provider}' incomplète. Le workflow risque d'échouer.\")\n",
    "\n",
    "embedding_provider = settings.DEFAULT_EMBEDDING_PROVIDER.lower()\n",
    "logger.info(f\"Fournisseur d'Embedding (pour RAG via RetrievalEngine) : '{embedding_provider}'\")\n",
    "config_embedding_ok = False\n",
    "if embedding_provider == \"openai\":\n",
    "    if settings.OPENAI_API_KEY and settings.OPENAI_EMBEDDING_MODEL_NAME:\n",
    "        config_embedding_ok = True\n",
    "        logger.info(f\"  OpenAI Embeddings: Clé API trouvée, Modèle: {settings.OPENAI_EMBEDDING_MODEL_NAME}\")\n",
    "    else:\n",
    "        logger.error(\"  ERREUR OpenAI Embeddings: OPENAI_API_KEY et/ou OPENAI_EMBEDDING_MODEL_NAME manquants.\")\n",
    "elif embedding_provider == \"huggingface\":\n",
    "    if settings.HUGGINGFACE_EMBEDDING_MODEL_NAME:\n",
    "        config_embedding_ok = True\n",
    "        logger.info(f\"  HuggingFace Embeddings (local): Modèle: {settings.HUGGINGFACE_EMBEDDING_MODEL_NAME}\")\n",
    "    else:\n",
    "        logger.error(\"  ERREUR HuggingFace Embeddings: HUGGINGFACE_EMBEDDING_MODEL_NAME manquant.\")\n",
    "elif embedding_provider == \"ollama\":\n",
    "    if settings.OLLAMA_BASE_URL and settings.OLLAMA_EMBEDDING_MODEL_NAME:\n",
    "        config_embedding_ok = True\n",
    "        logger.info(f\"  Ollama Embeddings: URL Base: {settings.OLLAMA_BASE_URL}, Modèle: {settings.OLLAMA_EMBEDDING_MODEL_NAME}\")\n",
    "        logger.info(f\"    (Assurez-vous que le modèle d'embedding '{settings.OLLAMA_EMBEDDING_MODEL_NAME}' est servi par Ollama.)\")\n",
    "    else:\n",
    "        logger.error(\"  ERREUR Ollama Embeddings: OLLAMA_BASE_URL et/ou OLLAMA_EMBEDDING_MODEL_NAME manquants.\")\n",
    "else:\n",
    "    logger.error(f\"  ERREUR: Fournisseur d'embedding inconnu : '{embedding_provider}'\")\n",
    "\n",
    "if not config_embedding_ok:\n",
    "    logger.warning(f\"  AVERTISSEMENT: Configuration Embedding pour '{embedding_provider}' incomplète. Le RAG risque d'échouer.\")\n",
    "\n",
    "if not settings.MONGO_URI or (\"<user>\" in settings.MONGO_URI and \"<password>\" in settings.MONGO_URI) or \"<cluster_url>\" in settings.MONGO_URI:\n",
    "    logger.error(\"ERREUR CRITIQUE : MONGO_URI non trouvé ou semble non configuré (contient des placeholders). Le checkpointer et le RetrievalEngine (RAG) échoueront.\")\n",
    "else:\n",
    "    logger.info(f\"MongoDB URI configuré (début): {settings.MONGO_URI[:30]}...\")\n",
    "    logger.info(f\"  Base de données MongoDB: {settings.MONGO_DATABASE_NAME}\")\n",
    "    logger.info(f\"  Collection des checkpoints LangGraph: {settings.LANGGRAPH_CHECKPOINTS_COLLECTION}\")\n",
    "    logger.info(f\"  Collection des chunks (RAG default): {MongoDBManager.DEFAULT_CHUNK_COLLECTION_NAME}\") \n",
    "\n",
    "logger.info(\"--- Fin de la Vérification de Configuration Active ---\")\n",
    "\n",
    "def display_final_synthesis(final_state: Dict[str, Any]):\n",
    "    print(\"\\n--- Synthèse Finale Produite (ou Erreur) ---\")\n",
    "    if not final_state:\n",
    "        print(\"Aucun état final retourné.\")\n",
    "        return\n",
    "    \n",
    "    synthesis = final_state.get(\"synthesis_output\")\n",
    "    error_msg = final_state.get(\"error_message\")\n",
    "\n",
    "    if synthesis:\n",
    "        print(synthesis)\n",
    "    elif error_msg:\n",
    "        print(f\"ERREUR DANS LE WORKFLOW : {error_msg}\")\n",
    "    else:\n",
    "        print(\"Aucune synthèse explicite ni message d'erreur trouvé dans les champs dédiés de l'état final.\")\n",
    "        print(\"Affichage de l'état final complet pour débogage :\")\n",
    "        pprint.pprint(final_state) \n",
    "    print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0c5ec",
   "metadata": {},
   "source": [
    "### 1. Définition d'une Requête Utilisateur Complexe et Multi-Facettes\n",
    "\n",
    "Nous allons choisir une requête qui nécessite une planification, potentiellement une recherche de nouveaux documents et une analyse de plusieurs aspects avant la synthèse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836af575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de requête complexe :\n",
    "complex_query = (\n",
    "    \"Provide a comprehensive overview of the use of deep reinforcement learning (DRL) for \"\n",
    "    \"autonomous drone navigation in complex, cluttered urban environments. \"\n",
    "    \"The overview should cover: \"\n",
    "    \"1. Key DRL algorithms employed (e.g., PPO, SAC, DDPG variations). \"\n",
    "    \"2. Common simulation environments and sim-to-real transfer challenges specific to this domain. \"\n",
    "    \"3. How sensor fusion (e.g., vision, LiDAR, IMU) is typically handled in DRL policies for drones. \"\n",
    "    \"4. Explicitly search for and include findings from any ArXiv papers published in the last 12-18 months on this topic, \"\n",
    "    \"especially those addressing safety or obstacle avoidance. \"\n",
    "    \"5. Summarize future research directions.\"\n",
    ")\n",
    "\n",
    "logger.info(f\"Requête complexe pour le test de bout en bout : '{complex_query}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f7984",
   "metadata": {},
   "source": [
    "### 2. Exécution du Workflow \"Cognitive Swarm\"\n",
    "\n",
    "Nous lançons le workflow avec cette requête. Le checkpointer MongoDB sauvegardera les états intermédiaires.\n",
    "Nous allons observer les logs (surtout si `LOG_LEVEL_NOTEBOOK` est à `DEBUG` dans la cellule de configuration ou si la fonction `run_cognitive_swarm_v2_1` a sa propre verbosité d'événements activée)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_thread_id = \"e2e_test_thread_\" + str(uuid.uuid4())\n",
    "\n",
    "# complex_query est défini dans la cellule précédente (ID 836af575)\n",
    "# LOG_LEVEL_NOTEBOOK est défini dans la première cellule de ce notebook (ID 718a65b4)\n",
    "# Les providers LLM et embedding sont lus depuis settings (chargés depuis .env)\n",
    "print(f\"Lancement du workflow de bout en bout pour la requête avec thread_id: {e2e_thread_id}\")\n",
    "print(f\"Utilisation du LLM provider configuré: '{settings.DEFAULT_LLM_MODEL_PROVIDER}' et du provider d'embedding: '{settings.DEFAULT_EMBEDDING_PROVIDER}'.\")\n",
    "print(f\"Niveau de Log pour ce notebook: '{LOG_LEVEL_NOTEBOOK}'. Surveillez la console pour les logs détaillés du flux d'agents et des appels d'outils...\")\n",
    "print(\"Le traitement de la requête complexe peut prendre plusieurs minutes...\")\n",
    "\n",
    "# --- Gestion asyncio pour Jupyter ---\n",
    "# Nécessaire si asyncio.run() est appelé dans un environnement avec une boucle d'événements déjà active (comme Jupyter)\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "# --- Fin Gestion asyncio ---\n",
    "\n",
    "final_state_e2e = None\n",
    "\n",
    "# Vérification principale: MONGO_URI est essentiel pour le checkpointer et souvent pour les outils RAG.\n",
    "# Les configurations LLM/Embedding ont été vérifiées (et des logs émis) dans la première cellule de ce notebook.\n",
    "# Les erreurs d'instanciation dues à des configurations manquantes pour ces services seront attrapées par le try/except.\n",
    "if not settings.MONGO_URI or (\"<user>\" in settings.MONGO_URI and \"<password>\" in settings.MONGO_URI) or \"<cluster_url>\" in settings.MONGO_URI:\n",
    "    print(\"\\nERREUR CRITIQUE: MONGO_URI n'est pas configuré correctement dans le fichier .env (il manque ou contient des placeholders comme <user>).\")\n",
    "    print(\"L'exécution du workflow est annulée car le checkpointer MongoDB et potentiellement les outils RAG sont requis.\")\n",
    "    logger.error(\"MONGO_URI non configuré ou contient des placeholders. Workflow de bout en bout non exécuté.\")\n",
    "else:\n",
    "    try:\n",
    "        # La fonction run_cognitive_swarm_v2_1 est importée depuis src.graph.main_workflow\n",
    "        # Elle utilisera les providers LLM et embedding configurés via settings.py (et .env).\n",
    "        # Les erreurs de configuration (clés API, URLs, modèles non trouvés) seront levées par \n",
    "        # les modules sous-jacents (llm_factory, RetrievalEngine) et attrapées ici.\n",
    "        logger.info(f\"Appel de run_cognitive_swarm_v2_1 avec la requête: \\\"{complex_query[:100]}...\\\" et thread_id: {e2e_thread_id}\")\n",
    "        final_state_e2e = asyncio.run(run_cognitive_swarm_v2_1(complex_query, thread_id=e2e_thread_id))\n",
    "        \n",
    "    except ValueError as ve: # Pour les erreurs de configuration de get_llm ou RetrievalEngine\n",
    "        logger.error(f\"Erreur de configuration (ValueError) lors de l'exécution du workflow de bout en bout: {ve}\", exc_info=True)\n",
    "        print(f\"\\nERREUR DE CONFIGURATION PENDANT L'EXÉCUTION DU WORKFLOW : {ve}\")\n",
    "        print(\"Veuillez vérifier les configurations pour DEFAULT_LLM_MODEL_PROVIDER, DEFAULT_EMBEDDING_PROVIDER, \")\n",
    "        print(\"et leurs dépendances respectives (clés API, URLs de base, noms de modèles exacts) dans votre fichier .env et settings.py.\")\n",
    "        print(f\"Provider LLM actuel: {settings.DEFAULT_LLM_MODEL_PROVIDER}, Provider Embedding actuel: {settings.DEFAULT_EMBEDDING_PROVIDER}\")\n",
    "    except RuntimeError as re: # Pour les erreurs spécifiques à asyncio si nest_asyncio ne suffit pas\n",
    "        if \"cannot be called from a running event loop\" in str(re):\n",
    "            logger.error(f\"Erreur RuntimeError avec asyncio.run(): {re}. 'nest_asyncio.apply()' n'a peut-être pas été appelé ou n'a pas fonctionné.\", exc_info=True)\n",
    "            print(f\"\\nERREUR ASYNCIO : {re}. Assurez-vous que 'nest_asyncio' est installé et que 'nest_asyncio.apply()' est appelé avant 'asyncio.run()'.\")\n",
    "        else:\n",
    "            logger.error(f\"Erreur RuntimeError inattendue lors de l'exécution du workflow: {re}\", exc_info=True)\n",
    "            print(f\"\\nERREUR RUNTIME INATTENDUE PENDANT L'EXÉCUTION DU WORKFLOW : {re}\")\n",
    "    except Exception as e: # Pour les autres erreurs d'exécution inattendues\n",
    "        logger.error(f\"Erreur inattendue lors de l'exécution du workflow de bout en bout: {e}\", exc_info=True)\n",
    "        print(f\"\\nERREUR INATTENDUE PENDANT L'EXÉCUTION DU WORKFLOW : {e}\")\n",
    "\n",
    "# Afficher la synthèse finale (ou l'erreur)\n",
    "# La fonction display_final_synthesis est définie dans la première cellule de code de ce notebook (ID 718a65b4)\n",
    "if final_state_e2e:\n",
    "    display_final_synthesis(final_state_e2e) \n",
    "else:\n",
    "    print(\"\\nL'exécution du workflow n'a pas retourné d'état final ou a échoué avant de pouvoir retourner un état.\")\n",
    "    if not settings.MONGO_URI or (\"<user>\" in settings.MONGO_URI and \"<password>\" in settings.MONGO_URI) or \"<cluster_url>\" in settings.MONGO_URI:\n",
    "        print(\"Rappel : MONGO_URI n'était pas (ou mal) configuré, ce qui a pu empêcher l'exécution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee64145",
   "metadata": {},
   "source": [
    "### 3. Analyse Qualitative des Sorties Intermédiaires (si `final_state_e2e` est disponible)\n",
    "\n",
    "Si l'exécution précédente a réussi et retourné `final_state_e2e`, nous pouvons examiner certains des champs clés de cet état pour comprendre le comportement du système."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded95f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_state_e2e and not final_state_e2e.get(\"error_message\"):\n",
    "    print(\"\\n--- Analyse des Sorties Intermédiaires Clés ---\")\n",
    "\n",
    "    # 1. Plan de Recherche\n",
    "    research_plan = final_state_e2e.get(\"research_plan\")\n",
    "    if research_plan:\n",
    "        print(\"\\n### Plan de Recherche Généré par ResearchPlannerAgent ###\")\n",
    "        # Pour un affichage potentiellement long, on peut tronquer ou utiliser IPython.display.Markdown si c'est du Markdown\n",
    "        if isinstance(research_plan, str) and (\"\\n##\" in research_plan or \"\\n*\" in research_plan):\n",
    "            try:\n",
    "                from IPython.display import display, Markdown\n",
    "                display(Markdown(research_plan))\n",
    "            except ImportError:\n",
    "                print(research_plan)\n",
    "        else:\n",
    "            print(research_plan)\n",
    "    else:\n",
    "        print(\"\\nAucun plan de recherche explicite trouvé dans l'état final.\")\n",
    "\n",
    "    # 2. Analyse des Messages (pour les résultats d'outils et les pensées des agents)\n",
    "    print(\"\\n### Analyse des Messages Clés de l'Exécution ###\")\n",
    "    messages = final_state_e2e.get(\"messages\", [])\n",
    "    \n",
    "    if not messages:\n",
    "        print(\"Aucun message dans l'état final.\")\n",
    "    else:\n",
    "        # Afficher les quelques derniers messages pour voir le contexte final\n",
    "        # La fonction pretty_print_final_state de la première cellule de 04_... était plus détaillée ici.\n",
    "        # Pour cette cellule, on se concentre sur les ToolMessages.\n",
    "        print(f\"Nombre total de messages: {len(messages)}. Affichage des ToolMessages et des derniers AIMessages:\")\n",
    "\n",
    "        for i, msg in enumerate(messages):\n",
    "            msg_type_str = getattr(msg, 'type', 'UNKNOWN').upper()\n",
    "            msg_name_str = getattr(msg, 'name', None)\n",
    "            display_name = f\"{msg_type_str} ({msg_name_str})\" if msg_name_str else msg_type_str\n",
    "\n",
    "            if msg_type_str == \"TOOL\":\n",
    "                tool_call_id = getattr(msg, 'tool_call_id', 'N/A')\n",
    "                print(f\"\\n  Message #{i+1}: [{display_name}] - Tool Call ID: {tool_call_id}\")\n",
    "                tool_content_str = str(getattr(msg, 'content', 'N/A'))\n",
    "                try:\n",
    "                    # Tenter de parser si c'est une chaîne JSON (pour les outils structurés)\n",
    "                    if tool_content_str.strip().startswith((\"{\", \"[\")):\n",
    "                        tool_content_parsed = json.loads(tool_content_str)\n",
    "                        print(\"    Contenu (parsé en JSON):\")\n",
    "                        print(json.dumps(tool_content_parsed, indent=2, ensure_ascii=False))\n",
    "                        \n",
    "                        # Heuristique pour identifier le type d'outil basé sur le contenu\n",
    "                        if isinstance(tool_content_parsed, list) and tool_content_parsed:\n",
    "                            if isinstance(tool_content_parsed[0], dict):\n",
    "                                if \"pdf_url\" in tool_content_parsed[0]:\n",
    "                                    print(f\"    (Semble être un résultat de ArXiv Search - {len(tool_content_parsed)} items)\")\n",
    "                                elif \"text_chunk\" in tool_content_parsed[0]:\n",
    "                                    print(f\"    (Semble être un résultat de KB Retrieval - {len(tool_content_parsed)} chunks)\")\n",
    "                    else: # Si ce n'est pas du JSON évident, afficher comme chaîne\n",
    "                        print(f\"    Contenu (chaîne): {tool_content_str[:500]}{'...' if len(tool_content_str) > 500 else ''}\")\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"    Contenu (chaîne non-JSON): {tool_content_str[:500]}{'...' if len(tool_content_str) > 500 else ''}\")\n",
    "                except Exception as e_parse:\n",
    "                    print(f\"    Impossible d'analyser/afficher le contenu de ToolMessage : {e_parse}\")\n",
    "            \n",
    "            # Optionnel: Afficher les derniers messages d'IA non-tool-calling (pour voir les \"pensées\" finales des agents)\n",
    "            elif msg_type_str == \"AI\" and not getattr(msg, 'tool_calls', None) and i >= len(messages) - 3 : # Derniers 3 messages\n",
    "                 print(f\"\\n  Message #{i+1}: [{display_name}] (Pensée/Réponse finale de l'agent)\")\n",
    "                 print(f\"    Contenu: {str(getattr(msg, 'content', 'N/A'))[:500]}{'...' if len(str(getattr(msg, 'content', 'N/A'))) > 500 else ''}\")\n",
    "\n",
    "\n",
    "    # 3. Résumé de l'Analyse de Documents (si produit par DocumentAnalysisAgent sans être un appel d'outil direct)\n",
    "    doc_analysis_summary = final_state_e2e.get(\"document_analysis_summary\")\n",
    "    if doc_analysis_summary:\n",
    "        print(\"\\n### Résumé de l'Analyse de Documents (champ 'document_analysis_summary') ###\")\n",
    "        # Ce champ peut contenir du Markdown si le document_deep_dive_analysis_tool a été utilisé\n",
    "        if isinstance(doc_analysis_summary, str) and (\"\\n##\" in doc_analysis_summary or \"\\n*\" in doc_analysis_summary):\n",
    "             try:\n",
    "                from IPython.display import display, Markdown\n",
    "                display(Markdown(doc_analysis_summary))\n",
    "             except ImportError:\n",
    "                print(doc_analysis_summary)\n",
    "        else:\n",
    "            print(doc_analysis_summary)\n",
    "    else:\n",
    "        print(\"\\nAucun résumé d'analyse de document explicite trouvé dans le champ 'document_analysis_summary' de l'état final.\")\n",
    "    \n",
    "    print(\"\\n--- Fin de l'Analyse des Sorties Intermédiaires ---\")\n",
    "\n",
    "elif final_state_e2e and final_state_e2e.get(\"error_message\"):\n",
    "    # Ce message est déjà géré par display_final_synthesis dans la cellule précédente\n",
    "    print(f\"\\nL'exécution du workflow a produit une erreur (voir message dans la sortie de la cellule précédente). Analyse des sorties intermédiaires impossible.\")\n",
    "else:\n",
    "    print(\"\\nÉtat final ('final_state_e2e') non disponible ou vide. Impossible d'analyser les sorties intermédiaires.\")\n",
    "    print(\"Veuillez exécuter la cellule précédente (exécution du workflow) avec succès.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d5b268",
   "metadata": {},
   "source": [
    "### 4. Inspection des Checkpoints dans MongoDB\n",
    "\n",
    "Si le `MongoDBSaver` est actif (ce qui est le cas par défaut dans notre `main_workflow.py`), nous pouvons interroger MongoDB pour voir les états sauvegardés pour le `thread_id` de cette exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4dfff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AJOUT DE L'IMPORT MANQUANT ---\n",
    "from pymongo.errors import ConnectionFailure \n",
    "\n",
    "async def inspect_checkpoints(thread_id: str):\n",
    "    logger.info(f\"\\n--- Inspection des Checkpoints pour Thread ID: {thread_id} ---\")\n",
    "    if not settings.MONGO_URI: # MONGO_URI est vérifié aussi dans la 1ère cellule, mais redondance ici est ok.\n",
    "        logger.error(\"MONGO_URI non configuré. Impossible d'inspecter les checkpoints.\")\n",
    "        print(\"ERREUR: MONGO_URI non configuré. Inspection des checkpoints annulée.\")\n",
    "        return\n",
    "\n",
    "    checkpointer = None \n",
    "    try:\n",
    "        # MongoDBSaver est importé dans la première cellule de ce notebook\n",
    "        checkpointer = MongoDBSaver(\n",
    "            collection_name=settings.LANGGRAPH_CHECKPOINTS_COLLECTION \n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Récupération des checkpoints pour thread_id='{thread_id}' depuis la collection '{settings.LANGGRAPH_CHECKPOINTS_COLLECTION}'...\")\n",
    "        \n",
    "        config_for_list = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        checkpoints_history = []\n",
    "        async for checkpoint_tuple in checkpointer.alist(config=config_for_list):\n",
    "            checkpoints_history.append(checkpoint_tuple)\n",
    "        \n",
    "        if not checkpoints_history:\n",
    "            print(f\"Aucun checkpoint trouvé pour le thread_id: {thread_id}\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nTrouvé {len(checkpoints_history)} checkpoints pour le thread_id: {thread_id} (du plus récent au plus ancien):\")\n",
    "        \n",
    "        for i, cp_tuple in enumerate(checkpoints_history[:3]): \n",
    "            checkpoint_id_ts = cp_tuple.checkpoint.get('id', 'N/A') \n",
    "            \n",
    "            print(f\"\\nCheckpoint #{i+1} (ts/id: {checkpoint_id_ts}):\")\n",
    "            print(f\"  Config du checkpoint: {cp_tuple.config}\")\n",
    "            print(f\"  Metadata: {cp_tuple.metadata}\")\n",
    "            \n",
    "            parent_ts_info = \"None\"\n",
    "            if cp_tuple.parent_config and cp_tuple.parent_config.get(\"configurable\"):\n",
    "                parent_ts_info = cp_tuple.parent_config[\"configurable\"].get('thread_ts', 'N/A')\n",
    "            print(f\"  Parent ts (depuis parent_config): {parent_ts_info}\")\n",
    "            \n",
    "            messages_in_checkpoint = cp_tuple.checkpoint.get(\"channel_values\", {}).get(\"messages\", [])\n",
    "            if messages_in_checkpoint:\n",
    "                last_msg_in_cp = messages_in_checkpoint[-1]\n",
    "                msg_type = getattr(last_msg_in_cp, 'type', 'UNKNOWN').upper()\n",
    "                msg_name = getattr(last_msg_in_cp, 'name', '') \n",
    "                msg_content = str(getattr(last_msg_in_cp, 'content', ''))\n",
    "                print(f\"  Dernier message dans ce checkpoint: [{msg_type}{' ('+msg_name+')' if msg_name else ''}]: {msg_content[:100]}...\")\n",
    "            else:\n",
    "                print(\"  Aucun message trouvé dans channel_values pour ce checkpoint.\")\n",
    "        \n",
    "        if len(checkpoints_history) > 3:\n",
    "            print(f\"\\n... et {len(checkpoints_history) - 3} checkpoint(s) plus ancien(s) non affiché(s) en détail.\")\n",
    "\n",
    "    except ConnectionFailure as cf: \n",
    "        logger.error(f\"Erreur de connexion MongoDB lors de l'inspection des checkpoints: {cf}\", exc_info=True)\n",
    "        print(f\"ERREUR DE CONNEXION MONGODB: {cf}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'inspection des checkpoints: {e}\", exc_info=True)\n",
    "        print(f\"Erreur inattendue lors de l'inspection des checkpoints: {e}\")\n",
    "    finally:\n",
    "        if checkpointer and hasattr(checkpointer, 'aclose'): \n",
    "            await checkpointer.aclose()\n",
    "            logger.info(\"Connexion du checkpointer MongoDB fermée.\")\n",
    "\n",
    "if 'e2e_thread_id' in locals() and e2e_thread_id:\n",
    "    print(f\"\\nTentative d'inspection des checkpoints pour le thread_id: {e2e_thread_id}\")\n",
    "    # nest_asyncio.apply() a été appelé dans la cellule d'exécution du workflow (ID af3a09d2).\n",
    "    # Si cette cellule est exécutée indépendamment après un redémarrage du noyau, \n",
    "    # il faudrait décommenter les lignes nest_asyncio ci-dessous.\n",
    "    # import nest_asyncio \n",
    "    # nest_asyncio.apply() \n",
    "    \n",
    "    asyncio.run(inspect_checkpoints(e2e_thread_id))\n",
    "else:\n",
    "    logger.warning(\"'e2e_thread_id' non défini. L'exécution précédente du workflow a peut-être échoué ou cette cellule est exécutée hors séquence.\")\n",
    "    print(\"\\nVariable 'e2e_thread_id' non trouvée. Exécutez d'abord la cellule d'exécution du workflow principal pour définir un thread_id.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074122d4",
   "metadata": {},
   "source": [
    "### 5. Discussion et Analyse Qualitative de la Synthèse Finale\n",
    "\n",
    "Revenons à la synthèse finale produite à l'étape 2 (stockée dans `final_state_e2e['synthesis_output']`).\n",
    "* La synthèse répond-elle de manière complète et précise à la requête complexe initiale ?\n",
    "* Les différents aspects de la requête (algorithmes DRL, environnements de simulation, défis sim-to-real, fusion de capteurs, résultats récents d'ArXiv, directions futures) sont-ils couverts ?\n",
    "* L'information est-elle bien structurée et cohérente ?\n",
    "* Y a-t-il des signes d'hallucination ou des informations manquantes cruciales (en supposant que le corpus contient les informations nécessaires) ?\n",
    "\n",
    "Cette analyse qualitative est subjective mais essentielle pour comprendre les forces et faiblesses actuelles du système. Elle peut guider les améliorations des prompts des agents, de la logique de routage, ou des stratégies RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed1bb2e",
   "metadata": {},
   "source": [
    "## Conclusion de ce Test de Bout en Bout\n",
    "\n",
    "Ce notebook a permis d'exécuter le \"Cognitive Swarm\" sur une requête complexe, d'examiner certaines sorties intermédiaires et la synthèse finale, et de voir comment les checkpoints sont gérés.\n",
    "\n",
    "Ce type de test approfondi est utile pour :\n",
    "- Identifier les goulots d'étranglement ou les points faibles dans le flux des agents.\n",
    "- Évaluer qualitativement la performance globale.\n",
    "- Déboguer des comportements inattendus.\n",
    "- Générer des exemples concrets pour l'évaluation quantitative (par exemple, des paires `(requête, contexte, synthèse)` pour `SynthesisEvaluator`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognitive-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
