{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79506aac",
   "metadata": {},
   "source": [
    "# Notebook 01: Pipeline d'Ingestion et d'Embedding des Données ArXiv\n",
    "\n",
    "Ce notebook démontre le processus complet d'acquisition, de traitement, d'embedding et de stockage des articles scientifiques d'ArXiv dans notre base de données MongoDB.\n",
    "\n",
    "**Prérequis :**\n",
    "* Assurez-vous d'avoir exécuté le notebook `00_setup_environment.ipynb` et que votre environnement est correctement configuré (variables d'environnement chargées, clés API valides, MongoDB accessible).\n",
    "* Les bibliothèques nécessaires doivent être installées via `environment.yml`.\n",
    "\n",
    "**Étapes de ce Notebook :**\n",
    "1.  Configuration initiale (imports, logging, connexion MongoDB).\n",
    "2.  Téléchargement d'articles depuis ArXiv.\n",
    "3.  Parsing des documents PDF et de leurs métadonnées.\n",
    "4.  Prétraitement du texte (nettoyage et chunking).\n",
    "5.  Génération des embeddings pour les chunks.\n",
    "6.  Stockage des chunks et de leurs embeddings dans MongoDB et création des index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# S'assurer que la racine du projet est dans le PYTHONPATH pour les imports de src\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "    print(f\"Ajout de {project_root} au PYTHONPATH\")\n",
    "\n",
    "# Charger les variables d'environnement (si .env est à la racine du projet)\n",
    "from dotenv import load_dotenv\n",
    "dotenv_path = project_root / \".env\"\n",
    "if dotenv_path.exists():\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    print(f\"Variables d'environnement chargées depuis : {dotenv_path}\")\n",
    "else:\n",
    "    print(f\"ATTENTION: Fichier .env non trouvé à {dotenv_path}. Assurez-vous qu'il existe et est configuré.\")\n",
    "\n",
    "\n",
    "# Importer nos modules\n",
    "from config.settings import settings\n",
    "from config.logging_config import setup_logging\n",
    "from src.data_processing.arxiv_downloader import download_pipeline as download_arxiv_papers\n",
    "from src.data_processing.document_parser import parse_document_collection, PDF_INPUT_DIR, METADATA_INPUT_DIR\n",
    "from src.data_processing.preprocessor import preprocess_parsed_documents, ParsedDocument\n",
    "from src.data_processing.embedder import generate_embeddings_for_chunks, ProcessedChunkWithEmbedding\n",
    "from src.vector_store.mongodb_manager import MongoDBManager\n",
    "\n",
    "# Configurer le logging pour le notebook\n",
    "setup_logging(level=\"INFO\") # Mettre à DEBUG pour plus de détails\n",
    "logger = logging.getLogger(\"nb_01_ingestion_embedding\")\n",
    "\n",
    "logger.info(\"Configuration initiale du notebook terminée.\")\n",
    "logger.info(f\"Utilisation de DATA_DIR: {settings.DATA_DIR}\")\n",
    "logger.info(f\"PDFs ArXiv seront (télé)chargés depuis/vers: {PDF_INPUT_DIR}\")\n",
    "logger.info(f\"Métadonnées ArXiv seront (télé)chargées depuis/vers: {METADATA_INPUT_DIR}\")\n",
    "\n",
    "# --- Affichage de la configuration d'embedding active et vérification des prérequis ---\n",
    "logger.info(f\"--- Configuration d'Embedding Active (depuis settings.py et .env) ---\")\n",
    "active_embedding_provider = settings.DEFAULT_EMBEDDING_PROVIDER.lower()\n",
    "logger.info(f\"Fournisseur d'embedding par défaut configuré : {active_embedding_provider}\")\n",
    "\n",
    "if active_embedding_provider == \"openai\":\n",
    "    logger.info(f\"  Modèle OpenAI Embedding à utiliser : {settings.OPENAI_EMBEDDING_MODEL_NAME}\")\n",
    "    logger.info(f\"  Dimension OpenAI Embedding (configurée) : {settings.OPENAI_EMBEDDING_DIMENSION}\")\n",
    "    if not settings.OPENAI_API_KEY:\n",
    "        logger.error(\"ERREUR : Le fournisseur d'embedding est 'openai', mais OPENAI_API_KEY n'est pas configurée dans .env. La génération d'embeddings échouera.\")\n",
    "elif active_embedding_provider == \"huggingface\":\n",
    "    logger.info(f\"  Modèle HuggingFace Embedding à utiliser : {settings.HUGGINGFACE_EMBEDDING_MODEL_NAME}\")\n",
    "    logger.info(f\"  Dimension HuggingFace Embedding (configurée) : {settings.HUGGINGFACE_EMBEDDING_MODEL_DIMENSION}\")\n",
    "    # Les embeddings HuggingFace locaux (SentenceTransformers) ne nécessitent généralement pas de clé API.\n",
    "elif active_embedding_provider == \"ollama\":\n",
    "    logger.info(f\"  Modèle Ollama Embedding à utiliser : {settings.OLLAMA_EMBEDDING_MODEL_NAME}\")\n",
    "    logger.info(f\"  Dimension Ollama Embedding (configurée) : {settings.OLLAMA_EMBEDDING_MODEL_DIMENSION}\")\n",
    "    logger.info(f\"  URL de base Ollama : {settings.OLLAMA_BASE_URL}\")\n",
    "    if not settings.OLLAMA_BASE_URL:\n",
    "        logger.error(\"ERREUR : Le fournisseur d'embedding est 'ollama', mais OLLAMA_BASE_URL n'est pas configurée. La génération d'embeddings échouera.\")\n",
    "    logger.info(f\"  ASSUREZ-VOUS que le modèle '{settings.OLLAMA_EMBEDDING_MODEL_NAME}' est disponible sur votre serveur Ollama (ex: via 'ollama pull {settings.OLLAMA_EMBEDDING_MODEL_NAME}').\")\n",
    "else:\n",
    "    logger.error(f\"ERREUR : Fournisseur d'embedding inconnu configuré dans settings.py : '{active_embedding_provider}'. La génération d'embeddings échouera.\")\n",
    "# --- Fin de la section sur la configuration d'embedding ---\n",
    "\n",
    "# Paramètres pour cette exécution de notebook (pour limiter le nombre d'articles traités)\n",
    "ARXIV_QUERY_NOTEBOOK = \"explainable artificial intelligence for robotics\" \n",
    "MAX_RESULTS_NOTEBOOK = 2 \n",
    "COLLECTION_NAME_NOTEBOOK = \"arxiv_chunks_notebook_test\" \n",
    "VECTOR_INDEX_NAME_NOTEBOOK = \"vector_index_notebook_test\"\n",
    "TEXT_INDEX_NAME_NOTEBOOK = \"text_index_notebook_test\"\n",
    "\n",
    "# Nettoyer les répertoires de données de test (optionnel)\n",
    "# import shutil\n",
    "# if PDF_INPUT_DIR.exists():\n",
    "#     logger.info(f\"Nettoyage du répertoire PDF: {PDF_INPUT_DIR}\")\n",
    "#     shutil.rmtree(PDF_INPUT_DIR)\n",
    "# PDF_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# if METADATA_INPUT_DIR.exists():\n",
    "#     logger.info(f\"Nettoyage du répertoire Metadata: {METADATA_INPUT_DIR}\")\n",
    "#     shutil.rmtree(METADATA_INPUT_DIR)\n",
    "# METADATA_INPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a39ee30",
   "metadata": {},
   "source": [
    "### Étape 1: Téléchargement d'Articles depuis ArXiv\n",
    "\n",
    "Nous utilisons `arxiv_downloader.download_pipeline` pour rechercher et télécharger quelques articles.\n",
    "Pour ce notebook, nous limitons la recherche à `MAX_RESULTS_NOTEBOOK` articles pour que l'exécution soit rapide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"--- Étape 1: Téléchargement d'Articles ArXiv (max: {MAX_RESULTS_NOTEBOOK}) ---\")\n",
    "\n",
    "downloaded_files_info = download_arxiv_papers(\n",
    "    query=ARXIV_QUERY_NOTEBOOK,\n",
    "    max_results=MAX_RESULTS_NOTEBOOK \n",
    "    # Utilise les sort_by, sort_order, et paths de settings par défaut\n",
    ")\n",
    "\n",
    "if downloaded_files_info and downloaded_files_info.get('pdfs'):\n",
    "    logger.info(f\"Téléchargement terminé. {len(downloaded_files_info['pdfs'])} PDFs et {len(downloaded_files_info['metadata'])} fichiers de métadonnées.\")\n",
    "    for pdf_path in downloaded_files_info['pdfs'][:2]: # Afficher les 2 premiers\n",
    "        logger.info(f\"  PDF téléchargé : {pdf_path}\")\n",
    "    for meta_path in downloaded_files_info['metadata'][:2]:\n",
    "        logger.info(f\"  Métadonnées sauvegardées : {meta_path}\")\n",
    "else:\n",
    "    logger.warning(\"Aucun fichier PDF n'a été téléchargé. Vérifiez la requête ArXiv ou la connexion.\")\n",
    "    # On pourrait arrêter ici si aucun fichier n'est téléchargé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e64f8",
   "metadata": {},
   "source": [
    "### Étape 2: Parsing des Documents\n",
    "\n",
    "Maintenant, nous utilisons `document_parser.parse_document_collection` pour lire les PDFs téléchargés et leurs fichiers de métadonnées JSON associés. Cela extraira le texte brut et structurera les métadonnées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6537dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"\\n--- Étape 2: Parsing des Documents ---\")\n",
    "\n",
    "# parse_document_collection utilise les chemins PDF_INPUT_DIR et METADATA_INPUT_DIR définis dans document_parser.py\n",
    "# qui sont basés sur settings.DATA_DIR\n",
    "parsed_documents: List[ParsedDocument] = parse_document_collection()\n",
    "\n",
    "if parsed_documents:\n",
    "    logger.info(f\"{len(parsed_documents)} documents ont été parsés avec succès.\")\n",
    "    # Afficher un extrait du premier document parsé\n",
    "    if len(parsed_documents) > 0:\n",
    "        doc_example = parsed_documents[0]\n",
    "        logger.info(f\"Exemple de document parsé (ID ArXiv: {doc_example['arxiv_id']}):\")\n",
    "        logger.info(f\"  Titre (depuis métadonnées): {doc_example['metadata'].get('title', 'N/A')}\")\n",
    "        logger.info(f\"  Extrait du texte: '{doc_example['text_content'][:200].replace(chr(10), ' ')}...'\")\n",
    "        logger.info(f\"  Chemin PDF: {doc_example['pdf_path']}\")\n",
    "        logger.info(f\"  Chemin Métadonnées: {doc_example['metadata_path']}\")\n",
    "else:\n",
    "    logger.warning(\"Aucun document n'a été parsé. Vérifiez si des PDFs existent dans le répertoire attendu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f71ac7d",
   "metadata": {},
   "source": [
    "### Étape 3: Prétraitement du Texte (Nettoyage et Chunking)\n",
    "\n",
    "Les documents parsés sont maintenant nettoyés et découpés en chunks plus petits et gérables en utilisant `preprocessor.preprocess_parsed_documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0550720",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"\\n--- Étape 3: Prétraitement du Texte ---\")\n",
    "\n",
    "if parsed_documents:\n",
    "    processed_chunks = preprocess_parsed_documents(parsed_documents)\n",
    "    if processed_chunks:\n",
    "        logger.info(f\"{len(processed_chunks)} chunks ont été générés après prétraitement.\")\n",
    "        # Afficher un extrait du premier chunk\n",
    "        if len(processed_chunks) > 0:\n",
    "            chunk_example = processed_chunks[0]\n",
    "            logger.info(f\"Exemple de chunk traité (ID: {chunk_example['chunk_id']}):\")\n",
    "            logger.info(f\"  ID ArXiv d'origine: {chunk_example['arxiv_id']}\")\n",
    "            logger.info(f\"  Titre d'origine: {chunk_example['original_document_title']}\")\n",
    "            logger.info(f\"  Extrait du chunk: '{chunk_example['text_chunk'][:200].replace(chr(10), ' ')}...'\")\n",
    "    else:\n",
    "        logger.warning(\"Aucun chunk n'a été généré lors du prétraitement.\")\n",
    "else:\n",
    "    logger.warning(\"Aucun document parsé à prétraiter. Étape de prétraitement sautée.\")\n",
    "    processed_chunks = [] # S'assurer que la variable existe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b24a9",
   "metadata": {},
   "source": [
    "### Étape 4: Génération des Embeddings\n",
    "\n",
    "Chaque chunk de texte est maintenant converti en une représentation vectorielle (embedding) en utilisant `embedder.generate_embeddings_for_chunks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ea79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"\\n--- Étape 4: Génération des Embeddings ---\")\n",
    "chunks_with_embeddings: List[ProcessedChunkWithEmbedding] = []\n",
    "\n",
    "if processed_chunks:\n",
    "    # Vérification proactive des prérequis pour le fournisseur d'embedding configuré\n",
    "    provider_check_ok = True\n",
    "    active_provider = settings.DEFAULT_EMBEDDING_PROVIDER.lower()\n",
    "    if active_provider == \"openai\" and not settings.OPENAI_API_KEY:\n",
    "        logger.error(\"OpenAI est le fournisseur d'embedding, mais OPENAI_API_KEY n'est pas configurée. Impossible de générer les embeddings.\")\n",
    "        provider_check_ok = False\n",
    "    elif active_provider == \"ollama\":\n",
    "        if not settings.OLLAMA_BASE_URL:\n",
    "            logger.error(\"Ollama est le fournisseur d'embedding, mais OLLAMA_BASE_URL n'est pas configurée.\")\n",
    "            provider_check_ok = False\n",
    "        if not settings.OLLAMA_EMBEDDING_MODEL_NAME: # Vérification ajoutée\n",
    "            logger.error(\"Ollama est le fournisseur d'embedding, mais OLLAMA_EMBEDDING_MODEL_NAME n'est pas configuré.\")\n",
    "            provider_check_ok = False\n",
    "    # Pas de vérification de clé pour \"huggingface\" ici, car les modèles SentenceTransformers locaux n'en nécessitent pas.\n",
    "\n",
    "    if provider_check_ok:\n",
    "        logger.info(f\"Appel de generate_embeddings_for_chunks avec le provider: {active_provider}\")\n",
    "        chunks_with_embeddings = generate_embeddings_for_chunks(processed_chunks)\n",
    "        if chunks_with_embeddings:\n",
    "            logger.info(f\"{len(chunks_with_embeddings)} chunks ont maintenant des embeddings.\")\n",
    "            if len(chunks_with_embeddings) > 0:\n",
    "                chunk_emb_example = chunks_with_embeddings[0]\n",
    "                logger.info(f\"Exemple de chunk avec embedding (ID: {chunk_emb_example['chunk_id']}):\")\n",
    "                logger.info(f\"  Fournisseur d'Embedding Utilisé: {chunk_emb_example.get('embedding_provider', 'N/A')}\")\n",
    "                logger.info(f\"  Modèle d'Embedding Utilisé: {chunk_emb_example.get('embedding_model', 'N/A')}\")\n",
    "                logger.info(f\"  Dimension Réelle de l'Embedding: {chunk_emb_example.get('embedding_dimension', 'N/A')}\") \n",
    "                logger.info(f\"  Vecteur d'Embedding (5 premières dimensions): {chunk_emb_example.get('embedding', [])[:5]}...\")\n",
    "        else:\n",
    "            logger.warning(\"Aucun embedding n'a été généré.\")\n",
    "    else:\n",
    "        logger.warning(\"Prérequis non remplis pour le fournisseur d'embedding configuré. Étape d'embedding sautée.\")\n",
    "else:\n",
    "    logger.warning(\"Aucun chunk traité à embedder. Étape d'embedding sautée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1c78c",
   "metadata": {},
   "source": [
    "### Étape 5: Stockage dans MongoDB et Création des Index\n",
    "\n",
    "Enfin, les chunks avec leurs embeddings sont insérés dans une collection MongoDB. Nous créons également les index de recherche vectorielle et textuelle nécessaires pour notre moteur RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc36835",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"\\n--- Étape 5: Stockage MongoDB et Création d'Index ---\")\n",
    "mongo_mgr = None \n",
    "\n",
    "if chunks_with_embeddings:\n",
    "    try:\n",
    "        logger.info(f\"Initialisation de MongoDBManager pour la collection: {COLLECTION_NAME_NOTEBOOK}\")\n",
    "        mongo_mgr = MongoDBManager(mongo_uri=settings.MONGO_URI, db_name=settings.MONGO_DATABASE_NAME)\n",
    "        mongo_mgr.connect()\n",
    "\n",
    "        test_collection = mongo_mgr.get_collection(COLLECTION_NAME_NOTEBOOK)\n",
    "        if test_collection is not None: # Vérifier que la collection a bien été récupérée\n",
    "            logger.info(f\"Suppression des documents existants dans la collection de test '{COLLECTION_NAME_NOTEBOOK}'...\")\n",
    "            delete_result = test_collection.delete_many({})\n",
    "            logger.info(f\"{delete_result.deleted_count} documents supprimés.\")\n",
    "        else:\n",
    "            logger.error(f\"Impossible d'obtenir la collection {COLLECTION_NAME_NOTEBOOK}. L'insertion et la création d'index vont échouer.\")\n",
    "            # Il serait peut-être préférable de lever une exception ici pour arrêter le flux du notebook\n",
    "            # car les étapes suivantes dépendent de test_collection.\n",
    "            # Pour l'instant, on logue et on continue, mais les étapes suivantes pourraient échouer.\n",
    "            # raise ConnectionError(f\"Impossible d'obtenir la collection {COLLECTION_NAME_NOTEBOOK}\")\n",
    "\n",
    "\n",
    "        logger.info(f\"Insertion de {len(chunks_with_embeddings)} chunks dans MongoDB...\")\n",
    "        insertion_summary = mongo_mgr.insert_chunks_with_embeddings(\n",
    "            chunks_with_embeddings,\n",
    "            collection_name=COLLECTION_NAME_NOTEBOOK\n",
    "        )\n",
    "        logger.info(f\"Résumé de l'insertion MongoDB: {insertion_summary}\")\n",
    "\n",
    "        if insertion_summary.get(\"inserted_count\", 0) > 0:\n",
    "            logger.info(f\"Création/Vérification de l'index vectoriel '{VECTOR_INDEX_NAME_NOTEBOOK}'. La dimension de l'index sera basée sur le fournisseur d'embedding configuré: '{settings.DEFAULT_EMBEDDING_PROVIDER}'.\")\n",
    "            vector_filter_fields = [\n",
    "                \"arxiv_id\", \n",
    "                \"original_document_title\", \n",
    "                \"metadata.primary_category\", \n",
    "                \"embedding_provider\", \n",
    "                \"embedding_model\"     \n",
    "            ]\n",
    "            success_vector_idx = mongo_mgr.create_vector_search_index(\n",
    "                collection_name=COLLECTION_NAME_NOTEBOOK,\n",
    "                index_name=VECTOR_INDEX_NAME_NOTEBOOK,\n",
    "                embedding_field=\"embedding\",\n",
    "                filter_fields=vector_filter_fields\n",
    "            )\n",
    "            if success_vector_idx:\n",
    "                logger.info(\"Index vectoriel géré avec succès.\")\n",
    "            else:\n",
    "                logger.warning(\"Problème lors de la gestion de l'index vectoriel.\")\n",
    "\n",
    "            logger.info(f\"Création/Vérification de l'index textuel '{TEXT_INDEX_NAME_NOTEBOOK}'...\")\n",
    "            success_text_idx = mongo_mgr.create_text_search_index(\n",
    "                collection_name=COLLECTION_NAME_NOTEBOOK,\n",
    "                index_name=TEXT_INDEX_NAME_NOTEBOOK,\n",
    "                text_field=\"text_chunk\",\n",
    "                additional_text_fields={\"original_document_title\": \"string\", \"metadata.title\": \"string\"}\n",
    "            )\n",
    "            if success_text_idx:\n",
    "                logger.info(\"Index textuel géré avec succès.\")\n",
    "            else:\n",
    "                logger.warning(\"Problème lors de la gestion de l'index textuel.\")\n",
    "            \n",
    "            # S'assurer que test_collection est disponible avant de l'utiliser\n",
    "            if test_collection is not None:\n",
    "                # Récupérer un _id valide à partir des chunks insérés\n",
    "                first_chunk_id = chunks_with_embeddings[0][\"chunk_id\"]\n",
    "                sample_doc_from_db = test_collection.find_one({\"_id\": first_chunk_id})\n",
    "                if sample_doc_from_db:\n",
    "                    logger.info(f\"Exemple de document récupéré de MongoDB (ID: {sample_doc_from_db['_id']}):\")\n",
    "                    logger.info(f\"  Texte: {sample_doc_from_db.get('text_chunk', '')[:100]}...\")\n",
    "                    logger.info(f\"  Fournisseur Embedding: {sample_doc_from_db.get('embedding_provider')}\")\n",
    "                    logger.info(f\"  Modèle Embedding: {sample_doc_from_db.get('embedding_model')}\")\n",
    "                    logger.info(f\"  Dimension Embedding (stockée): {sample_doc_from_db.get('embedding_dimension')}\")\n",
    "                    logger.info(f\"  Vecteur Embedding (premières dims): {str(sample_doc_from_db.get('embedding', [])[:3])[:100]}...\")\n",
    "                else:\n",
    "                    logger.warning(f\"Impossible de récupérer le document d'exemple '{first_chunk_id}' depuis MongoDB.\")\n",
    "            else:\n",
    "                logger.warning(\"test_collection non disponible, impossible de récupérer un document d'exemple.\")\n",
    "        else:\n",
    "            logger.warning(\"Aucun document n'a été inséré, la création des index pourrait ne pas être pertinente ou échouer sur une collection vide.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors des opérations MongoDB: {e}\", exc_info=True)\n",
    "    finally:\n",
    "        if mongo_mgr:\n",
    "            mongo_mgr.close()\n",
    "            logger.info(\"Connexion MongoDB fermée.\")\n",
    "else:\n",
    "    logger.warning(\"Aucun chunk avec embedding à stocker. Étape MongoDB sautée.\")\n",
    "\n",
    "logger.info(\"\\nPipeline d'Ingestion et d'Embedding terminé pour ce notebook !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5416e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook a illustré l'ensemble du pipeline d'ingestion :\n",
    "- Téléchargement des données sources (ArXiv).\n",
    "- Parsing pour extraire le texte et les métadonnées.\n",
    "- Prétraitement pour nettoyer et diviser le texte en chunks.\n",
    "- Génération des embeddings pour chaque chunk.\n",
    "- Stockage des données enrichies dans MongoDB et création des index nécessaires pour la recherche.\n",
    "\n",
    "Les données sont maintenant prêtes à être utilisées par le `RetrievalEngine` et les agents du \"Cognitive Swarm\". Vous pouvez explorer la collection MongoDB (`arxiv_chunks_notebook_test` dans la base `cognitive_swarm_db` par défaut) pour voir les documents stockés."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognitive-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
