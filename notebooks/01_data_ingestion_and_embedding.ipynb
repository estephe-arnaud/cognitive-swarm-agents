{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79506aac",
   "metadata": {},
   "source": [
    "# Notebook 01: Pipeline d'Ingestion et d'Embedding des Données ArXiv\n",
    "\n",
    "Ce notebook démontre le processus complet d'acquisition, de traitement, d'embedding et de stockage des articles scientifiques d'ArXiv dans notre base de données MongoDB. Il est conçu pour être une démonstration pratique des modules développés dans `src/`.\n",
    "\n",
    "**Prérequis :**\n",
    "*   Assurez-vous d'avoir exécuté le notebook `00_setup_environment.ipynb` et que votre environnement est correctement configuré (variables d'environnement chargées, clés API valides, MongoDB accessible).\n",
    "*   Les bibliothèques nécessaires doivent être installées via `environment.yml` (pour Conda) ou `requirements.txt`.\n",
    "*   Votre serveur Ollama (si utilisé comme fournisseur d'embedding) doit être en cours d'exécution avec le modèle d'embedding spécifié (`nomic-embed-text` par défaut) disponible.\n",
    "\n",
    "**Étapes de ce Notebook :**\n",
    "1.  **Configuration Initiale** : Imports, configuration du logging, définition des chemins et paramètres spécifiques au notebook.\n",
    "2.  **Téléchargement d'Articles depuis ArXiv** : Recherche et téléchargement des PDFs et de leurs métadonnées.\n",
    "3.  **Parsing des Documents** : Extraction du texte des PDFs et chargement des métadonnées.\n",
    "4.  **Prétraitement du Texte** : Nettoyage (minimal ici) et segmentation (chunking) du contenu textuel.\n",
    "5.  **Génération des Embeddings** : Conversion des chunks de texte en vecteurs numériques.\n",
    "6.  **Stockage dans MongoDB** : Insertion des chunks et de leurs embeddings dans une collection MongoDB et création des index de recherche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e65b0c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-05 20:59:04 - nb_01_ingestion_embedding - INFO - --- Initialisation du Notebook 01: Ingestion et Embedding ---\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:04 - nb_01_ingestion_embedding - INFO - Répertoire DATA_DIR global (settings.py): /home/facetoface/makers/data\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:04 - nb_01_ingestion_embedding - INFO - PDFs pour ce notebook seront stockés dans : /home/facetoface/makers/data/corpus/what_are_the_latest_advancements_in_using_large_la/pdfs\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:04 - nb_01_ingestion_embedding - INFO - Métadonnées pour ce notebook seront stockées dans : /home/facetoface/makers/data/corpus/what_are_the_latest_advancements_in_using_large_la/metadata\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:04 - nb_01_ingestion_embedding - INFO - --- Configuration d'Embedding Active (depuis settings.py & .env) ---\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:04 - nb_01_ingestion_embedding - INFO - Fournisseur d'embedding par défaut configuré : ollama\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:04 - nb_01_ingestion_embedding - INFO -   Modèle Ollama Embedding : nomic-embed-text (Dimension: 768)\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:04 - nb_01_ingestion_embedding - INFO -   URL de base Ollama : http://localhost:11434\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:04 - nb_01_ingestion_embedding - INFO -   ASSUREZ-VOUS que le modèle 'nomic-embed-text' est disponible sur votre serveur Ollama (ex: via 'ollama pull nomic-embed-text').\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:04 - nb_01_ingestion_embedding - INFO - Configuration initiale du notebook terminée et paramètres définis.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# --- Imports Standards et de Configuration ---\n",
    "import logging\n",
    "from typing import List, Optional, Dict, Any\n",
    "from pathlib import Path\n",
    "import re # Pour la fonction de nettoyage des noms de répertoires\n",
    "import shutil # Pour le nettoyage optionnel des répertoires\n",
    "import sys\n",
    "\n",
    "# --- Configuration du Projet et Logging ---\n",
    "# Assurer que la racine du projet est dans sys.path pour les imports de src/ et config/\n",
    "project_root_path = Path.cwd().parent \n",
    "if str(project_root_path) not in sys.path:\n",
    "    sys.path.append(str(project_root_path))\n",
    "    print(f\"Ajout de {project_root_path} à sys.path\")\n",
    "\n",
    "from pymongo.errors import ConnectionFailure \n",
    "from config.settings import settings\n",
    "from config.logging_config import setup_logging\n",
    "\n",
    "# --- Imports des Modules de Traitement de Données de src/ ---\n",
    "from src.data_processing.arxiv_downloader import search_arxiv_papers, download_papers as execute_arxiv_downloads\n",
    "from src.data_processing.document_parser import parse_document_collection, ParsedDocument # ParsedDocument importé pour type hinting si besoin\n",
    "from src.data_processing.preprocessor import preprocess_parsed_documents # ParsedDocument est utilisé en interne par cette fonction\n",
    "from src.data_processing.embedder import generate_embeddings_for_chunks \n",
    "from src.vector_store.mongodb_manager import MongoDBManager\n",
    "\n",
    "# --- Configuration du Logging pour ce Notebook ---\n",
    "setup_logging(level=\"INFO\") # Changer à \"DEBUG\" pour des logs plus détaillés\n",
    "logger = logging.getLogger(\"nb_01_ingestion_embedding\")\n",
    "\n",
    "logger.info(\"--- Initialisation du Notebook 01: Ingestion et Embedding ---\")\n",
    "\n",
    "# --- Paramètres Spécifiques à l'Exécution de ce Notebook ---\n",
    "# Requête ArXiv pour ce notebook\n",
    "ARXIV_QUERY_NOTEBOOK = \"What are the latest advancements in using large language models for robot task planning?\" \n",
    "# Nombre maximum de résultats à télécharger pour ce notebook (garder bas pour des tests rapides)\n",
    "MAX_RESULTS_NOTEBOOK = 3 \n",
    "# Nom de la collection MongoDB pour les chunks de ce notebook (pour isoler les données de test)\n",
    "COLLECTION_NAME_NOTEBOOK = f\"arxiv_chunks_nb_test_{settings.DEFAULT_EMBEDDING_PROVIDER}\" # Inclure le provider dans le nom\n",
    "# Noms des index pour cette collection de test\n",
    "VECTOR_INDEX_NAME_NOTEBOOK = \"vector_index_nb_test\"\n",
    "TEXT_INDEX_NAME_NOTEBOOK = \"text_index_nb_test\"\n",
    "\n",
    "\n",
    "# --- Définition et Création des Chemins de Sortie pour ce Notebook ---\n",
    "def sanitize_for_dir_name(query: str) -> str:\n",
    "    \"\"\"Convertit une chaîne de requête en un nom de répertoire valide.\"\"\"\n",
    "    s = query.lower()\n",
    "    s = re.sub(r'[\\s\\W-]+', '_', s) # Remplace espaces et caractères non alphanumériques (sauf _) par _\n",
    "    s = s.strip('_') # Enlève les _ en début/fin\n",
    "    return s[:50] # Limite la longueur\n",
    "\n",
    "# Crée un sous-répertoire unique pour les données de ce notebook basé sur la requête\n",
    "corpus_sub_dir_name_nb = sanitize_for_dir_name(ARXIV_QUERY_NOTEBOOK) \n",
    "notebook_corpus_base_path = Path(settings.DATA_DIR) / \"corpus\" / corpus_sub_dir_name_nb\n",
    "\n",
    "# Chemins spécifiques pour les PDFs et métadonnées téléchargés par ce notebook\n",
    "NB_PDF_OUTPUT_DIR = notebook_corpus_base_path / \"pdfs\"\n",
    "NB_METADATA_OUTPUT_DIR = notebook_corpus_base_path / \"metadata\"\n",
    "\n",
    "# Optionnel: Nettoyer les répertoires de sortie avant chaque exécution\n",
    "# Changez `clean_output_dirs` à True pour activer le nettoyage.\n",
    "CLEAN_OUTPUT_DIRS_ON_RUN = False \n",
    "if CLEAN_OUTPUT_DIRS_ON_RUN:\n",
    "    logger.info(\"Nettoyage des répertoires de sortie du notebook...\")\n",
    "    if NB_PDF_OUTPUT_DIR.exists():\n",
    "        shutil.rmtree(NB_PDF_OUTPUT_DIR)\n",
    "        logger.info(f\"Répertoire PDF supprimé : {NB_PDF_OUTPUT_DIR}\")\n",
    "    if NB_METADATA_OUTPUT_DIR.exists():\n",
    "        shutil.rmtree(NB_METADATA_OUTPUT_DIR)\n",
    "        logger.info(f\"Répertoire Metadata supprimé : {NB_METADATA_OUTPUT_DIR}\")\n",
    "\n",
    "# S'assurer que les répertoires de sortie existent\n",
    "NB_PDF_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "NB_METADATA_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(f\"Répertoire DATA_DIR global (settings.py): {settings.DATA_DIR}\")\n",
    "logger.info(f\"PDFs pour ce notebook seront stockés dans : {NB_PDF_OUTPUT_DIR}\")\n",
    "logger.info(f\"Métadonnées pour ce notebook seront stockées dans : {NB_METADATA_OUTPUT_DIR}\")\n",
    "\n",
    "# --- Vérification et Affichage de la Configuration d'Embedding Active ---\n",
    "logger.info(f\"--- Configuration d'Embedding Active (depuis settings.py & .env) ---\")\n",
    "active_embedding_provider = settings.DEFAULT_EMBEDDING_PROVIDER.lower()\n",
    "logger.info(f\"Fournisseur d'embedding par défaut configuré : {active_embedding_provider}\")\n",
    "\n",
    "if active_embedding_provider == \"openai\":\n",
    "    logger.info(f\"  Modèle OpenAI Embedding : {settings.OPENAI_EMBEDDING_MODEL_NAME} (Dimension: {settings.OPENAI_EMBEDDING_DIMENSION})\")\n",
    "    if not settings.OPENAI_API_KEY:\n",
    "        logger.error(\"  ERREUR : OPENAI_API_KEY n'est pas configurée dans .env. L'embedding OpenAI échouera.\")\n",
    "elif active_embedding_provider == \"huggingface\":\n",
    "    # Note: Pour HuggingFace, on suppose l'utilisation de sentence-transformers via langchain, pas d'appel API direct ici.\n",
    "    logger.info(f\"  Modèle HuggingFace Embedding : {settings.HUGGINGFACE_EMBEDDING_MODEL_NAME} (Dimension: {settings.HUGGINGFACE_EMBEDDING_MODEL_DIMENSION})\")\n",
    "elif active_embedding_provider == \"ollama\":\n",
    "    logger.info(f\"  Modèle Ollama Embedding : {settings.OLLAMA_EMBEDDING_MODEL_NAME} (Dimension: {settings.OLLAMA_EMBEDDING_MODEL_DIMENSION})\")\n",
    "    logger.info(f\"  URL de base Ollama : {settings.OLLAMA_BASE_URL}\")\n",
    "    if not settings.OLLAMA_BASE_URL:\n",
    "        logger.error(\"  ERREUR : OLLAMA_BASE_URL n'est pas configurée. L'embedding Ollama échouera.\")\n",
    "    logger.info(f\"  ASSUREZ-VOUS que le modèle '{settings.OLLAMA_EMBEDDING_MODEL_NAME}' est disponible sur votre serveur Ollama (ex: via 'ollama pull {settings.OLLAMA_EMBEDDING_MODEL_NAME}').\")\n",
    "else:\n",
    "    logger.error(f\"ERREUR : Fournisseur d'embedding inconnu ou non supporté dans ce notebook ('{active_embedding_provider}') configuré dans settings.py. La génération d'embeddings échouera.\")\n",
    "\n",
    "logger.info(\"Configuration initiale du notebook terminée et paramètres définis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a39ee30",
   "metadata": {},
   "source": [
    "### Étape 1: Téléchargement d'Articles depuis ArXiv\n",
    "\n",
    "Cette section utilise les fonctions `search_arxiv_papers` et `execute_arxiv_downloads` (anciennement `download_papers`) du module `src.data_processing.arxiv_downloader`.\n",
    "\n",
    "1.  **Recherche** : Interroge l'API ArXiv avec la `ARXIV_QUERY_NOTEBOOK` et les paramètres de tri/limite.\n",
    "2.  **Téléchargement** : Pour chaque article trouvé, télécharge le fichier PDF et sauvegarde ses métadonnées dans un fichier JSON.\n",
    "\n",
    "Les fichiers sont sauvegardés dans les répertoires `NB_PDF_OUTPUT_DIR` et `NB_METADATA_OUTPUT_DIR` définis précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d6b941a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-05 20:59:04 - nb_01_ingestion_embedding - INFO - --- Début Étape 1: Téléchargement d'Articles ArXiv ---\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:04 - nb_01_ingestion_embedding - INFO - Requête ArXiv : 'What are the latest advancements in using large language models for robot task planning?', Max résultats : 3\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:04 - nb_01_ingestion_embedding - INFO - Répertoire de sortie PDF : /home/facetoface/makers/data/corpus/what_are_the_latest_advancements_in_using_large_la/pdfs\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:04 - nb_01_ingestion_embedding - INFO - Répertoire de sortie Metadata : /home/facetoface/makers/data/corpus/what_are_the_latest_advancements_in_using_large_la/metadata\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:04 - nb_01_ingestion_embedding - INFO - Recherche en cours sur ArXiv...\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:04 - src.data_processing.arxiv_downloader - INFO - Searching ArXiv with query: What are the latest advancements in using large language models for robot task planning?\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:04 - arxiv - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=What+are+the+latest+advancements+in+using+large+language+models+for+robot+task+planning%3F&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:08 - arxiv - INFO - Got first page: 100 of 2751125 total results\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:08 - src.data_processing.arxiv_downloader - INFO - Found 3 results for query: What are the latest advancements in using large language models for robot task planning?\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:08 - nb_01_ingestion_embedding - INFO - 3 article(s) trouvé(s) sur ArXiv. Début du téléchargement...\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:08 - src.data_processing.arxiv_downloader - INFO - Processing paper 1/3: 2506.04229v1\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:08 - src.data_processing.arxiv_downloader - INFO - Downloading PDF for 2506.04229v1\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:09 - src.data_processing.arxiv_downloader - INFO - Saving metadata for 2506.04229v1\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:12 - src.data_processing.arxiv_downloader - INFO - Processing paper 2/3: 2506.04228v1\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:12 - src.data_processing.arxiv_downloader - INFO - Downloading PDF for 2506.04228v1\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:21 - src.data_processing.arxiv_downloader - INFO - Saving metadata for 2506.04228v1\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:24 - src.data_processing.arxiv_downloader - INFO - Processing paper 3/3: 2506.04227v1\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:24 - src.data_processing.arxiv_downloader - INFO - Downloading PDF for 2506.04227v1\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - src.data_processing.arxiv_downloader - INFO - Saving metadata for 2506.04227v1\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - src.data_processing.arxiv_downloader - INFO - Download statistics:\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - src.data_processing.arxiv_downloader - INFO -   Total papers: 3\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - src.data_processing.arxiv_downloader - INFO -   Successfully downloaded: 3\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - src.data_processing.arxiv_downloader - INFO -   Failed downloads: 0\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - src.data_processing.arxiv_downloader - INFO -   Skipped (already downloaded): 0\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO - Processus de téléchargement ArXiv terminé.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO - --- Bilan du Téléchargement ArXiv ---\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO -   Articles recherchés (max) : 3\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO -   Articles trouvés par ArXiv : 3\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO -   Articles traités pour téléchargement : 3\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO -   Téléchargements réussis : 3\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO -   Téléchargements échoués : 0\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO -   Téléchargements sautés (déjà existants) : 0\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO -   Vérification: 3 fichier(s) PDF dans /home/facetoface/makers/data/corpus/what_are_the_latest_advancements_in_using_large_la/pdfs.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO -     Exemple PDF: 2506.04228v1.pdf\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO -   Vérification: 3 fichier(s) JSON dans /home/facetoface/makers/data/corpus/what_are_the_latest_advancements_in_using_large_la/metadata.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO -     Exemple Metadata: 2506.04229v1_metadata.json\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO - --- Fin Étape 1 ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"--- Début Étape 1: Téléchargement d'Articles ArXiv ---\")\n",
    "logger.info(f\"Requête ArXiv : '{ARXIV_QUERY_NOTEBOOK}', Max résultats : {MAX_RESULTS_NOTEBOOK}\")\n",
    "logger.info(f\"Répertoire de sortie PDF : {NB_PDF_OUTPUT_DIR}\")\n",
    "logger.info(f\"Répertoire de sortie Metadata : {NB_METADATA_OUTPUT_DIR}\")\n",
    "\n",
    "# Initialiser une variable pour les statistiques de téléchargement\n",
    "download_stats = {\"successful\": 0, \"failed\": 0, \"skipped\": 0, \"total\": 0}\n",
    "search_results_list: List[Any] = [] # Pour stocker les résultats de recherche ArXiv\n",
    "\n",
    "try:\n",
    "    # Étape 1.1: Recherche des articles sur ArXiv\n",
    "    logger.info(f\"Recherche en cours sur ArXiv...\")\n",
    "    search_results_list = search_arxiv_papers(\n",
    "        query=ARXIV_QUERY_NOTEBOOK,\n",
    "        max_results=MAX_RESULTS_NOTEBOOK,\n",
    "        sort_by=settings.ARXIV_SORT_BY, \n",
    "        sort_order=settings.ARXIV_SORT_ORDER \n",
    "    )\n",
    "    \n",
    "    if not search_results_list:\n",
    "        logger.warning(\"Aucun article trouvé sur ArXiv pour cette requête. Vérifiez la requête ou les paramètres.\")\n",
    "    else:\n",
    "        logger.info(f\"{len(search_results_list)} article(s) trouvé(s) sur ArXiv. Début du téléchargement...\")\n",
    "        \n",
    "        # Étape 1.2: Téléchargement des articles trouvés\n",
    "        # La fonction execute_arxiv_downloads gère le délai et la journalisation interne.\n",
    "        download_stats = execute_arxiv_downloads(\n",
    "            results=search_results_list,\n",
    "            pdf_dir=NB_PDF_OUTPUT_DIR,\n",
    "            metadata_dir=NB_METADATA_OUTPUT_DIR\n",
    "        )\n",
    "        logger.info(f\"Processus de téléchargement ArXiv terminé.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Une erreur majeure est survenue lors de la recherche ou du téléchargement ArXiv : {e}\", exc_info=True)\n",
    "    # Mettre à jour les stats pour refléter l'échec si l'erreur s'est produite avant que download_stats ne soit peuplé\n",
    "    if not search_results_list: # Si l'erreur est survenue pendant la recherche\n",
    "        download_stats[\"total\"] = MAX_RESULTS_NOTEBOOK # Estimation\n",
    "        download_stats[\"failed\"] = MAX_RESULTS_NOTEBOOK # Estimation\n",
    "    elif download_stats[\"successful\"] == 0 and download_stats[\"skipped\"] == 0 : # Si erreur pendant le DL mais stats non màj\n",
    "        download_stats[\"total\"] = len(search_results_list)\n",
    "        download_stats[\"failed\"] = len(search_results_list)\n",
    "\n",
    "\n",
    "# --- Bilan du Téléchargement ---\n",
    "logger.info(f\"--- Bilan du Téléchargement ArXiv ---\")\n",
    "logger.info(f\"  Articles recherchés (max) : {MAX_RESULTS_NOTEBOOK}\")\n",
    "logger.info(f\"  Articles trouvés par ArXiv : {len(search_results_list)}\")\n",
    "logger.info(f\"  Articles traités pour téléchargement : {download_stats.get('total', len(search_results_list))}\") # .get pour si stats pas peuplé\n",
    "logger.info(f\"  Téléchargements réussis : {download_stats.get('successful', 0)}\")\n",
    "logger.info(f\"  Téléchargements échoués : {download_stats.get('failed', 0)}\")\n",
    "logger.info(f\"  Téléchargements sautés (déjà existants) : {download_stats.get('skipped', 0)}\")\n",
    "\n",
    "# Lister les fichiers téléchargés pour vérification (utile dans un notebook)\n",
    "if download_stats.get('successful', 0) > 0 or download_stats.get('skipped', 0) > 0 :\n",
    "    downloaded_pdfs = list(NB_PDF_OUTPUT_DIR.glob(\"*.pdf\"))\n",
    "    downloaded_metadata_files = list(NB_METADATA_OUTPUT_DIR.glob(\"*.json\"))\n",
    "    logger.info(f\"  Vérification: {len(downloaded_pdfs)} fichier(s) PDF dans {NB_PDF_OUTPUT_DIR}.\")\n",
    "    if downloaded_pdfs:\n",
    "        logger.info(f\"    Exemple PDF: {downloaded_pdfs[0].name}\")\n",
    "    logger.info(f\"  Vérification: {len(downloaded_metadata_files)} fichier(s) JSON dans {NB_METADATA_OUTPUT_DIR}.\")\n",
    "    if downloaded_metadata_files:\n",
    "        logger.info(f\"    Exemple Metadata: {downloaded_metadata_files[0].name}\")\n",
    "else:\n",
    "    logger.warning(\"Aucun nouvel article n'a été téléchargé avec succès. Les étapes suivantes pourraient ne pas avoir de données à traiter.\")\n",
    "\n",
    "logger.info(\"--- Fin Étape 1 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e64f8",
   "metadata": {},
   "source": [
    "### Étape 2: Parsing des Documents Téléchargés\n",
    "\n",
    "Après le téléchargement, cette étape cruciale consiste à :\n",
    "1.  **Lire les fichiers PDF** pour en extraire le contenu textuel brut.\n",
    "2.  **Charger les fichiers de métadonnées JSON** associés à chaque PDF.\n",
    "3.  **Combiner** le texte extrait et les métadonnées dans une structure de données unifiée (une liste d'objets `ParsedDocument` ou de dictionnaires similaires) pour chaque article.\n",
    "\n",
    "Nous utilisons la fonction `parse_document_collection` du module `src.data_processing.document_parser`. Cette fonction prend en argument les chemins vers les répertoires contenant les PDFs et les métadonnées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6537dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO - --- Début Étape 2: Parsing des Documents ---\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO - Répertoire des PDFs à parser : /home/facetoface/makers/data/corpus/what_are_the_latest_advancements_in_using_large_la/pdfs\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO - Répertoire des métadonnées à parser : /home/facetoface/makers/data/corpus/what_are_the_latest_advancements_in_using_large_la/metadata\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - nb_01_ingestion_embedding - INFO - Début du parsing pour 3 PDF(s) potentiels...\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - src.data_processing.document_parser - INFO - Found 3 PDF files in /home/facetoface/makers/data/corpus/what_are_the_latest_advancements_in_using_large_la/pdfs to parse\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:38 - src.data_processing.document_parser - INFO - Parsing PDF: 2506.04228v1.pdf\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - src.data_processing.document_parser - INFO - Successfully processed and added document: 2506.04228\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - src.data_processing.document_parser - INFO - Parsing PDF: 2506.04227v1.pdf\u001b[0m\n",
      "MuPDF error: syntax error: could not parse color space (218 0 R)\n",
      "\n",
      "\u001b[34m2025-06-05 20:59:39 - src.data_processing.document_parser - INFO - Successfully processed and added document: 2506.04227\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - src.data_processing.document_parser - INFO - Parsing PDF: 2506.04229v1.pdf\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - src.data_processing.document_parser - INFO - Successfully processed and added document: 2506.04229\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - src.data_processing.document_parser - INFO - Finished parsing collection. Successfully processed 3 documents\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO - 3 document(s) ont été parsé(s) avec succès.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO -   Exemple de document parsé (ID ArXiv): 2506.04228\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO -   Titre: LayerFlow: A Unified Model for Layer-aware Video Generation...\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO -   Extrait du texte (100 premiers car.): LayerFlow: A Unified Model for Layer-aware Video Generation SIHUI JI∗, The University of Hong Kong, ...\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO -   Nombre de pages: N/A\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO - Bilan Parsing: 3 document(s) prêt(s) pour le prétraitement.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO - --- Fin Étape 2 ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"--- Début Étape 2: Parsing des Documents ---\")\n",
    "logger.info(f\"Répertoire des PDFs à parser : {NB_PDF_OUTPUT_DIR}\")\n",
    "logger.info(f\"Répertoire des métadonnées à parser : {NB_METADATA_OUTPUT_DIR}\")\n",
    "\n",
    "# Initialiser la liste des documents parsés\n",
    "# parse_document_collection retourne List[ParsedDocument], où ParsedDocument est un TypedDict.\n",
    "parsed_documents: List[ParsedDocument] = [] \n",
    "\n",
    "# Vérifier si des PDFs existent pour le parsing (suite à l'étape de téléchargement)\n",
    "pdf_files_for_parsing = list(NB_PDF_OUTPUT_DIR.glob(\"*.pdf\"))\n",
    "metadata_files_for_parsing = list(NB_METADATA_OUTPUT_DIR.glob(\"*.json\"))\n",
    "\n",
    "if not pdf_files_for_parsing:\n",
    "    logger.warning(f\"Aucun fichier PDF trouvé dans {NB_PDF_OUTPUT_DIR}. L'étape de parsing sera sautée.\")\n",
    "elif not metadata_files_for_parsing:\n",
    "    logger.warning(f\"Aucun fichier de métadonnées JSON trouvé dans {NB_METADATA_OUTPUT_DIR}. Le parsing pourrait manquer d'informations contextuelles.\")\n",
    "    # On peut décider de continuer ou d'arrêter. Pour ce notebook, on continue en signalant.\n",
    "    \n",
    "if pdf_files_for_parsing: \n",
    "    logger.info(f\"Début du parsing pour {len(pdf_files_for_parsing)} PDF(s) potentiels...\")\n",
    "    try:\n",
    "        # Appel de la fonction de parsing\n",
    "        parsed_documents = parse_document_collection(\n",
    "            pdf_dir=NB_PDF_OUTPUT_DIR,\n",
    "            metadata_dir=NB_METADATA_OUTPUT_DIR \n",
    "        )\n",
    "        \n",
    "        if parsed_documents:\n",
    "            logger.info(f\"{len(parsed_documents)} document(s) ont été parsé(s) avec succès.\")\n",
    "            # Afficher des informations sur le premier document parsé (si disponible)\n",
    "            first_doc_parsed = parsed_documents[0]\n",
    "            # CORRECTION: Utiliser l'accès par clé pour TypedDict\n",
    "            logger.info(f\"  Exemple de document parsé (ID ArXiv): {first_doc_parsed.get('arxiv_id', 'N/A')}\")\n",
    "            logger.info(f\"  Titre: {first_doc_parsed.get('metadata', {}).get('title', 'N/A')[:100]}...\")\n",
    "            logger.info(f\"  Extrait du texte (100 premiers car.): {first_doc_parsed.get('text_content', '')[:100].replace(chr(10), ' ')}...\")\n",
    "            logger.info(f\"  Nombre de pages: {first_doc_parsed.get('metadata', {}).get('page_count', 'N/A')}\")\n",
    "        else:\n",
    "            logger.warning(\"Aucun document n'a pu être parsé. Vérifiez les logs précédents pour des erreurs spécifiques (ex: PDF corrompu).\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Une erreur majeure est survenue lors du parsing des documents : {e}\", exc_info=True)\n",
    "        # S'assurer que parsed_documents est une liste vide en cas d'erreur\n",
    "        parsed_documents = []\n",
    "\n",
    "# Bilan du Parsing\n",
    "if not parsed_documents:\n",
    "    logger.warning(\"Bilan Parsing: Aucun document n'a été parsé avec succès. Les étapes suivantes (prétraitement, embedding, stockage) seront impactées ou sautées.\")\n",
    "else:\n",
    "    logger.info(f\"Bilan Parsing: {len(parsed_documents)} document(s) prêt(s) pour le prétraitement.\")\n",
    "\n",
    "logger.info(\"--- Fin Étape 2 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f71ac7d",
   "metadata": {},
   "source": [
    "### Étape 3: Prétraitement des Documents Parsés (Chunking)\n",
    "\n",
    "Le texte brut extrait de chaque article est souvent volumineux. Pour le rendre exploitable par les modèles d'embedding et les LLMs (qui ont des limites de taille de contexte), nous devons le segmenter.\n",
    "\n",
    "Cette étape, appelée \"chunking\", consiste à :\n",
    "1.  Prendre le contenu textuel de chaque `ParsedDocument`.\n",
    "2.  Le diviser en morceaux (chunks) plus petits, tout en essayant de préserver la cohérence sémantique (par exemple, en ne coupant pas au milieu d'une phrase si possible, ou en utilisant un chevauchement entre les chunks).\n",
    "3.  Associer à chaque chunk les métadonnées pertinentes du document d'origine et des informations spécifiques au chunk (comme son numéro de séquence).\n",
    "\n",
    "Nous utilisons `preprocess_parsed_documents` de `src.data_processing.preprocessor`. Les paramètres de chunking (taille `CHUNK_SIZE`, chevauchement `CHUNK_OVERLAP`) sont définis dans `config/settings.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0550720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO - --- Début Étape 3: Prétraitement des Documents (Chunking) ---\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO - Début du prétraitement pour 3 document(s) parsé(s).\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO - Paramètres de chunking (depuis settings.py): CHUNK_SIZE=1000, CHUNK_OVERLAP=200\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - src.data_processing.preprocessor - INFO - Starting preprocessing for 3 parsed documents.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - src.data_processing.preprocessor - INFO - Preprocessing document 1/3: 2506.04228\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - src.data_processing.preprocessor - INFO - Preprocessing document 2/3: 2506.04227\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - src.data_processing.preprocessor - INFO - Preprocessing document 3/3: 2506.04229\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - src.data_processing.preprocessor - INFO - Finished preprocessing. Generated 47 chunks in total.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO - 47 chunks ont été générés au total.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO -   Exemple de chunk généré (ID ArXiv d'origine): N/A\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO -     Titre d'origine: N/A...\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO -     Numéro du chunk: N/A\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO -     Taille du texte du chunk: 4282 caractères\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO -     Extrait du texte du chunk (100 premiers car.): LayerFlow: A Unified Model for Layer-aware Video Generation SIHUI JI∗, The University of Hong Kong, ...\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO - Bilan Prétraitement: 47 chunks prêts pour la génération d'embeddings.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO - --- Fin Étape 3 ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"--- Début Étape 3: Prétraitement des Documents (Chunking) ---\")\n",
    "\n",
    "# Initialiser la liste des chunks\n",
    "chunks_with_metadata: List[Dict[str, Any]] = [] # La fonction retourne une liste de dictionnaires\n",
    "\n",
    "if not parsed_documents:\n",
    "    logger.warning(\"Aucun document parsé disponible pour le prétraitement (chunking). Étape sautée.\")\n",
    "else:\n",
    "    logger.info(f\"Début du prétraitement pour {len(parsed_documents)} document(s) parsé(s).\")\n",
    "    logger.info(f\"Paramètres de chunking (depuis settings.py): CHUNK_SIZE={settings.CHUNK_SIZE}, CHUNK_OVERLAP={settings.CHUNK_OVERLAP}\")\n",
    "    \n",
    "    try:\n",
    "        # La fonction preprocess_parsed_documents prend List[ParsedDocument]\n",
    "        # et retourne une List[Dict[str, Any]], où chaque dict est un chunk.\n",
    "        chunks_with_metadata = preprocess_parsed_documents(parsed_documents)\n",
    "        \n",
    "        if chunks_with_metadata:\n",
    "            logger.info(f\"{len(chunks_with_metadata)} chunks ont été générés au total.\")\n",
    "            # Afficher des informations sur le premier chunk (si disponible) pour vérification\n",
    "            first_chunk_example = chunks_with_metadata[0]\n",
    "            logger.info(f\"  Exemple de chunk généré (ID ArXiv d'origine): {first_chunk_example.get('metadata', {}).get('arxiv_id', 'N/A')}\")\n",
    "            logger.info(f\"    Titre d'origine: {first_chunk_example.get('metadata', {}).get('original_document_title', 'N/A')[:70]}...\")\n",
    "            logger.info(f\"    Numéro du chunk: {first_chunk_example.get('metadata', {}).get('chunk_sequence_number', 'N/A')}\")\n",
    "            logger.info(f\"    Taille du texte du chunk: {len(first_chunk_example.get('text_chunk', ''))} caractères\")\n",
    "            logger.info(f\"    Extrait du texte du chunk (100 premiers car.): {first_chunk_example.get('text_chunk', '')[:100].replace(chr(10), ' ')}...\")\n",
    "        else:\n",
    "            logger.warning(\"Aucun chunk n'a été généré. Vérifiez le contenu des documents (pourraient être trop courts) ou les paramètres de chunking.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Une erreur majeure est survenue lors du prétraitement (chunking) des documents : {e}\", exc_info=True)\n",
    "        chunks_with_metadata = [] # S'assurer que la variable est une liste vide en cas d'erreur\n",
    "\n",
    "# Bilan du Prétraitement\n",
    "if not chunks_with_metadata:\n",
    "    logger.warning(\"Bilan Prétraitement: Aucun chunk n'a été généré. Les étapes suivantes (embedding, stockage) seront impactées ou sautées.\")\n",
    "else:\n",
    "    logger.info(f\"Bilan Prétraitement: {len(chunks_with_metadata)} chunks prêts pour la génération d'embeddings.\")\n",
    "\n",
    "logger.info(f\"--- Fin Étape 3 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b24a9",
   "metadata": {},
   "source": [
    "### Étape 4: Génération des Embeddings pour les Chunks\n",
    "\n",
    "Les chunks de texte doivent être transformés en représentations vectorielles numériques (embeddings) pour permettre des recherches de similarité sémantique. Chaque chunk sera associé à un vecteur d'embedding.\n",
    "\n",
    "Le fournisseur d'embedding (Ollama, OpenAI, HuggingFace Sentence Transformers, etc.) et le modèle spécifique sont déterminés par les configurations dans `config/settings.py` (et chargées via `.env`). Ce notebook utilisera la configuration active.\n",
    "\n",
    "La fonction `generate_embeddings_for_chunks` du module `src.data_processing.embedder` est utilisée. Elle prend la liste des chunks (dictionnaires) et y ajoute une clé `embedding` contenant le vecteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "817ea79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO - --- Début Étape 4: Génération des Embeddings ---\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO - Début de la génération des embeddings pour 47 chunk(s).\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO - Utilisation du fournisseur d'embedding configuré: OLLAMA\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO -   Modèle Ollama: nomic-embed-text, URL: http://localhost:11434\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - nb_01_ingestion_embedding - INFO -   Assurez-vous que le serveur Ollama est actif et le modèle 'nomic-embed-text' est accessible (ex: 'ollama pull nomic-embed-text').\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - src.data_processing.embedder - INFO - Starting embedding generation for 47 chunks.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - src.data_processing.embedder - INFO - Initializing embedding client for provider: ollama\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - src.data_processing.embedder - INFO - Using OllamaEmbeddings with model: nomic-embed-text via http://localhost:11434\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:39 - src.data_processing.embedder - INFO - Embedding batch 1/2 (size: 32) using ollama provider.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/facetoface/makers/src/data_processing/embedder.py:75: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  return OllamaEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-05 20:59:40 - src.data_processing.embedder - INFO - Embedding batch 2/2 (size: 15) using ollama provider.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - src.data_processing.embedder - INFO - Finished embedding generation. Successfully structured 47 chunks for DB out of 47.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - nb_01_ingestion_embedding - INFO - Embeddings générés avec succès pour 47 chunk(s).\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - nb_01_ingestion_embedding - INFO -   Exemple d'embedding pour le premier chunk (ID ArXiv d'origine: 2506.04228):\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - nb_01_ingestion_embedding - INFO -     Type de l'embedding: <class 'list'>\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - nb_01_ingestion_embedding - INFO -     Dimension de l'embedding: 768\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - nb_01_ingestion_embedding - INFO -     Extrait de l'embedding (5 premières valeurs): [1.0089068412780762, 0.9489849805831909, -2.6395182609558105, -0.37363749742507935, 0.31627190113067627]...\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - nb_01_ingestion_embedding - INFO -     La dimension de l'embedding (768) correspond à la dimension attendue (768).\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - nb_01_ingestion_embedding - INFO - Bilan Embeddings: 47 chunks avec embeddings sont prêts pour le stockage.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - nb_01_ingestion_embedding - INFO - --- Fin Étape 4 ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"--- Début Étape 4: Génération des Embeddings ---\")\n",
    "\n",
    "# Initialiser la liste des chunks avec embeddings\n",
    "# La fonction modifie les dictionnaires en place et les retourne.\n",
    "embedded_chunks: List[Dict[str, Any]] = [] \n",
    "\n",
    "if not chunks_with_metadata:\n",
    "    logger.warning(\"Aucun chunk disponible pour la génération d'embeddings. Étape sautée.\")\n",
    "else:\n",
    "    logger.info(f\"Début de la génération des embeddings pour {len(chunks_with_metadata)} chunk(s).\")\n",
    "    logger.info(f\"Utilisation du fournisseur d'embedding configuré: {settings.DEFAULT_EMBEDDING_PROVIDER.upper()}\")\n",
    "    \n",
    "    # Rappel des configurations spécifiques au fournisseur (déjà affiché mais utile ici)\n",
    "    if settings.DEFAULT_EMBEDDING_PROVIDER == 'ollama':\n",
    "        logger.info(f\"  Modèle Ollama: {settings.OLLAMA_EMBEDDING_MODEL_NAME}, URL: {settings.OLLAMA_BASE_URL}\")\n",
    "        logger.info(f\"  Assurez-vous que le serveur Ollama est actif et le modèle '{settings.OLLAMA_EMBEDDING_MODEL_NAME}' est accessible (ex: 'ollama pull {settings.OLLAMA_EMBEDDING_MODEL_NAME}').\")\n",
    "    elif settings.DEFAULT_EMBEDDING_PROVIDER == 'openai':\n",
    "        logger.info(f\"  Modèle OpenAI: {settings.OPENAI_EMBEDDING_MODEL_NAME}\")\n",
    "        if not settings.OPENAI_API_KEY:\n",
    "             logger.error(\"  ATTENTION: OPENAI_API_KEY n'est pas configurée. L'embedding via OpenAI échouera.\")\n",
    "    # Ajouter d'autres fournisseurs si nécessaire\n",
    "\n",
    "    try:\n",
    "        # La fonction generate_embeddings_for_chunks prend List[Dict[str, Any]] (chunks)\n",
    "        # et retourne la même liste, chaque dictionnaire étant enrichi d'une clé 'embedding'.\n",
    "        embedded_chunks = generate_embeddings_for_chunks(chunks_with_metadata) # Passe une copie pour éviter modif en place si on réexécute la cellule\n",
    "        \n",
    "        if embedded_chunks and 'embedding' in embedded_chunks[0] and embedded_chunks[0]['embedding'] is not None:\n",
    "            logger.info(f\"Embeddings générés avec succès pour {len(embedded_chunks)} chunk(s).\")\n",
    "            first_embedded_chunk = embedded_chunks[0]\n",
    "            embedding_sample = first_embedded_chunk['embedding']\n",
    "            logger.info(f\"  Exemple d'embedding pour le premier chunk (ID ArXiv d'origine: {first_embedded_chunk.get('metadata', {}).get('arxiv_id', 'N/A')}):\")\n",
    "            logger.info(f\"    Type de l'embedding: {type(embedding_sample)}\")\n",
    "            logger.info(f\"    Dimension de l'embedding: {len(embedding_sample) if isinstance(embedding_sample, list) else 'N/A'}\")\n",
    "            logger.info(f\"    Extrait de l'embedding (5 premières valeurs): {embedding_sample[:5]}...\")\n",
    "            \n",
    "            # Vérification de la dimension attendue\n",
    "            expected_dim = 0\n",
    "            if settings.DEFAULT_EMBEDDING_PROVIDER == 'ollama':\n",
    "                expected_dim = settings.OLLAMA_EMBEDDING_MODEL_DIMENSION\n",
    "            elif settings.DEFAULT_EMBEDDING_PROVIDER == 'openai':\n",
    "                expected_dim = settings.OPENAI_EMBEDDING_DIMENSION\n",
    "            elif settings.DEFAULT_EMBEDDING_PROVIDER == 'huggingface':\n",
    "                expected_dim = settings.HUGGINGFACE_EMBEDDING_MODEL_DIMENSION\n",
    "            \n",
    "            if expected_dim > 0 and isinstance(embedding_sample, list) and len(embedding_sample) != expected_dim:\n",
    "                logger.warning(f\"  ATTENTION: La dimension de l'embedding généré ({len(embedding_sample)}) ne correspond pas à la dimension attendue ({expected_dim}) configurée dans settings.py pour {settings.DEFAULT_EMBEDDING_PROVIDER}.\")\n",
    "            elif expected_dim > 0:\n",
    "                 logger.info(f\"    La dimension de l'embedding ({len(embedding_sample)}) correspond à la dimension attendue ({expected_dim}).\")\n",
    "\n",
    "        elif embedded_chunks and ('embedding' not in embedded_chunks[0] or embedded_chunks[0]['embedding'] is None):\n",
    "            logger.warning(f\"La génération d'embeddings semble avoir retourné des données, mais la clé 'embedding' est manquante ou vide pour le premier chunk. Vérifiez le processus d'embedding.\")\n",
    "        else:\n",
    "            logger.warning(\"Aucun embedding n'a été généré ou retourné. Vérifiez les logs du fournisseur d'embedding.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Une erreur majeure est survenue lors de la génération des embeddings : {e}\", exc_info=True)\n",
    "        embedded_chunks = [] # S'assurer que la variable est une liste vide en cas d'erreur\n",
    "\n",
    "# Bilan de la Génération d'Embeddings\n",
    "if not embedded_chunks or not ('embedding' in embedded_chunks[0] and embedded_chunks[0]['embedding'] is not None):\n",
    "    logger.warning(\"Bilan Embeddings: Aucun embedding valide n'a été généré. Le stockage dans MongoDB sera impacté ou sauté.\")\n",
    "else:\n",
    "    logger.info(f\"Bilan Embeddings: {len(embedded_chunks)} chunks avec embeddings sont prêts pour le stockage.\")\n",
    "\n",
    "logger.info(f\"--- Fin Étape 4 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1c78c",
   "metadata": {},
   "source": [
    "### Étape 5: Stockage des Chunks et Embeddings dans MongoDB\n",
    "\n",
    "Les chunks de texte, enrichis de leurs métadonnées et de leurs embeddings vectoriels, sont maintenant prêts à être stockés dans une base de données vectorielle. Nous utilisons MongoDB Atlas comme backend, qui supporte la recherche vectorielle.\n",
    "\n",
    "Cette étape implique :\n",
    "1.  **Connexion à MongoDB** : Établir une connexion à l'instance MongoDB spécifiée dans `MONGODB_URI`.\n",
    "2.  **Préparation des Données** : S'assurer que les données sont dans un format adéquat pour l'insertion.\n",
    "3.  **Insertion des Données** : Insérer les chunks (avec texte, métadonnées, et embedding) dans la collection MongoDB spécifiée (`COLLECTION_NAME_NOTEBOOK`).\n",
    "4.  **Création d'Index** :\n",
    "    *   Créer un **index de recherche vectorielle** sur le champ `embedding` pour permettre des recherches de similarité rapides.\n",
    "    *   Optionnellement, créer des **index textuels** sur d'autres champs (comme `text_chunk` ou des champs de métadonnées) pour des recherches hybrides ou par mots-clés.\n",
    "\n",
    "Nous utilisons la classe `MongoDBManager` de `src.vector_store.mongodb_manager` pour gérer ces opérations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecc36835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-05 20:59:41 - nb_01_ingestion_embedding - INFO - --- Début Étape 5: Stockage dans MongoDB ---\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - nb_01_ingestion_embedding - INFO - Préparation au stockage de 47 chunks dans MongoDB.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - nb_01_ingestion_embedding - INFO -   Base de données cible : makers_db\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - nb_01_ingestion_embedding - INFO -   Collection cible pour ce notebook : arxiv_chunks_nb_test_ollama\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - nb_01_ingestion_embedding - INFO - Initialisation du MongoDBManager...\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - src.vector_store.mongodb_manager - INFO - Initialized MongoDB manager for database: makers_db\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - src.vector_store.mongodb_manager - INFO - Connected to MongoDB database: makers_db\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - nb_01_ingestion_embedding - INFO - Connecté à MongoDB (Base de données: makers_db).\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - nb_01_ingestion_embedding - INFO - La collection 'arxiv_chunks_nb_test_ollama' n'existe pas encore, aucune suppression nécessaire.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:41 - nb_01_ingestion_embedding - INFO - Insertion de 47 chunks dans la collection 'arxiv_chunks_nb_test_ollama'...\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:43 - nb_01_ingestion_embedding - INFO - Résumé de l'insertion : {'inserted_count': 47, 'duplicate_count': 0, 'errors': []}\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:43 - nb_01_ingestion_embedding - INFO - Création de l'index de recherche vectorielle 'vector_index_nb_test' sur le champ 'embedding'. Dimension attendue: 768.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:43 - src.vector_store.mongodb_manager - INFO - Using embedding dimension 768 for index 'vector_index_nb_test'\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:44 - src.vector_store.mongodb_manager - INFO - Vector search index 'vector_index_nb_test' creation initiated\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:44 - nb_01_ingestion_embedding - INFO - Requête de création de l'index vectoriel 'vector_index_nb_test' soumise. Cela peut prendre quelques minutes pour être actif sur Atlas.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:44 - nb_01_ingestion_embedding - INFO - Création de l'index textuel 'text_index_nb_test' sur le champ 'text_chunk' et certains champs de métadonnées...\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:44 - src.vector_store.mongodb_manager - INFO - Text search index 'text_index_nb_test' creation initiated\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:44 - nb_01_ingestion_embedding - INFO - Requête de création de l'index textuel 'text_index_nb_test' soumise.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:44 - nb_01_ingestion_embedding - INFO - Stockage dans MongoDB et création des index terminés.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:44 - nb_01_ingestion_embedding - INFO - Fermeture de la connexion MongoDB.\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:44 - src.vector_store.mongodb_manager - INFO - MongoDB connection closed\u001b[0m\n",
      "\u001b[34m2025-06-05 20:59:44 - nb_01_ingestion_embedding - INFO - --- Fin Étape 5 ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"--- Début Étape 5: Stockage dans MongoDB ---\")\n",
    "\n",
    "if not embedded_chunks or not ('embedding' in embedded_chunks[0] and embedded_chunks[0]['embedding'] is not None):\n",
    "    logger.warning(\"Aucun chunk avec embedding valide disponible pour le stockage dans MongoDB. Étape sautée.\")\n",
    "else:\n",
    "    logger.info(f\"Préparation au stockage de {len(embedded_chunks)} chunks dans MongoDB.\")\n",
    "    logger.info(f\"  Base de données cible : {settings.MONGO_DATABASE_NAME}\")\n",
    "    logger.info(f\"  Collection cible pour ce notebook : {COLLECTION_NAME_NOTEBOOK}\")\n",
    "\n",
    "    mongo_mgr = None \n",
    "    try:\n",
    "        logger.info(\"Initialisation du MongoDBManager...\")\n",
    "        mongo_mgr = MongoDBManager(\n",
    "            mongo_uri=settings.MONGODB_URI,\n",
    "            db_name=settings.MONGO_DATABASE_NAME\n",
    "        )\n",
    "        mongo_mgr.connect() \n",
    "        logger.info(f\"Connecté à MongoDB (Base de données: {mongo_mgr.db.name}).\")\n",
    "\n",
    "        CLEAN_COLLECTION_BEFORE_INSERT = True \n",
    "        if CLEAN_COLLECTION_BEFORE_INSERT:\n",
    "            if mongo_mgr.collection_exists(COLLECTION_NAME_NOTEBOOK): \n",
    "                logger.warning(f\"La collection '{COLLECTION_NAME_NOTEBOOK}' existe déjà. Tentative de suppression pour ce test de notebook...\")\n",
    "                mongo_mgr.db[COLLECTION_NAME_NOTEBOOK].drop()\n",
    "                logger.info(f\"Collection '{COLLECTION_NAME_NOTEBOOK}' supprimée.\")\n",
    "            else:\n",
    "                logger.info(f\"La collection '{COLLECTION_NAME_NOTEBOOK}' n'existe pas encore, aucune suppression nécessaire.\")\n",
    "        \n",
    "        logger.info(f\"Insertion de {len(embedded_chunks)} chunks dans la collection '{COLLECTION_NAME_NOTEBOOK}'...\")\n",
    "        insert_summary = mongo_mgr.insert_chunks_with_embeddings(\n",
    "            chunks=embedded_chunks, \n",
    "            collection_name=COLLECTION_NAME_NOTEBOOK\n",
    "        )\n",
    "        logger.info(f\"Résumé de l'insertion : {insert_summary}\")\n",
    "\n",
    "        if insert_summary.get(\"inserted_count\", 0) > 0:\n",
    "            expected_dimension_for_log = 0\n",
    "            try:\n",
    "                expected_dimension_for_log = mongo_mgr.get_effective_embedding_dimension()\n",
    "            except ValueError:\n",
    "                logger.warning(\"Impossible de déterminer la dimension d'embedding attendue depuis les settings.\")\n",
    "\n",
    "            logger.info(f\"Création de l'index de recherche vectorielle '{VECTOR_INDEX_NAME_NOTEBOOK}' sur le champ 'embedding'. Dimension attendue: {expected_dimension_for_log if expected_dimension_for_log > 0 else 'auto-détectée par le manager'}.\")\n",
    "            \n",
    "            vector_index_filter_fields = [\n",
    "                \"metadata.arxiv_id\",\n",
    "                \"metadata.original_document_title\",\n",
    "                \"metadata.primary_category\",\n",
    "                \"metadata.published_year\" \n",
    "            ]\n",
    "            # CORRECTION 2: Retirer également 'similarity_metric' de l'appel\n",
    "            mongo_mgr.create_vector_search_index(\n",
    "                collection_name=COLLECTION_NAME_NOTEBOOK,\n",
    "                index_name=VECTOR_INDEX_NAME_NOTEBOOK,\n",
    "                embedding_field=\"embedding\", \n",
    "                filter_fields=vector_index_filter_fields \n",
    "            )\n",
    "            logger.info(f\"Requête de création de l'index vectoriel '{VECTOR_INDEX_NAME_NOTEBOOK}' soumise. Cela peut prendre quelques minutes pour être actif sur Atlas.\")\n",
    "\n",
    "            logger.info(f\"Création de l'index textuel '{TEXT_INDEX_NAME_NOTEBOOK}' sur le champ 'text_chunk' et certains champs de métadonnées...\")\n",
    "            text_index_additional_fields = {\n",
    "                \"metadata.original_document_title\": \"text\", \n",
    "                \"metadata.summary\": \"text\" \n",
    "            }\n",
    "            mongo_mgr.create_text_search_index(\n",
    "                collection_name=COLLECTION_NAME_NOTEBOOK,\n",
    "                index_name=TEXT_INDEX_NAME_NOTEBOOK,\n",
    "                text_field=\"text_chunk\", \n",
    "                additional_text_fields=text_index_additional_fields\n",
    "            )\n",
    "            logger.info(f\"Requête de création de l'index textuel '{TEXT_INDEX_NAME_NOTEBOOK}' soumise.\")\n",
    "            \n",
    "            logger.info(\"Stockage dans MongoDB et création des index terminés.\")\n",
    "        else:\n",
    "            logger.warning(\"Aucun document n'a été inséré. La création des index sera sautée.\")\n",
    "\n",
    "    except ConnectionFailure:\n",
    "        logger.error(\"ERREUR CRITIQUE : Échec de la connexion à MongoDB. Vérifiez votre MONGODB_URI et l'accessibilité du serveur.\", exc_info=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Une erreur majeure est survenue lors de l'interaction avec MongoDB : {e}\", exc_info=True)\n",
    "    finally:\n",
    "        if mongo_mgr:\n",
    "            logger.info(\"Fermeture de la connexion MongoDB.\")\n",
    "            mongo_mgr.close()\n",
    "\n",
    "logger.info(f\"--- Fin Étape 5 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5416e",
   "metadata": {},
   "source": [
    "## Fin du Notebook 01: Ingestion et Embedding\n",
    "\n",
    "Ce notebook a couvert l'ensemble du pipeline d'ingestion de données ArXiv :\n",
    "- Téléchargement des articles et de leurs métadonnées.\n",
    "- Parsing des PDFs pour extraire le texte.\n",
    "- Prétraitement du texte par segmentation (chunking).\n",
    "- Génération des embeddings vectoriels pour chaque chunk.\n",
    "- Stockage des chunks enrichis dans MongoDB Atlas, avec création d'index vectoriels et textuels.\n",
    "\n",
    "**Prochaines Étapes Suggérées :**\n",
    "- **Explorer les Données Stockées** : Vous pouvez utiliser MongoDB Compass ou un autre notebook pour interroger la collection `COLLECTION_NAME_NOTEBOOK` et examiner les documents stockés.\n",
    "- **Notebook 02: Exploration des Stratégies RAG** : Utiliser les données ingérées pour tester différentes approches de recherche sémantique et de génération de réponses.\n",
    "\n",
    "Si des erreurs sont survenues, veuillez examiner attentivement les logs de chaque étape pour diagnostiquer le problème. Assurez-vous que toutes les configurations (fichier `.env`, serveur Ollama, accès MongoDB) sont correctes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
