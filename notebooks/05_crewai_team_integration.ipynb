{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14b5a0d",
   "metadata": {},
   "source": [
    "# Notebook 05: Int√©gration et Test d'une √âquipe CrewAI (Document Deep Dive Crew)\n",
    "\n",
    "Ce notebook se concentre sur le test et la d√©monstration de notre √©quipe d'agents CrewAI sp√©cialis√©e dans l'analyse approfondie de documents : la `DocumentAnalysisCrew`. Nous allons instancier cette √©quipe en utilisant le module `src.agents.crewai_teams.document_analysis_crew`, lui soumettre un exemple de document et un focus de recherche, et examiner le rapport d'analyse qu'elle produit.\n",
    "\n",
    "L'√©quipe `DocumentAnalysisCrew` utilise en interne la fonction `get_llm` de `src/llm_services/llm_factory.py` pour instancier le mod√®le de langage en fonction de la configuration `DEFAULT_LLM_MODEL_PROVIDER` de votre projet.\n",
    "\n",
    "**Pr√©requis :**\n",
    "* **Environnement de Base :** Avoir ex√©cut√© le notebook `00_setup_environment.ipynb` pour configurer l'environnement Conda, les d√©pendances Python (y compris `crewai`, et d'autres comme `langchain-huggingface` si vous utilisez le fournisseur Hugging Face API), et s'assurer que le fichier `.env` √† la racine du projet est correctement rempli.\n",
    "* **Configuration du Fournisseur LLM (dans `.env`) :** La `DocumentAnalysisCrew` utilisera le fournisseur LLM sp√©cifi√© par `DEFAULT_LLM_MODEL_PROVIDER` (et les variables associ√©es) dans votre fichier `.env`.\n",
    "    * Si `DEFAULT_LLM_MODEL_PROVIDER` est `\"openai\"` : `OPENAI_API_KEY` et `DEFAULT_OPENAI_GENERATIVE_MODEL` sont requis.\n",
    "    * Si `DEFAULT_LLM_MODEL_PROVIDER` est `\"huggingface_api\"` : `HUGGINGFACE_API_KEY` et `HUGGINGFACE_REPO_ID` sont requis. Assurez-vous que `langchain-huggingface` et `transformers` sont bien install√©s.\n",
    "    * Si `DEFAULT_LLM_MODEL_PROVIDER` est `\"ollama\"` : Assurez-vous que `OLLAMA_BASE_URL` pointe vers votre instance Ollama et que `OLLAMA_GENERATIVE_MODEL_NAME` est un mod√®le que vous avez t√©l√©charg√© (`ollama pull ...`) et qui est servi par Ollama.\n",
    "* **(Optionnel) Base de Donn√©es pour Contexte G√©n√©ral :** Bien que ce notebook fournisse directement le contenu du document √† analyser √† la `DocumentAnalysisCrew`, il est utile de se rappeler que dans un sc√©nario d'utilisation complet, le contenu pourrait provenir d'une base de donn√©es peupl√©e via `01_data_ingestion_and_embedding.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6fa31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables d'environnement charg√©es depuis : .env\n",
      "\n",
      "\u001b[34m2025-06-03 10:16:18 - nb_05_crewai_integration - INFO - L'√©quipe CrewAI (via DocumentAnalysisCrew -> get_llm) tentera d'utiliser le fournisseur LLM configur√© par d√©faut : 'ollama'.\u001b[0m\n",
      "\u001b[34m2025-06-03 10:16:18 - nb_05_crewai_integration - INFO - Provider Ollama s√©lectionn√©. URL de base: http://localhost:11434, Mod√®le: mistral\u001b[0m\n",
      "\u001b[34m2025-06-03 10:16:18 - nb_05_crewai_integration - INFO - ASSUREZ-VOUS que le mod√®le 'mistral' est disponible sur votre serveur Ollama (via 'ollama pull mistral').\u001b[0m\n",
      "\u001b[34m2025-06-03 10:16:18 - nb_05_crewai_integration - INFO - Biblioth√®que CrewAI version 0.121.1 import√©e.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json # Pour un affichage lisible (bien que non utilis√© directement dans cette cellule)\n",
    "from typing import Optional, List \n",
    "\n",
    "# --- Activation du mode verbeux de LiteLLM et configuration de l'alias ---\n",
    "import litellm\n",
    "litellm.set_verbose = True # Pour obtenir des logs d√©taill√©s de LiteLLM\n",
    "\n",
    "project_root = Path()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# De m√™me, si CWD est la racine du projet, dotenv_path serait `Path().resolve() / \".env\"`\n",
    "dotenv_path = project_root / \".env\"\n",
    "if dotenv_path.exists():\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    print(f\"Variables d'environnement charg√©es depuis : {dotenv_path}\")\n",
    "else:\n",
    "    print(f\"ATTENTION: Fichier .env non trouv√© √† {dotenv_path}. Assurez-vous qu'il est √† la racine du projet.\")\n",
    "\n",
    "# Imports des modules du projet APRES la configuration potentielle du PYTHONPATH et de dotenv\n",
    "from config.settings import settings\n",
    "from config.logging_config import setup_logging\n",
    "from src.agents.crewai_teams.document_analysis_crew import run_document_deep_dive_crew\n",
    "\n",
    "# --- Configuration de l'alias de mod√®le LiteLLM (APRES que 'settings' soit charg√©) ---\n",
    "# Ceci aide LiteLLM √† mapper un nom de mod√®le simple (ex: \"mistral\") \n",
    "# vers le format pr√©fix√© qu'il attend pour Ollama (ex: \"ollama/mistral\"),\n",
    "# en particulier si CrewAI ne passe pas le contexte du fournisseur de mani√®re explicite √† LiteLLM.\n",
    "if hasattr(settings, 'DEFAULT_LLM_MODEL_PROVIDER') and settings.DEFAULT_LLM_MODEL_PROVIDER.lower() == \"ollama\":\n",
    "    if hasattr(settings, 'OLLAMA_GENERATIVE_MODEL_NAME') and settings.OLLAMA_GENERATIVE_MODEL_NAME:\n",
    "        configured_ollama_model_name = settings.OLLAMA_GENERATIVE_MODEL_NAME\n",
    "        # S'assurer que model_alias_map existe avant de l'utiliser\n",
    "        if not hasattr(litellm, 'model_alias_map') or litellm.model_alias_map is None:\n",
    "            litellm.model_alias_map = {} # Initialiser si ce n'est pas d√©j√† un dict\n",
    "            \n",
    "        litellm.model_alias_map[configured_ollama_model_name] = f\"ollama/{configured_ollama_model_name}\"\n",
    "        # On peut aussi logger, mais le logger n'est pas encore configur√©.\n",
    "        print(f\"INFO (pre-logging): LiteLLM model alias set: '{configured_ollama_model_name}' -> 'ollama/{configured_ollama_model_name}'\")\n",
    "    else:\n",
    "        print(\"WARNING (pre-logging): OLLAMA_GENERATIVE_MODEL_NAME non d√©fini, alias LiteLLM non appliqu√©.\")\n",
    "# --- Fin de la configuration de l'alias ---\n",
    "\n",
    "# Configurer le logging pour le notebook\n",
    "LOG_LEVEL_NOTEBOOK = \"INFO\" \n",
    "setup_logging(level=LOG_LEVEL_NOTEBOOK) \n",
    "logger = logging.getLogger(\"nb_05_crewai_integration\") # Le logger est maintenant disponible\n",
    "\n",
    "# --- V√©rification des pr√©requis pour le LLM utilis√© par CrewAI ---\n",
    "# Cette section logue les configurations pour informer l'utilisateur.\n",
    "# Les erreurs de configuration critiques sont lev√©es par get_llm() dans llm_factory.py.\n",
    "active_llm_provider_for_crew = settings.DEFAULT_LLM_MODEL_PROVIDER.lower()\n",
    "logger.info(f\"L'√©quipe CrewAI (via DocumentAnalysisCrew -> get_llm) tentera d'utiliser le fournisseur LLM configur√© par d√©faut : '{active_llm_provider_for_crew}'.\")\n",
    "\n",
    "config_ok = False\n",
    "if active_llm_provider_for_crew == \"openai\":\n",
    "    if settings.OPENAI_API_KEY and settings.DEFAULT_OPENAI_GENERATIVE_MODEL:\n",
    "        logger.info(f\"Provider OpenAI s√©lectionn√©. Cl√© API trouv√©e. Mod√®le: {settings.DEFAULT_OPENAI_GENERATIVE_MODEL}\")\n",
    "        config_ok = True\n",
    "    else:\n",
    "        logger.error(\"ERREUR : Le fournisseur LLM pour CrewAI est 'openai', mais OPENAI_API_KEY et/ou DEFAULT_OPENAI_GENERATIVE_MODEL ne sont pas (enti√®rement) configur√©s.\")\n",
    "        if not settings.OPENAI_API_KEY: print(\"ERREUR: Cl√© API OpenAI (OPENAI_API_KEY) manquante dans .env.\")\n",
    "        if not settings.DEFAULT_OPENAI_GENERATIVE_MODEL: print(\"ERREUR: Mod√®le OpenAI (DEFAULT_OPENAI_GENERATIVE_MODEL) manquant dans .env/settings.py.\")\n",
    "elif active_llm_provider_for_crew == \"huggingface_api\":\n",
    "    if settings.HUGGINGFACE_API_KEY and settings.HUGGINGFACE_REPO_ID:\n",
    "        logger.info(f\"Provider HuggingFace API s√©lectionn√©. Cl√© API trouv√©e. Repo ID: {settings.HUGGINGFACE_REPO_ID}\")\n",
    "        config_ok = True\n",
    "    else:\n",
    "        logger.error(\"ERREUR : Le fournisseur LLM pour CrewAI est 'huggingface_api', mais HUGGINGFACE_API_KEY et/ou HUGGINGFACE_REPO_ID ne sont pas (enti√®rement) configur√©s.\")\n",
    "        if not settings.HUGGINGFACE_API_KEY: print(\"ERREUR: Cl√© API HuggingFace (HUGGINGFACE_API_KEY) manquante dans .env.\")\n",
    "        if not settings.HUGGINGFACE_REPO_ID: print(\"ERREUR: Repo ID HuggingFace (HUGGINGFACE_REPO_ID) manquant dans .env/settings.py.\")\n",
    "elif active_llm_provider_for_crew == \"ollama\":\n",
    "    if settings.OLLAMA_BASE_URL and settings.OLLAMA_GENERATIVE_MODEL_NAME:\n",
    "        logger.info(f\"Provider Ollama s√©lectionn√©. URL de base: {settings.OLLAMA_BASE_URL}, Mod√®le: {settings.OLLAMA_GENERATIVE_MODEL_NAME}\")\n",
    "        logger.info(f\"ASSUREZ-VOUS que le mod√®le '{settings.OLLAMA_GENERATIVE_MODEL_NAME}' est disponible sur votre serveur Ollama (via 'ollama pull {settings.OLLAMA_GENERATIVE_MODEL_NAME}').\")\n",
    "        config_ok = True\n",
    "    else:\n",
    "        logger.error(\"ERREUR : Le fournisseur LLM pour CrewAI est 'ollama', mais OLLAMA_BASE_URL et/ou OLLAMA_GENERATIVE_MODEL_NAME ne sont pas (enti√®rement) configur√©s.\")\n",
    "        if not settings.OLLAMA_BASE_URL: print(\"ERREUR: URL de base Ollama (OLLAMA_BASE_URL) manquante dans .env/settings.py.\")\n",
    "        if not settings.OLLAMA_GENERATIVE_MODEL_NAME: print(\"ERREUR: Nom du mod√®le g√©n√©ratif Ollama (OLLAMA_GENERATIVE_MODEL_NAME) manquant dans .env/settings.py.\")\n",
    "else:\n",
    "    logger.error(f\"ERREUR : Fournisseur LLM '{active_llm_provider_for_crew}' non reconnu ou non support√© par les v√©rifications de ce notebook.\")\n",
    "    print(f\"ERREUR: Fournisseur LLM '{active_llm_provider_for_crew}' non support√© par ces v√©rifications. L'ex√©cution de CrewAI pourrait √©chouer.\")\n",
    "\n",
    "if not config_ok:\n",
    "    logger.warning(\"ATTENTION : La configuration pour le fournisseur LLM s√©lectionn√© semble incompl√®te. L'initialisation de l'LLM par CrewAI via get_llm() risque d'√©chouer.\")\n",
    "\n",
    "# V√©rifier si crewai est install√©\n",
    "try:\n",
    "    import crewai\n",
    "    logger.info(f\"Biblioth√®que CrewAI version {crewai.__version__} import√©e.\")\n",
    "except ImportError:\n",
    "    logger.error(\"La biblioth√®que 'crewai' n'est PAS install√©e. Veuillez l'installer (pip install crewai) et relancer.\")\n",
    "    crewai = None \n",
    "    print(\"ERREUR: Biblioth√®que 'crewai' non trouv√©e. Veuillez l'installer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82128400",
   "metadata": {},
   "source": [
    "### 1. Pr√©paration des Donn√©es d'Exemple pour l'Analyse\n",
    "\n",
    "Nous allons d√©finir un exemple de contenu de document, son ID, et un focus de recherche sp√©cifique pour tester notre `DocumentAnalysisCrew`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed249b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-03 10:16:18 - nb_05_crewai_integration - INFO - Pr√™t √† analyser le document ID: arxiv_sample_2401.001\u001b[0m\n",
      "\u001b[34m2025-06-03 10:16:18 - nb_05_crewai_integration - INFO - Focus de la recherche: Identify the core methodology, key results, and explicitly stated limitations regarding sim-to-real transfer techniques.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sample_doc_id = \"arxiv_sample_2401.001\"\n",
    "sample_research_focus = \"Identify the core methodology, key results, and explicitly stated limitations regarding sim-to-real transfer techniques.\"\n",
    "\n",
    "sample_doc_content = \"\"\"\n",
    "Abstract: This research presents \"TransferRL\", a novel framework for enhancing sim-to-real transfer in robotic manipulation tasks. \n",
    "By employing adaptive instance normalization in the feature space of a vision-based policy and a curriculum of progressively complex simulated environments, \n",
    "TransferRL significantly reduces the reality gap. We demonstrate its efficacy on a pick-and-place task, achieving an 82% success rate on a physical KUKA LBR iiwa robot after training purely in simulation.\n",
    "\n",
    "Introduction: Sim-to-real transfer is a pivotal challenge in applying deep reinforcement learning (RL) to real-world robotics. \n",
    "While training RL agents in simulation is often faster, safer, and more scalable than on physical systems, policies learned in simulation frequently\n",
    "fail to generalize to the real world due to discrepancies in dynamics, visual appearance, and sensor noise. \n",
    "This paper addresses these challenges with a focus on visual domain adaptation.\n",
    "\n",
    "Methodology: TransferRL consists of two main components. First, an image-to-image translation network based on CycleGAN is used to translate\n",
    "simulated images to a more realistic style, though this is only used for initial policy pre-training. The core of TransferRL is an\n",
    "encoder-decoder policy architecture where the encoder features are modulated by adaptive instance normalization (AdaIN) layers. \n",
    "These AdaIN layers are conditioned on a learned domain embedding, allowing the policy to adapt its visual processing dynamically.\n",
    "The agent is trained using Soft Actor-Critic (SAC) on a curriculum of tasks in a custom OpenAI Gym environment built with PyBullet. \n",
    "The curriculum gradually increases object diversity and complexity of grasp poses. Domain randomization is applied to lighting, textures, and camera positions.\n",
    "\n",
    "Results: On the primary pick-and-place task, TransferRL (82% success on physical robot) outperformed: \n",
    "(a) direct transfer without AdaIN or curriculum (35%), \n",
    "(b) AdaIN without curriculum (65%), and \n",
    "(c) curriculum without AdaIN (58%). \n",
    "The domain embedding was observed to cluster according to different simulation variations, suggesting successful domain identification. \n",
    "Qualitative analysis showed improved handling of novel object textures and lighting in the real world.\n",
    "\n",
    "Limitations: The current implementation of TransferRL requires a small set of real-world images to learn the target domain embedding for AdaIN, making it not entirely \"zero-shot\" sim-to-real. \n",
    "The computational overhead of the AdaIN layers adds a minor latency to the policy execution. \n",
    "The system has only been tested on quasi-static pick-and-place tasks.\n",
    "\n",
    "Conclusion: TransferRL offers an effective method for robust sim-to-real transfer in vision-based robotic manipulation by combining adaptive normalization with a structured training curriculum. Future work will explore fully zero-shot adaptation and application to dynamic tasks.\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f\"Pr√™t √† analyser le document ID: {sample_doc_id}\")\n",
    "logger.info(f\"Focus de la recherche: {sample_research_focus}\")\n",
    "# Un log pour la longueur du contenu peut √™tre utile aussi:\n",
    "# logger.info(f\"Longueur du contenu du document √©chantillon: {len(sample_doc_content)} caract√®res\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f211f",
   "metadata": {},
   "source": [
    "### 2. Ex√©cution de l'√âquipe CrewAI `DocumentAnalysisCrew`\n",
    "\n",
    "Nous appelons la fonction `run_document_deep_dive_crew` (d√©finie dans `src/agents/crewai_teams/document_analysis_crew.py`) avec nos donn√©es d'exemple. Cette fonction orchestre l'√©quipe CrewAI pour produire un rapport d'analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "041d9d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-03 10:16:18 - nb_05_crewai_integration - INFO - Tentative de lancement de DocumentAnalysisCrew...\u001b[0m\n",
      "\n",
      "üîÑ Lancement de l'√©quipe CrewAI pour l'analyse du document ID: arxiv_sample_2401.001...\n",
      "   Focus de la recherche: Identify the core methodology, key results, and explicitly stated limitations regarding sim-to-real transfer techniques.\n",
      "   Utilisation du fournisseur LLM: 'ollama' (Mod√®le: 'mistral')\n",
      "   Cela peut prendre du temps...\n",
      "\n",
      "\u001b[34m2025-06-03 10:16:18 - src.llm_services.llm_factory - INFO - Initializing LLM from llm_factory for provider: 'ollama' with temperature: 0.2\u001b[0m\n",
      "\u001b[34m2025-06-03 10:16:18 - src.llm_services.llm_factory - INFO - Using Ollama model (via langchain_ollama): mistral from http://localhost:11434\u001b[0m\n",
      "\u001b[34m2025-06-03 10:16:18 - src.agents.crewai_teams.document_analysis_crew - INFO - Starting Document Analysis Crew for doc ID: arxiv_sample_2401.001, focus: 'Identify the core methodology, key results, and explicitly stated limitations regarding sim-to-real transfer techniques.'\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:16:18 - LiteLLM:WARNING\u001b[0m: utils.py:511 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mExpert Information Extractor for Scientific Papers\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mAnalyze the following document (ID: arxiv_sample_2401.001) with a research focus on 'Identify the core methodology, key results, and explicitly stated limitations regarding sim-to-real transfer techniques.'. Extract key information: primary methodology, datasets used (if any), main quantitative results, and author-stated limitations. Present this as a structured list or key-value pairs. Document Content:\n",
      "\n",
      "---\n",
      "\n",
      "Abstract: This research presents \"TransferRL\", a novel framework for enhancing sim-to-real transfer in robotic manipulation tasks. \n",
      "By employing adaptive instance normalization in the feature space of a vision-based policy and a curriculum of progressively complex simulated environments, \n",
      "TransferRL significantly reduces the reality gap. We demonstrate its efficacy on a pick-and-place task, achieving an 82% success rate on a physical KUKA LBR iiwa robot after training purely in simulation.\n",
      "\n",
      "Introduction: Sim-to-real transfer is a pivotal challenge in applying deep reinforcement learning (RL) to real-world robotics. \n",
      "While training RL agents in simulation is often faster, safer, and more scalable than on physical systems, policies learned in simulation frequently\n",
      "fail to generalize to the real world due to discrepancies in dynamics, visual appearance, and sensor noise. \n",
      "This paper addresses these challenges with a focus on visual domain adaptation.\n",
      "\n",
      "Methodology: TransferRL consists of two main components. First, an image-to-image translation network based on CycleGAN is used to translate\n",
      "simulated images to a more realistic style, though this is only used for initial policy pre-training. The core of TransferRL is an\n",
      "encoder-decoder policy architecture where the encoder features are modulated by adaptive instance normalization (AdaIN) layers. \n",
      "These AdaIN layers are conditioned on a learned domain embedding, allowing the policy to adapt its visual processing dynamically.\n",
      "The agent is trained using Soft Actor-Critic (SAC) on a curriculum of tasks in a custom OpenAI Gym environment built with PyBullet. \n",
      "The curriculum gradually increases object diversity and complexity of grasp poses. Domain randomization is applied to lighting, textures, and camera positions.\n",
      "\n",
      "Results: On the primary pick-and-place task, TransferRL (82% success on physical robot) outperformed: \n",
      "(a) direct transfer without AdaIN or curriculum (35%), \n",
      "(b) AdaIN without curriculum (65%), and \n",
      "(c) curriculum without AdaIN (58%). \n",
      "The domain embedding was observed to cluster according to different simulation variations, suggesting successful domain identification. \n",
      "Qualitative analysis showed improved handling of novel object textures and lighting in the real world.\n",
      "\n",
      "Limitations: The current implementation of TransferRL requires a small set of real-world images to learn the target domain embedding for AdaIN, making it not entirely \"zero-shot\" sim-to-real. \n",
      "The computational overhead of the AdaIN layers adds a minor latency to the policy execution. \n",
      "The system has only been tested on quasi-static pick-and-place tasks.\n",
      "\n",
      "Conclusion: TransferRL offers an effective method for robust sim-to-real transfer in vision-based robotic manipulation by combining adaptive normalization with a structured training curriculum. Future work will explore fully zero-shot adaptation and application to dynamic tasks.\n",
      "\n",
      "---\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:16:18 - LiteLLM:INFO\u001b[0m: utils.py:2827 - \n",
      "LiteLLM completion() model= mistral; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Final returned optional params: {'temperature': 0.2, 'stream': False, 'stop': ['\\nObservation:']}\n",
      "\u001b[34m2025-06-03 10:16:26 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:16:26 - LiteLLM:INFO\u001b[0m: utils.py:1185 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:16:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama/mistral\n",
      "\u001b[92m10:16:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama/mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-03 10:16:26 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[34m2025-06-03 10:16:26 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mExpert Information Extractor for Scientific Papers\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "Methodology:\n",
      "- TransferRL consists of two main components: an image-to-image translation network based on CycleGAN for initial policy pre-training and an encoder-decoder policy architecture with adaptive instance normalization (AdaIN) layers conditioned on a learned domain embedding.\n",
      "- The agent is trained using Soft Actor-Critic (SAC) in a custom OpenAI Gym environment built with PyBullet, following a curriculum of tasks that gradually increases object diversity and complexity of grasp poses. Domain randomization is applied to lighting, textures, and camera positions.\n",
      "\n",
      "Datasets Used:\n",
      "- The paper does not explicitly state which datasets were used for the image-to-image translation network based on CycleGAN. However, it mentions that a small set of real-world images was required to learn the target domain embedding for AdaIN.\n",
      "\n",
      "Key Results:\n",
      "- On the primary pick-and-place task, TransferRL achieved an 82% success rate on a physical KUKA LBR iiwa robot after training purely in simulation. This outperformed direct transfer without AdaIN or curriculum (35%), AdaIN without curriculum (65%), and curriculum without AdaIN (58%).\n",
      "- The domain embedding was observed to cluster according to different simulation variations, suggesting successful domain identification. Qualitative analysis showed improved handling of novel object textures and lighting in the real world.\n",
      "\n",
      "Explicitly Stated Limitations:\n",
      "- The current implementation of TransferRL requires a small set of real-world images to learn the target domain embedding for AdaIN, making it not entirely \"zero-shot\" sim-to-real.\n",
      "- The computational overhead of the AdaIN layers adds a minor latency to the policy execution.\n",
      "- The system has only been tested on quasi-static pick-and-place tasks.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[34m2025-06-03 10:16:26 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:16:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama/mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-03 10:16:26 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[34m2025-06-03 10:16:26 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:16:26 - LiteLLM:WARNING\u001b[0m: utils.py:511 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mScientific Section Summarizer\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mBased on the document (ID: arxiv_sample_2401.001) and focusing on 'Identify the core methodology, key results, and explicitly stated limitations regarding sim-to-real transfer techniques.', provide concise summaries for its main logical sections (e.g., Abstract, Introduction, Methods, Results, Conclusion). If sections are not clearly delineated, provide a general summary. Document Content:\n",
      "\n",
      "---\n",
      "\n",
      "Abstract: This research presents \"TransferRL\", a novel framework for enhancing sim-to-real transfer in robotic manipulation tasks. \n",
      "By employing adaptive instance normalization in the feature space of a vision-based policy and a curriculum of progressively complex simulated environments, \n",
      "TransferRL significantly reduces the reality gap. We demonstrate its efficacy on a pick-and-place task, achieving an 82% success rate on a physical KUKA LBR iiwa robot after training purely in simulation.\n",
      "\n",
      "Introduction: Sim-to-real transfer is a pivotal challenge in applying deep reinforcement learning (RL) to real-world robotics. \n",
      "While training RL agents in simulation is often faster, safer, and more scalable than on physical systems, policies learned in simulation frequently\n",
      "fail to generalize to the real world due to discrepancies in dynamics, visual appearance, and sensor noise. \n",
      "This paper addresses these challenges with a focus on visual domain adaptation.\n",
      "\n",
      "Methodology: TransferRL consists of two main components. First, an image-to-image translation network based on CycleGAN is used to translate\n",
      "simulated images to a more realistic style, though this is only used for initial policy pre-training. The core of TransferRL is an\n",
      "encoder-decoder policy architecture where the encoder features are modulated by adaptive instance normalization (AdaIN) layers. \n",
      "These AdaIN layers are conditioned on a learned domain embedding, allowing the policy to adapt its visual processing dynamically.\n",
      "The agent is trained using Soft Actor-Critic (SAC) on a curriculum of tasks in a custom OpenAI Gym environment built with PyBullet. \n",
      "The curriculum gradually increases object diversity and complexity of grasp poses. Domain randomization is applied to lighting, textures, and camera positions.\n",
      "\n",
      "Results: On the primary pick-and-place task, TransferRL (82% success on physical robot) outperformed: \n",
      "(a) direct transfer without AdaIN or curriculum (35%), \n",
      "(b) AdaIN without curriculum (65%), and \n",
      "(c) curriculum without AdaIN (58%). \n",
      "The domain embedding was observed to cluster according to different simulation variations, suggesting successful domain identification. \n",
      "Qualitative analysis showed improved handling of novel object textures and lighting in the real world.\n",
      "\n",
      "Limitations: The current implementation of TransferRL requires a small set of real-world images to learn the target domain embedding for AdaIN, making it not entirely \"zero-shot\" sim-to-real. \n",
      "The computational overhead of the AdaIN layers adds a minor latency to the policy execution. \n",
      "The system has only been tested on quasi-static pick-and-place tasks.\n",
      "\n",
      "Conclusion: TransferRL offers an effective method for robust sim-to-real transfer in vision-based robotic manipulation by combining adaptive normalization with a structured training curriculum. Future work will explore fully zero-shot adaptation and application to dynamic tasks.\n",
      "\n",
      "---\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:16:26 - LiteLLM:INFO\u001b[0m: utils.py:2827 - \n",
      "LiteLLM completion() model= mistral; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Final returned optional params: {'temperature': 0.2, 'stream': False, 'stop': ['\\nObservation:']}\n",
      "\u001b[34m2025-06-03 10:16:34 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:16:34 - LiteLLM:INFO\u001b[0m: utils.py:1185 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:16:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama/mistral\n",
      "\u001b[92m10:16:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama/mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-03 10:16:34 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[34m2025-06-03 10:16:34 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mScientific Section Summarizer\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "Methodology: The proposed method, TransferRL, consists of two main components. An image-to-image translation network based on CycleGAN is used for initial policy pre-training. The core of TransferRL is an encoder-decoder policy architecture with adaptive instance normalization (AdaIN) layers conditioned on a learned domain embedding. The agent is trained using Soft Actor-Critic (SAC) in a custom OpenAI Gym environment built with PyBullet, following a curriculum of tasks that gradually increases object diversity and complexity of grasp poses. Domain randomization is applied to lighting, textures, and camera positions.\n",
      "\n",
      "Datasets Used: The paper does not explicitly state which datasets were used for the image-to-image translation network based on CycleGAN. However, it mentions that a small set of real-world images was required to learn the target domain embedding for AdaIN.\n",
      "\n",
      "Key Results: On the primary pick-and-place task, TransferRL achieved an 82% success rate on a physical KUKA LBR iiwa robot after training purely in simulation. This outperformed direct transfer without AdaIN or curriculum (35%), AdaIN without curriculum (65%), and curriculum without AdaIN (58%). The domain embedding was observed to cluster according to different simulation variations, suggesting successful domain identification. Qualitative analysis showed improved handling of novel object textures and lighting in the real world.\n",
      "\n",
      "Explicitly Stated Limitations: The current implementation of TransferRL requires a small set of real-world images to learn the target domain embedding for AdaIN, making it not entirely \"zero-shot\" sim-to-real. The computational overhead of the AdaIN layers adds a minor latency to the policy execution. The system has only been tested on quasi-static pick-and-place tasks.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[34m2025-06-03 10:16:34 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:16:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama/mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-03 10:16:34 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[34m2025-06-03 10:16:34 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:16:34 - LiteLLM:WARNING\u001b[0m: utils.py:511 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mCritical Analyst of Scientific Research\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mPerform a critical analysis of the document (ID: arxiv_sample_2401.001) focusing on 'Identify the core methodology, key results, and explicitly stated limitations regarding sim-to-real transfer techniques.'. Use the extracted information and section summaries (if available from previous tasks) to identify strengths, weaknesses, and novel contributions. Document Content (for reference, primary input should be outputs of previous tasks if available):\n",
      "\n",
      "---\n",
      "\n",
      "Abstract: This research presents \"TransferRL\", a novel framework for enhancing sim-to-real transfer in robotic manipulation tasks. \n",
      "By employing adaptive instance normalization in the feature space of a vision-based policy and a curriculum of progressively complex simulated environments, \n",
      "TransferRL significantly reduces the reality gap. We demonstrate its efficacy on a pick-and-place task, achieving an 82% success rate on a physical KUKA LBR iiwa robot after training purely in simulation.\n",
      "\n",
      "Introduction: Sim-to-real transfer is a pivotal challenge in applying deep reinforcement learning (RL) to real-world robotics. \n",
      "While training RL agents in simulation is often faster, safer, and more scalable than on physical systems, policies learned in simulation frequently\n",
      "fail to generalize to the real world due to discrepancies in dynamics, visual appearance, and sensor noise. \n",
      "This paper addresses these challenges with a focus on visual domain adaptation.\n",
      "\n",
      "Methodology: TransferRL consists of two main components. First, an image-to-image translation network based on CycleGAN is used to translate\n",
      "simulated images to a more realistic style, though this is only used for initial policy pre-training. The core of TransferRL is an\n",
      "encoder-decoder policy architecture where the encoder features are modulated by adaptive instance normalization (AdaIN) layers. \n",
      "These AdaIN layers are conditioned on a learned domain embedding, allowing the policy to adapt its visual processing dynamically.\n",
      "The agent is trained using Soft Actor-Critic (SAC) on a curriculum of tasks in a custom OpenAI Gym environment built with PyBullet. \n",
      "The curriculum gradually increases object diversity and complexity of grasp poses. Domain randomization is applied to lighting, textures, and camera positions.\n",
      "\n",
      "Results: On the primary pick-and-place task, TransferRL (82% success on physical robot) outperformed: \n",
      "(a) direct transfer without AdaIN or curriculum (35%), \n",
      "(b) AdaIN without curriculum (65%), and \n",
      "(c) curriculum without AdaIN (58%). \n",
      "The domain embedding was observed to cluster according to different simulation variations, suggesting successful domain identification. \n",
      "Qualitative analysis showed improved handling of novel object textures and lighting in the real world.\n",
      "\n",
      "Limitations: The current implementation of TransferRL requires a small set of real-world images to learn the target domain embedding for AdaIN, making it not entirely \"zero-shot\" sim-to-real. \n",
      "The computational overhead of the AdaIN layers adds a minor latency to the policy execution. \n",
      "The system has only been tested on quasi-static pick-and-place tasks.\n",
      "\n",
      "Conclusion: TransferRL offers an effective method for robust sim-to-real transfer in vision-based robotic manipulation by combining adaptive normalization with a structured training curriculum. Future work will explore fully zero-shot adaptation and application to dynamic tasks.\n",
      "\n",
      "---\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:16:34 - LiteLLM:INFO\u001b[0m: utils.py:2827 - \n",
      "LiteLLM completion() model= mistral; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Final returned optional params: {'temperature': 0.2, 'stream': False, 'stop': ['\\nObservation:']}\n",
      "\u001b[34m2025-06-03 10:16:44 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:16:44 - LiteLLM:INFO\u001b[0m: utils.py:1185 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:16:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama/mistral\n",
      "\u001b[92m10:16:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama/mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-03 10:16:44 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[34m2025-06-03 10:16:44 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mCritical Analyst of Scientific Research\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "The paper presents \"TransferRL\", a novel framework for enhancing sim-to-real transfer in robotic manipulation tasks. The methodology involves two main components: an image-to-image translation network based on CycleGAN for initial policy pre-training and an encoder-decoder policy architecture with adaptive instance normalization (AdaIN) layers conditioned on a learned domain embedding. The agent is trained using Soft Actor-Critic (SAC) in a custom OpenAI Gym environment built with PyBullet, following a curriculum of tasks that gradually increases object diversity and complexity of grasp poses. Domain randomization is applied to lighting, textures, and camera positions.\n",
      "\n",
      "The key results demonstrate the effectiveness of TransferRL, as it achieved an 82% success rate on a physical KUKA LBR iiwa robot after training purely in simulation, outperforming various control strategies without AdaIN or curriculum. The domain embedding was observed to cluster according to different simulation variations, suggesting successful domain identification. Qualitative analysis showed improved handling of novel object textures and lighting in the real world.\n",
      "\n",
      "However, there are some explicitly stated limitations. First, the current implementation of TransferRL requires a small set of real-world images to learn the target domain embedding for AdaIN, making it not entirely \"zero-shot\" sim-to-real. Second, the computational overhead of the AdaIN layers adds a minor latency to the policy execution. Lastly, the system has only been tested on quasi-static pick-and-place tasks.\n",
      "\n",
      "In terms of novel contributions, TransferRL combines adaptive normalization with a structured training curriculum to offer an effective method for robust sim-to-real transfer in vision-based robotic manipulation. Future work will explore fully zero-shot adaptation and application to dynamic tasks. The paper's focus on visual domain adaptation and the use of AdaIN layers to dynamically adapt the policy to different environments are notable contributions to the field.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[34m2025-06-03 10:16:44 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:16:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama/mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-03 10:16:44 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[34m2025-06-03 10:16:44 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:16:44 - LiteLLM:WARNING\u001b[0m: utils.py:511 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mLead Research Report Compiler\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mCompile a final, structured analytical report for document ID: arxiv_sample_2401.001 with a focus on 'Identify the core methodology, key results, and explicitly stated limitations regarding sim-to-real transfer techniques.'. Integrate the outputs from the Information Extractor, Section Summarizer, and Critical Analyst. The report should be well-organized and directly address the research focus.\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:16:44 - LiteLLM:INFO\u001b[0m: utils.py:2827 - \n",
      "LiteLLM completion() model= mistral; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Final returned optional params: {'temperature': 0.2, 'stream': False, 'stop': ['\\nObservation:']}\n",
      "\u001b[34m2025-06-03 10:17:26 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:17:26 - LiteLLM:INFO\u001b[0m: utils.py:1185 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama/mistral\n",
      "\u001b[92m10:17:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama/mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-03 10:17:26 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[34m2025-06-03 10:17:26 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mLead Research Report Compiler\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "The comprehensive analytical report for document arxiv_sample_2401.001 covering 1. Extracted Key Information (Methodology, Results, Limitations). 2. Section Summaries. 3. Critical Analysis (Strengths, Weaknesses, Contributions) is as follows:\n",
      "\n",
      "   The paper presents \"TransferRL\", a novel framework for enhancing sim-to-real transfer in robotic manipulation tasks. The methodology involves two main components: an image-to-image translation network based on CycleGAN for initial policy pre-training and an encoder-decoder policy architecture with adaptive instance normalization (AdaIN) layers conditioned on a learned domain embedding. The agent is trained using Soft Actor-Critic (SAC) in a custom OpenAI Gym environment built with PyBullet, following a curriculum of tasks that gradually increases object diversity and complexity of grasp poses. Domain randomization is applied to lighting, textures, and camera positions.\n",
      "\n",
      "   The key results demonstrate the effectiveness of TransferRL, as it achieved an 82% success rate on a physical KUKA LBR iiwa robot after training purely in simulation, outperforming various control strategies without AdaIN or curriculum. The domain embedding was observed to cluster according to different simulation variations, suggesting successful domain identification. Qualitative analysis showed improved handling of novel object textures and lighting in the real world.\n",
      "\n",
      "   However, there are some explicitly stated limitations. First, the current implementation of TransferRL requires a small set of real-world images to learn the target domain embedding for AdaIN, making it not entirely \"zero-shot\" sim-to-real. Second, the computational overhead of the AdaIN layers adds a minor latency to the policy execution. Lastly, the system has only been tested on quasi-static pick-and-place tasks.\n",
      "\n",
      "   In terms of novel contributions, TransferRL combines adaptive normalization with a structured training curriculum to offer an effective method for robust sim-to-real transfer in vision-based robotic manipulation. Future work will explore fully zero-shot adaptation and application to dynamic tasks. The paper's focus on visual domain adaptation and the use of AdaIN layers to dynamically adapt the policy to different environments are notable contributions to the field.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[34m2025-06-03 10:17:26 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:17:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: ollama/mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-03 10:17:26 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[34m2025-06-03 10:17:26 - src.agents.crewai_teams.document_analysis_crew - INFO - Document Analysis Crew finished for doc ID: arxiv_sample_2401.001. Result length: 2303\u001b[0m\n",
      "\n",
      "\n",
      "--- üìù Rapport d'Analyse Final Compil√© par l'√âquipe CrewAI ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The comprehensive analytical report for document arxiv_sample_2401.001 covering 1. Extracted Key Information (Methodology, Results, Limitations). 2. Section Summaries. 3. Critical Analysis (Strengths, Weaknesses, Contributions) is as follows:\n",
       "\n",
       "   The paper presents \"TransferRL\", a novel framework for enhancing sim-to-real transfer in robotic manipulation tasks. The methodology involves two main components: an image-to-image translation network based on CycleGAN for initial policy pre-training and an encoder-decoder policy architecture with adaptive instance normalization (AdaIN) layers conditioned on a learned domain embedding. The agent is trained using Soft Actor-Critic (SAC) in a custom OpenAI Gym environment built with PyBullet, following a curriculum of tasks that gradually increases object diversity and complexity of grasp poses. Domain randomization is applied to lighting, textures, and camera positions.\n",
       "\n",
       "   The key results demonstrate the effectiveness of TransferRL, as it achieved an 82% success rate on a physical KUKA LBR iiwa robot after training purely in simulation, outperforming various control strategies without AdaIN or curriculum. The domain embedding was observed to cluster according to different simulation variations, suggesting successful domain identification. Qualitative analysis showed improved handling of novel object textures and lighting in the real world.\n",
       "\n",
       "   However, there are some explicitly stated limitations. First, the current implementation of TransferRL requires a small set of real-world images to learn the target domain embedding for AdaIN, making it not entirely \"zero-shot\" sim-to-real. Second, the computational overhead of the AdaIN layers adds a minor latency to the policy execution. Lastly, the system has only been tested on quasi-static pick-and-place tasks.\n",
       "\n",
       "   In terms of novel contributions, TransferRL combines adaptive normalization with a structured training curriculum to offer an effective method for robust sim-to-real transfer in vision-based robotic manipulation. Future work will explore fully zero-shot adaptation and application to dynamic tasks. The paper's focus on visual domain adaptation and the use of AdaIN layers to dynamically adapt the policy to different environments are notable contributions to the field."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-03 10:17:26 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "final_report_from_crew = None\n",
    "\n",
    "# La condition principale est maintenant de v√©rifier si la biblioth√®que 'crewai' est disponible.\n",
    "# Les configurations sp√©cifiques au LLM (OpenAI, HuggingFace API, Ollama) sont g√©r√©es par:\n",
    "# 1. Les messages d'erreur/logs dans la premi√®re cellule de code de ce notebook (ID '1a6fa31a').\n",
    "# 2. La fonction get_llm() (dans src/llm_services/llm_factory.py) qui l√®ve une ValueError si la configuration est incorrecte.\n",
    "# 3. Le bloc try...except ci-dessous qui attrape ces erreurs lors de l'ex√©cution.\n",
    "\n",
    "if crewai: # 'crewai' est l'objet module import√© dans la premi√®re cellule (ID '1a6fa31a').\n",
    "    logger.info(\"Tentative de lancement de DocumentAnalysisCrew...\")\n",
    "    # Les variables sample_doc_id, sample_doc_content, sample_research_focus \n",
    "    # sont d√©finies dans la cellule pr√©c√©dente (ID '9ed249b5').\n",
    "    print(f\"\\nüîÑ Lancement de l'√©quipe CrewAI pour l'analyse du document ID: {sample_doc_id}...\")\n",
    "    print(f\"   Focus de la recherche: {sample_research_focus}\")\n",
    "    print(f\"   Utilisation du fournisseur LLM: '{settings.DEFAULT_LLM_MODEL_PROVIDER}' (Mod√®le: '{settings.OLLAMA_GENERATIVE_MODEL_NAME if settings.DEFAULT_LLM_MODEL_PROVIDER == 'ollama' else (settings.HUGGINGFACE_REPO_ID if settings.DEFAULT_LLM_MODEL_PROVIDER == 'huggingface_api' else settings.DEFAULT_OPENAI_GENERATIVE_MODEL)}')\")\n",
    "    print(\"   Cela peut prendre du temps...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Appel √† la fonction qui ex√©cute l'√©quipe CrewAI.\n",
    "        # Cette fonction utilise DocumentAnalysisCrew, qui utilise get_llm()\n",
    "        # pour instancier le LLM selon la configuration du projet.\n",
    "        final_report_from_crew = run_document_deep_dive_crew(\n",
    "            document_id=sample_doc_id,\n",
    "            document_content=sample_doc_content,\n",
    "            research_focus=sample_research_focus\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\\n--- üìù Rapport d'Analyse Final Compil√© par l'√âquipe CrewAI ---\")\n",
    "        if final_report_from_crew:\n",
    "            # V√©rifier si le rapport est un message d'erreur g√©r√© par DocumentAnalysisCrew.run()\n",
    "            if isinstance(final_report_from_crew, str) and final_report_from_crew.lower().startswith(\"error:\"):\n",
    "                 print(f\"L'√©quipe CrewAI a retourn√© une erreur : {final_report_from_crew}\")\n",
    "            else:\n",
    "                # Tenter d'afficher en Markdown si possible, sinon en texte brut.\n",
    "                try:\n",
    "                    from IPython.display import display, Markdown\n",
    "                    # S'assurer que le contenu est une cha√Æne avant de le passer √† Markdown\n",
    "                    display(Markdown(str(final_report_from_crew))) \n",
    "                except ImportError:\n",
    "                    print(str(final_report_from_crew)) \n",
    "        else:\n",
    "            print(\"L'√©quipe CrewAI n'a pas retourn√© de rapport explicite (v√©rifiez les logs pour des erreurs).\")\n",
    "\n",
    "    except ValueError as ve: # Attrape les erreurs de configuration de get_llm (ex: cl√© API manquante pour le provider choisi)\n",
    "        logger.error(f\"Erreur de configuration LLM lors de l'ex√©cution de DocumentAnalysisCrew: {ve}\", exc_info=True)\n",
    "        print(f\"ERREUR DE CONFIGURATION LLM (via get_llm) : {ve}\")\n",
    "        print(f\"Veuillez v√©rifier la configuration pour le provider '{settings.DEFAULT_LLM_MODEL_PROVIDER}' dans votre .env.\")\n",
    "    except Exception as e: # Capture les autres erreurs pendant l'ex√©cution de la Crew.\n",
    "        logger.error(f\"Erreur lors de l'ex√©cution de DocumentAnalysisCrew dans le notebook: {e}\", exc_info=True)\n",
    "        print(f\"ERREUR lors de l'ex√©cution de l'√©quipe CrewAI : {e}\")\n",
    "else:\n",
    "    # Ce bloc est atteint si l'import de 'crewai' a √©chou√© dans la premi√®re cellule de code.\n",
    "    print(\"Biblioth√®que CrewAI non install√©e ou import √©chou√©. Test de l'√©quipe CrewAI saut√©.\")\n",
    "    logger.warning(\"Skipping CrewAI team test because 'crewai' library is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68121d3",
   "metadata": {},
   "source": [
    "### 3. Analyse du Rapport et Discussion\n",
    "\n",
    "* Examinez attentivement le rapport g√©n√©r√© par l'√©quipe CrewAI.\n",
    "* **Structure :** Le `ReportCompilerAgent` a-t-il bien structur√© le rapport en fonction des contributions des autres agents (Extracteur, R√©sumeur, Analyste Critique) ?\n",
    "* **Contenu :**\n",
    "  * Les informations cl√©s extraites par l'`InfoExtractorAgent` (m√©thodologie, r√©sultats, limitations) sont-elles pr√©cises et pertinentes par rapport au `research_focus` ?\n",
    "  * Les r√©sum√©s de sections (si l'`SectionSummarizerAgent` a pu les produire) sont-ils concis et informatifs ?\n",
    "  * L'analyse critique du `CriticalAnalystAgent` (forces, faiblesses, contributions) est-elle judicieuse ?\n",
    "* **Focus de Recherche :** Le rapport final est-il bien ax√© sur le `research_focus` que nous avons sp√©cifi√© ?\n",
    "* **Verbosit√© de CrewAI :**\n",
    "    * Le param√®tre `verbose` pour l'objet `Crew` principal dans `src/agents/crewai_teams/document_analysis_crew.py` est actuellement r√©gl√© sur `False` pour √©viter les probl√®mes d'affichage des codes de couleur ANSI dans certains terminaux. Si vous souhaitez observer le d√©roulement d√©taill√© des t√¢ches de la `Crew`, vous pouvez le r√©gler temporairement sur `True`.\n",
    "    * Les agents individuels (`InfoExtractorAgent`, `SectionSummarizerAgent`, etc.) sont toujours configur√©s avec `verbose=True`. Si la verbosit√© de la `Crew` est √©galement sur `True`, vous verrez le d√©tail de leurs \"pens√©es\" et actions, ce qui est utile pour comprendre leur processus de collaboration et d√©boguer.\n",
    "* **Fournisseur LLM :** Le rapport a √©t√© g√©n√©r√© en utilisant le fournisseur LLM configur√© par `DEFAULT_LLM_MODEL_PROVIDER` (par exemple, Ollama avec \"mistral\"). La qualit√© et le style du rapport peuvent varier en fonction du mod√®le utilis√©.\n",
    "\n",
    "Cette ex√©cution isol√©e nous donne une id√©e de la capacit√© de l'√©quipe CrewAI √† traiter une t√¢che d'analyse de document complexe et sp√©cialis√©e avec le LLM configur√©."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104bb9de",
   "metadata": {},
   "source": [
    "### 4. Int√©gration Actuelle dans le Workflow LangGraph\n",
    "\n",
    "Cette √©quipe CrewAI est int√©gr√©e comme un outil sp√©cialis√© au sein de notre syst√®me LangGraph principal. Voici comment cela fonctionne :\n",
    "\n",
    "1.  **La CrewAI comme Outil LangChain (`document_deep_dive_analysis_tool`) :**\n",
    "    La fonction `run_document_deep_dive_crew` (qui ex√©cute l'√©quipe d'analyse de document) est encapsul√©e en tant qu'outil LangChain personnalis√© √† l'aide du d√©corateur `@tool`. Cet outil, nomm√© `document_deep_dive_analysis_tool`, est d√©fini dans `src/agents/tool_definitions.py`.\n",
    "\n",
    "    Voici un aper√ßu de sa structure (le code complet est dans `tool_definitions.py`) :\n",
    "    ```python\n",
    "    # Extrait de src/agents/tool_definitions.py :\n",
    "    from langchain_core.tools import tool\n",
    "    from src.agents.crewai_teams.document_analysis_crew import run_document_deep_dive_crew\n",
    "\n",
    "    @tool\n",
    "    def document_deep_dive_analysis_tool(\n",
    "        document_id: str, \n",
    "        document_content: str, \n",
    "        research_focus: str\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Performs an in-depth analysis of a single scientific document's text content \n",
    "        using a specialized team of AI agents (CrewAI).\n",
    "        Use this tool when a detailed, structured report focusing on specific aspects \n",
    "        of a document is required.\n",
    "        # Args, Returns...\n",
    "        \"\"\"\n",
    "        return run_document_deep_dive_crew(document_id, document_content, research_focus)\n",
    "    ```\n",
    "\n",
    "2.  **Utilisation par l'Agent LangGraph `DocumentAnalysisAgent` :**\n",
    "    L'agent `DocumentAnalysisAgent`, d√©fini dans `src/agents/agent_architectures.py`, est maintenant √©quip√© de cet outil `document_deep_dive_analysis_tool` (en plus de son outil `knowledge_base_retrieval_tool`).\n",
    "    Son prompt syst√®me (la constante `DOCUMENT_ANALYSIS_SYSTEM_PROMPT_V2` dans `agent_architectures.py`) a √©t√© mis √† jour pour le guider sur les situations o√π il est appropri√© de d√©l√©guer une analyse approfondie √† cette √©quipe CrewAI sp√©cialis√©e. Typiquement, cela se produit lorsqu'un rapport tr√®s structur√© et d√©taill√© sur un document unique est demand√©, avec un focus de recherche pr√©cis.\n",
    "\n",
    "3.  **Flux d'Ex√©cution dans LangGraph :**\n",
    "    * Le `ResearchPlannerAgent` peut, dans son plan de recherche, identifier un document cl√© qui n√©cessite une analyse pouss√©e et sp√©cifier un focus pour cette analyse.\n",
    "    * Le routeur dans LangGraph dirige ensuite la t√¢che vers le `DocumentAnalysisAgent`.\n",
    "    * Le `DocumentAnalysisAgent`, en se basant sur le plan ou la requ√™te, peut d√©terminer qu'une analyse en profondeur est n√©cessaire. Avant d'appeler `document_deep_dive_analysis_tool`, il pourrait avoir besoin de r√©cup√©rer le contenu complet du document sp√©cifi√© (par exemple, en utilisant `knowledge_base_retrieval_tool` pour agr√©ger tous les chunks d'un `arxiv_id` particulier).\n",
    "    * Une fois le contenu et le focus pr√™ts, il appelle `document_deep_dive_analysis_tool`.\n",
    "    * Le `ToolNode` de LangGraph ex√©cute cet outil, ce qui lance l'ensemble de l'√©quipe CrewAI.\n",
    "    * Le rapport final produit par l'√©quipe CrewAI est retourn√© au `DocumentAnalysisAgent` sous forme d'un `ToolMessage` dans l'√©tat du graphe LangGraph.\n",
    "    * Le `DocumentAnalysisAgent` peut alors utiliser ce rapport d√©taill√© pour formuler sa propre r√©ponse, la r√©sumer, ou la transmettre (potentiellement avec d'autres informations) au `SynthesisAgent` pour l'int√©gration dans le rapport final.\n",
    "\n",
    "Cette approche hybride permet de combiner la flexibilit√© et la gestion d'√©tat robuste de LangGraph pour l'orchestration g√©n√©rale du \"MAKERS\" avec la puissance de la collaboration structur√©e et orient√©e r√¥les de CrewAI pour des t√¢ches d'analyse de documents tr√®s sp√©cifiques et complexes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d1a26",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook a d√©montr√© comment instancier et ex√©cuter notre `DocumentAnalysisCrew` pour effectuer une analyse approfondie d'un document. Nous avons √©galement discut√© des pistes pour int√©grer cette √©quipe CrewAI comme un composant sp√©cialis√© au sein de notre architecture LangGraph principale.\n",
    "\n",
    "L'utilisation de CrewAI pour des sous-t√¢ches complexes peut enrichir les capacit√©s de notre \"MAKERS\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
