# makers/.env.example
# Copiez ce fichier en .env et remplissez les valeurs nécessaires.
# Ne committez jamais votre fichier .env rempli contenant des secrets.

# --- Clés API pour les LLMs ---
# Obligatoire si DEFAULT_LLM_MODEL_PROVIDER="openai" ou si OpenAI est utilisé spécifiquement
OPENAI_API_KEY="sk-..." 

# Optionnel, si vous prévoyez d'utiliser ces fournisseurs pour les LLMs génératifs
ANTHROPIC_API_KEY="sk-ant-..." 
GROQ_API_KEY="gsk_..."        
# GOOGLE_API_KEY="AIza..." # Si vous ajoutez un support pour les modèles Google

# --- Configuration pour Hugging Face (API d'Inférence pour LLMs génératifs) ---
# Requis si DEFAULT_LLM_MODEL_PROVIDER="huggingface_api"
HUGGINGFACE_API_KEY="hf_..." # Votre clé API Hugging Face pour l'API d'inférence
# Exemple : HUGGINGFACE_REPO_ID="mistralai/Mixtral-8x7B-Instruct-v0.1"
# Exemple : HUGGINGFACE_REPO_ID="NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO"
HUGGINGFACE_REPO_ID="mistralai/Mixtral-8x7B-Instruct-v0.1" # L'identifiant du modèle génératif sur Hugging Face Hub

# --- Configuration pour Ollama (si vous servez des modèles localement) ---
# Requis si DEFAULT_LLM_MODEL_PROVIDER="ollama" ou DEFAULT_EMBEDDING_PROVIDER="ollama"
OLLAMA_BASE_URL="http://localhost:11434" # URL de base de votre serveur Ollama

# Modèle génératif servi par Ollama (si DEFAULT_LLM_MODEL_PROVIDER="ollama")
# Exemple : OLLAMA_GENERATIVE_MODEL_NAME="mistral"
# Exemple : OLLAMA_GENERATIVE_MODEL_NAME="llama3"
OLLAMA_GENERATIVE_MODEL_NAME="mistral" 

# --- Clés API pour les Outils/Services ---
# Optionnel, pour l'outil de recherche Tavily
TAVILY_API_KEY="tvly-..."

# Obligatoire si vous utilisez le logging Weights & Biases
WANDB_API_KEY="..."
# Vous pouvez aussi configurer WANDB_PROJECT et WANDB_ENTITY ici ou via le logger/CLI
# WANDB_PROJECT="MAKERS-Experiments"
# WANDB_ENTITY="votre_entite_wandb"

# --- Configuration MongoDB ---
# Requis : Votre chaîne de connexion MongoDB Atlas ou locale
MONGODB_URI="mongodb+srv://<user>:<password>@<cluster_url>/<db_name>?retryWrites=true&w=majority"
# Ou pour une instance locale : MONGODB_URI="mongodb://localhost:27017/"

# Optionnel : Surcharge le nom de la base de données défini dans settings.py
# MONGO_DATABASE_NAME="custom_makers_db"

# Optionnel : Surcharge le nom de la collection pour les checkpoints LangGraph
# LANGGRAPH_CHECKPOINTS_COLLECTION="custom_langgraph_checkpoints"

# --- Configuration Générale de l'Application ---
# Optionnel : "development" ou "production". Affecte le logging coloré par défaut.
# PYTHON_ENV="development"

# Optionnel : Mettre à "True" pour un logging plus verbeux et d'autres comportements de débogage
# DEBUG="False"

# --- Configuration des Modèles Génératifs par Défaut (Optionnel - surchargent settings.py) ---
# Options pour DEFAULT_LLM_MODEL_PROVIDER: "openai", "huggingface_api", "ollama"
# La valeur par défaut dans settings.py est "huggingface_api".
# DEFAULT_LLM_MODEL_PROVIDER="huggingface_api" 
DEFAULT_OPENAI_GENERATIVE_MODEL="gpt-4o" # Modèle OpenAI pour la génération (si provider="openai")
# HUGGINGFACE_REPO_ID est utilisé si provider="huggingface_api" (voir plus haut)
# OLLAMA_GENERATIVE_MODEL_NAME est utilisé si provider="ollama" (voir plus haut)


# --- Configuration des Modèles d'Embedding par Défaut (Optionnel - surchargent settings.py) ---
# Options pour DEFAULT_EMBEDDING_PROVIDER: "openai", "huggingface", "ollama"
# La valeur par défaut dans settings.py est "huggingface".
# DEFAULT_EMBEDDING_PROVIDER="huggingface"

# Si DEFAULT_EMBEDDING_PROVIDER="openai" (ou si vous voulez surcharger les valeurs de settings.py) :
# OPENAI_EMBEDDING_MODEL_NAME="text-embedding-3-small"
# OPENAI_EMBEDDING_DIMENSION="1536"

# Si DEFAULT_EMBEDDING_PROVIDER="huggingface" (ou si vous voulez surcharger les valeurs de settings.py) :
# HUGGINGFACE_EMBEDDING_MODEL_NAME="sentence-transformers/all-MiniLM-L6-v2"
# HUGGINGFACE_EMBEDDING_MODEL_DIMENSION="384"

# Si DEFAULT_EMBEDDING_PROVIDER="ollama" (ou si vous voulez surcharger les valeurs de settings.py) :
# Assurez-vous que le modèle est servi par votre instance Ollama (ex: `ollama pull nomic-embed-text`)
# OLLAMA_EMBEDDING_MODEL_NAME="nomic-embed-text"
# OLLAMA_EMBEDDING_MODEL_DIMENSION="768" # Dimension pour nomic-embed-text
# OLLAMA_BASE_URL est partagé avec la configuration du LLM génératif Ollama (voir plus haut)


# --- Configuration du Traitement des Données (Optionnel - surchargent settings.py) ---
# CHUNK_SIZE="1000"
# CHUNK_OVERLAP="200"

# --- Configuration Spécifique à ArXiv (Optionnel - surchargent settings.py) ---
# ARXIV_DEFAULT_QUERY="Reinforcement Learning for Robotics"
# ARXIV_MAX_RESULTS="10"
# ... etc.

# --- Configuration de l'Évaluation (Optionnel - surchargent settings.py) ---
# EVALUATION_DATASET_PATH="data/evaluation/custom_rag_eval_dataset.json"